{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Programming with RDDs (Python)\n",
    "\n",
    "In this Notebook, we will study simple RDD, how to create them and how to operate with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Programming with RDDs\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RDD\n",
    "\n",
    "Create RDD from list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_rdd = sc.parallelize(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric RDD (from list): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Numeric RDD (from list): {0}\".format(numeric_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RDD from external file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = sc.textFile(\"../data/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text RDD (from external file): ['# Apache Spark', '', 'Spark is a fast and general cluster computing system for Big Data. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 'MLlib for machine learning, GraphX for graph processing,', 'and Spark Streaming for stream processing.', '', '<http://spark.apache.org/>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Text RDD (from external file): {0}\".format(text_rdd.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Actions\n",
    "\n",
    "In this section, we eplore come of the most common actions on RDDs:\n",
    "\n",
    "    * collect():\n",
    "    * take()\n",
    "    * count(), countByValue()\n",
    "    * tarkeOrdered(), takeSample()\n",
    "    * reduce(), fold()\n",
    "    * aggregate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect()` --> collect the RDD to the driver, `take(n)`: --> take to the driver n elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,1,2,3,33,1,4,5,8,6])\n",
    "rdd2 = sc.parallelize([1,2,9,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count()` --> count the total elements of the RDD, `countByValue()` --> count the number of occurences of each element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count(): 10\n",
      "countByValue(): defaultdict(<class 'int'>, {1: 3, 2: 1, 3: 1, 33: 1, 4: 1, 5: 1, 8: 1, 6: 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"count(): {0}\".format(rdd1.count()))\n",
    "print(\"countByValue(): {0}\".format(rdd1.countByValue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`takeOrdered(n)` take n elements of the RDD in order, `takeSample(n)` take n elements of the RDD randomly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takeOrdered: [1, 2, 8]\n",
      "takeSample: [8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"takeOrdered: {0}\".format(rdd2.takeOrdered(num=3)))\n",
    "print(\"takeSample: {0}\".format(rdd2.takeSample(num=2,withReplacement=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce()` --> reduce an RDD using some aggregation function, `fold()` -->  same as `reduce()`, but setting the initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of list using reduce(): 64\n",
      "Sum of list using fold(): 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of list using reduce(): {0}\".format(rdd1.reduce(lambda x, y: x + y)))\n",
    "print(\"Sum of list using fold(): {0}\".format(rdd1.fold(0, lambda x, y: x + y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating average using `reduce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average calculated using reduce(): 6.4\n"
     ]
    }
   ],
   "source": [
    "avg1 = rdd1.reduce(lambda x, y: x + y)/rdd1.count()\n",
    "print(\"Average calculated using reduce(): {0}\".format(avg1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating average using `aggregate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average calculated using aggregate(): 6.4\n"
     ]
    }
   ],
   "source": [
    "sum_values, count = rdd1.aggregate((0, 0), \n",
    "                                   (lambda acc, value: (acc[0] + value, acc[1] + 1)),\n",
    "                                   (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))\n",
    "\n",
    "avg2 = sum_values / count\n",
    "print(\"Average calculated using aggregate(): {0}\".format(avg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDD Transformations\n",
    "\n",
    "In this section, we will explore the most common transformation that can be performed on RDDs:\n",
    "\n",
    "    * map()\n",
    "    * flatMap()\n",
    "    * filter()\n",
    "    * distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map()` --> to apply element-wise operations over the elements of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD obtained using map(): [(0, 0), (1, 2), (2, 4), (3, 6), (4, 8), (5, 10), (6, 12), (7, 14), (8, 16), (9, 18)]\n"
     ]
    }
   ],
   "source": [
    "rdd_map = numeric_rdd.map(lambda x: (x, 2*x))\n",
    "print(\"RDD obtained using map(): {0}\".format(rdd_map.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMap()` --> to apply element-wise operations over the elements of an RDD flattening the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD obtained using flatMap(): ['#', 'Apache', 'Spark', '', 'Spark', 'is', 'a', 'fast', 'and', 'general']\n"
     ]
    }
   ],
   "source": [
    "rdd_flat_map = text_rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(\"RDD obtained using flatMap(): {0}\".format(rdd_flat_map.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter()` --> to filter the elements of an RDD according to a specific condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines that contains the word 'Spark': 16\n"
     ]
    }
   ],
   "source": [
    "lines_spark = text_rdd.map(lambda x: x.split(\" \")).filter(lambda x: \"Spark\" in x)\n",
    "print(\"Number of lines that contains the word 'Spark': {0}\".format(lines_spark.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times that the word 'Python' appears: 4\n"
     ]
    }
   ],
   "source": [
    "words_python = text_rdd.flatMap(lambda x: x.split(\" \")).filter(lambda x: \"Python\" == x.replace(\",\", \"\"))\n",
    "print(\"Number of times that the word 'Python' appears: {0}\".format(words_python.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`distinct()` --> to get the distinct elements of an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from distinct(): [4, 8, 1, 33, 5, 2, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"RDD from distinct(): {0}\".format(rdd1.distinct().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Set Operations\n",
    "\n",
    "Finally, we study the different pseudo-set operations that can be performed on two RDDs:\n",
    "\n",
    "    * union()\n",
    "    * subtract()\n",
    "    * intersection()\n",
    "    * cartesian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`union()` --> to concatenate two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from union(): [1, 1, 2, 3, 33, 1, 4, 5, 8, 6, 1, 2, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "rdd_union = rdd1.union(rdd2)\n",
    "print(\"RDD from union(): {0}\".format(rdd_union.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`subtract()` --> to subtract one RDD from another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from subtract(): [33, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "rdd_subtract = rdd1.subtract(rdd2)\n",
    "print(\"RDD from subtract(): {0}\".format(rdd_subtract.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`intersection()` --> to get the intersection between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from intersection(): [8, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd_intersection = rdd1.intersection(rdd2)\n",
    "print(\"RDD from intersection(): {0}\".format(rdd_intersection.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cartesian()` --> to get the cartesian product of two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from cartesian(): [(1, 1), (1, 1), (1, 2), (1, 2), (1, 9), (1, 9), (1, 8), (1, 8), (2, 1), (3, 1), (2, 2), (3, 2), (2, 9), (3, 9), (2, 8), (3, 8), (33, 1), (1, 1), (33, 2), (1, 2), (33, 9), (1, 9), (33, 8), (1, 8), (4, 1), (5, 1), (8, 1), (6, 1), (4, 2), (5, 2), (8, 2), (6, 2), (4, 9), (5, 9), (8, 9), (6, 9), (4, 8), (5, 8), (8, 8), (6, 8)]\n"
     ]
    }
   ],
   "source": [
    "rdd_cartesian = rdd1.cartesian(rdd2)\n",
    "print(\"RDD from cartesian(): {0}\".format(rdd_cartesian.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
