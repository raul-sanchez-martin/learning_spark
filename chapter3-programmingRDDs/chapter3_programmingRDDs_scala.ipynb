{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Programming with RDDs (Scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@7655a3a5\n",
       "sc = org.apache.spark.SparkContext@5885970d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://d244d6709d47:4041)\" target=\"new_tab\">Spark UI: local-1533139683237</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1533139683237: Some(http://d244d6709d47:4041)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"Programming with RDDs\").master(\"local[*]\").getOrCreate()\n",
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RDD\n",
    "\n",
    "Create RDD from list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numeric_rdd = ParallelCollectionRDD[0] at parallelize at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numeric_rdd = sc.parallelize(1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric RDD (from list): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n"
     ]
    }
   ],
   "source": [
    "println(\"Numeric RDD (from list): \" + numeric_rdd.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RDD from external file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_rdd = ../data/README.md MapPartitionsRDD[2] at textFile at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "../data/README.md MapPartitionsRDD[2] at textFile at <console>:29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text_rdd = sc.textFile(\"../data/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text RDD (from external file): # Apache Spark, , Spark is a fast and general cluster computing system for Big Data. It provides, high-level APIs in Scala, Java, Python, and R, and an optimized engine that, supports general computation graphs for data analysis. It also supports a, rich set of higher-level tools including Spark SQL for SQL and DataFrames,, MLlib for machine learning, GraphX for graph processing,, and Spark Streaming for stream processing., , <http://spark.apache.org/>\n"
     ]
    }
   ],
   "source": [
    "println(\"Text RDD (from external file): \" + text_rdd.take(10).mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD actions\n",
    "\n",
    "`collect()`, `take()`: --> previous section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1 = ParallelCollectionRDD[3] at parallelize at <console>:29\n",
       "rdd2 = ParallelCollectionRDD[4] at parallelize at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at <console>:30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(1,1,2,3,33,1,4,5,8,6))\n",
    "val rdd2 = sc.parallelize(List(1,2,9,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count()`, `countByValue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count(): 10\n",
      "countByValue(): Map(5 -> 1, 1 -> 3, 6 -> 1, 33 -> 1, 2 -> 1, 3 -> 1, 8 -> 1, 4 -> 1)\n"
     ]
    }
   ],
   "source": [
    "println(\"count(): \" + rdd1.count())\n",
    "println(\"countByValue(): \" + rdd1.countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`takeOrdered()`, `takeSample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takeOrdered(): 1, 2, 8\n",
      "takeSample(): 2, 9\n"
     ]
    }
   ],
   "source": [
    "println(\"takeOrdered(): \" + rdd2.takeOrdered(3).mkString(\", \"))\n",
    "println(\"takeSample(): \" + rdd2.takeSample(false, 2).mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce()`, `fold()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of list using reduce(): 64\n",
      "Sum of list using fold(): 64\n"
     ]
    }
   ],
   "source": [
    "println(\"Sum of list using reduce(): \" + rdd1.reduce(_ + _))\n",
    "println(\"Sum of list using fold(): \" + rdd1.fold(0)(_ + _))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating average using `reduce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average calculated using reduce(): 6.4\n"
     ]
    }
   ],
   "source": [
    "println(\"Average calculated using reduce(): \" + rdd1.reduce(_ + _).toFloat/rdd1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating average using `aggregate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average calculated using aggregate(): 6.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sum_values = 64\n",
       "count = 10\n",
       "avg = 6.4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (sum_values, count) = rdd1.aggregate((0,0))(\n",
    "                                        (acc, value) => (acc._1 + value, acc._2 + 1),\n",
    "                                        (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))\n",
    "\n",
    "val avg = sum_values.toFloat/count\n",
    "println(\"Average calculated using aggregate(): \" + avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDD Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD obtained using map: 3, 0, 1, 6, 7, 4, 5, 10, 11, 8"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_map = MapPartitionsRDD[10] at map at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at map at <console>:31"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_map = numeric_rdd.map(_^2)\n",
    "print(\"RDD obtained using map: \" + rdd_map.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMap()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD obtained using flatMap: #, Apache, Spark, , Spark, is, a, fast, and, general\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_flat_map = MapPartitionsRDD[11] at flatMap at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[11] at flatMap at <console>:31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_flat_map = text_rdd.flatMap(_.split(\" \"))\n",
    "println(\"RDD obtained using flatMap: \" + rdd_flat_map.take(10).mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines that contains the word 'Spark': 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines_spark = MapPartitionsRDD[13] at filter at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at filter at <console>:31"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines_spark = text_rdd.map(_.split(\" \")).filter(_.contains(\"Spark\"))\n",
    "println(\"Number of lines that contains the word 'Spark': \" + lines_spark.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times that the word 'Python' appears: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "words_python = MapPartitionsRDD[15] at filter at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[15] at filter at <console>:31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words_python = text_rdd.flatMap(_.split(\" \")).filter(_.replace(\",\", \"\") == \"Python\")\n",
    "println(\"Number of times that the word 'Python' appears: \" + words_python.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`distinct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from distinct(): 4, 8, 33, 1, 5, 6, 2, 3"
     ]
    }
   ],
   "source": [
    "print(\"RDD from distinct(): \" + rdd1.distinct().collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Set Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`union()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from union(): 1, 1, 2, 3, 33, 1, 4, 5, 8, 6, 1, 2, 9, 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_union = UnionRDD[19] at union at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[19] at union at <console>:32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_union = rdd1.union(rdd2)\n",
    "println(\"RDD from union(): \" + rdd_union.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`subtract()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from subtract(): 4, 33, 5, 6, 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_subtract = MapPartitionsRDD[23] at subtract at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[23] at subtract at <console>:32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_subtract = rdd1.subtract(rdd2)\n",
    "println(\"RDD from subtract(): \" + rdd_subtract.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`intersection()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from intersection(): 8, 1, 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_intersection = MapPartitionsRDD[29] at intersection at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[29] at intersection at <console>:32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_intersection = rdd1.intersection(rdd2)\n",
    "println(\"RDD from intersection(): \" + rdd_intersection.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cartesian()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from cartesian(): (1,1), (1,1), (1,2), (1,2), (1,9), (1,9), (1,8), (1,8), (2,1), (3,1), (33,1), (2,2), (3,2), (33,2), (2,9), (3,9), (33,9), (2,8), (3,8), (33,8), (1,1), (4,1), (1,2), (4,2), (1,9), (4,9), (1,8), (4,8), (5,1), (8,1), (6,1), (5,2), (8,2), (6,2), (5,9), (8,9), (6,9), (5,8), (8,8), (6,8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_cartesian = CartesianRDD[30] at cartesian at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CartesianRDD[30] at cartesian at <console>:32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_cartesian = rdd1.cartesian(rdd2)\n",
    "println(\"RDD from cartesian(): \" + rdd_cartesian.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
