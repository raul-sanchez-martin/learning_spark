{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with MLlib\n",
    "\n",
    "In this Notebook, we will review the RDD-Based Machine Learning library MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MLlib\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "First, we have to understand the different data structures used by MLlib. In particular, they are:\n",
    "\n",
    "    * Vectors\n",
    "    * Labeled Points\n",
    "    * Rating\n",
    "    * Model Classes\n",
    "\n",
    "We will se `Vectors` and `Labeled Points` in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vector()` --> thold the features values. It can be `dense` and `sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dense = Vectors.dense([1.0,1.0,2.0,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sparse_1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sparse_2 = Vectors.sparse(4, [0, 2], [1.0, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint()` --> hold both features values and label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_point = LabeledPoint(1, vector_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "In this section, we will review the different algorithms associated with Machine Learning problems. Among other, we could highlight the following families of algorithms:\n",
    "\n",
    "    * Feature Extraction\n",
    "    * Statistics\n",
    "    * Classification and Regression\n",
    "    * Collaborative Filtering and Recommendation\n",
    "    * Dimensionality Reduction\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "ML algorithms only accept numerical values as inputs. Here, we discuss some algorithm that help us to translate some inputs (like text, non-scaled numerical vectors, etc) to numerical values that ML algorithms can understand. In particular, we will discuss the following algorithms:\n",
    "\n",
    "    * TD-IDF\n",
    "    * Scaling\n",
    "    * Normalization\n",
    "    * Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### td-idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`td-idf()` --> Term Frecuency - Inverse Document Frequency, useful to convert text input to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sc.parallelize([\"hello\", \"hello how are you\", \"good bye\", \"bye\"])\n",
    "words = sentences.map(lambda word: word.split(\" \"))\n",
    "tf = HashingTF(100)\n",
    "tf_vectors = tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {45: 1.0}),\n",
       " SparseVector(100, {1: 1.0, 21: 1.0, 24: 1.0, 45: 1.0}),\n",
       " SparseVector(100, {64: 1.0, 88: 1.0}),\n",
       " SparseVector(100, {88: 1.0})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF()\n",
    "idf_model = idf.fit(tf_vectors)\n",
    "tf_idf_vectors = idf_model.transform(tf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {45: 0.5108}),\n",
       " SparseVector(100, {1: 0.9163, 21: 0.9163, 24: 0.9163, 45: 0.5108}),\n",
       " SparseVector(100, {64: 0.9163, 88: 0.5108}),\n",
       " SparseVector(100, {88: 0.5108})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect\n",
    "\n",
    "`Word2Vec` --> also useful to tranform text into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec().setMinCount(0)\n",
    "word2vec_model = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = word2vec_model.transform(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0005, 0.0002, -0.0012, -0.0001, 0.0049, -0.001, 0.0043, 0.0039, 0.0007, 0.0022, -0.0008, -0.0003, -0.001, 0.0046, 0.0, -0.0046, 0.0025, -0.003, -0.0031, -0.0001, 0.0038, 0.003, 0.0026, -0.0048, -0.002, 0.0037, 0.0007, -0.0013, 0.002, 0.0035, -0.0032, 0.0014, 0.0045, 0.0006, -0.0033, -0.0008, -0.0031, -0.0026, 0.0044, -0.0013, 0.0004, 0.0024, 0.0021, -0.0048, -0.0008, 0.0035, -0.0015, 0.0013, -0.0017, 0.0027, -0.0006, 0.0001, -0.0009, -0.0026, -0.001, 0.0008, -0.0007, 0.0038, -0.0032, -0.0025, -0.0032, 0.0039, -0.0, 0.0037, -0.0027, 0.0021, 0.0002, -0.0027, 0.0041, -0.0006, -0.001, 0.0043, 0.0011, -0.0022, 0.003, -0.0046, -0.0021, -0.005, 0.0022, 0.0014, 0.0033, -0.0009, 0.0008, 0.0018, 0.0012, -0.0047, 0.0009, -0.0037, 0.0029, 0.0002, 0.0039, 0.0037, 0.0032, -0.0011, -0.002, 0.0, -0.0006, -0.0004, -0.0012, 0.0004])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "While our input data could be already numeric, it is useful sometimes for the ML algorithms to scale that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler()` --> to scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [Vectors.dense([-2.0, 5.0, 1.0, 4.0]),\n",
    "           Vectors.dense([2.0, 0.0, 1.0, 7.2]),\n",
    "           Vectors.dense([4.0, 2.0, 0.5, 0.8])]\n",
    "\n",
    "vectors_rdd = sc.parallelize(vectors)\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "model = scaler.fit(vectors_rdd)\n",
    "scaled_data = model.transform(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-1.0911, 1.0596, 0.5774, 0.0]),\n",
       " DenseVector([0.2182, -0.9272, 0.5774, 1.0]),\n",
       " DenseVector([0.8729, -0.1325, -1.1547, -1.0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "As with scaling, sometimes it is very usefull to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "norm_data = norm.transform(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-0.2949, 0.7372, 0.1474, 0.5898]),\n",
       " DenseVector([0.2653, 0.0, 0.1326, 0.955]),\n",
       " DenseVector([0.8752, 0.4376, 0.1094, 0.175])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "The library MLlib includes useful functionalities to calculate some main statistics over numeric RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colStats()\n",
    "\n",
    "`colStats()` --> to calculate statistics over an RDD of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_stats = Statistics.colStats(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_stats_dict = {\n",
    "    \"count\": col_stats.count(),\n",
    "    \"max\": col_stats.max(),\n",
    "    \"mean\": col_stats.mean(),\n",
    "    \"min\": col_stats.min(),\n",
    "    \"normL1\": col_stats.normL1(),\n",
    "    \"normL2\": col_stats.normL2(),\n",
    "    \"numNonzeros\": col_stats.numNonzeros(),\n",
    "    \"variance\": col_stats.variance()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 3\n",
      "max: [ 4.   5.   1.   7.2]\n",
      "mean: [ 1.33333333  2.33333333  0.83333333  4.        ]\n",
      "min: [-2.   0.   0.5  0.8]\n",
      "normL1: [  8.    7.    2.5  12. ]\n",
      "normL2: [ 4.89897949  5.38516481  1.5         8.27526435]\n",
      "numNonzeros: [ 3.  2.  3.  3.]\n",
      "variance: [  9.33333333   6.33333333   0.08333333  10.24      ]\n"
     ]
    }
   ],
   "source": [
    "for key, value in col_stats_dict.items():\n",
    "    print(\"{0}: {1}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corr()\n",
    "\n",
    "`corr()` --> to calculate the correlation matrix between the columns of one RDD or between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.73704347, -0.75592895, -0.32732684],\n",
       "       [-0.73704347,  1.        ,  0.11470787, -0.39735971],\n",
       "       [-0.75592895,  0.11470787,  1.        ,  0.8660254 ],\n",
       "       [-0.32732684, -0.39735971,  0.8660254 ,  1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "data2 = sc.parallelize([10, 19, 32, 41, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996326893005933"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chiSqTest()\n",
    "\n",
    "`chiSqTest()` --> to compute the Pearson's independence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_point_rdd = vectors_rdd.map(lambda x: LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_test = Statistics.chiSqTest(label_point_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test value: 1.0: \n",
      "Test value: 1.0: \n",
      "Test value: 1.0: \n",
      "Test value: 1.0: \n"
     ]
    }
   ],
   "source": [
    "for test in chi_sq_test:\n",
    "    print(\"Test value: {0}: \".format(test.pValue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Regression\n",
    "\n",
    "In this section, we will explore the conventional Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, random\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create training data according to a Linear Regression model with the following weights and intercept:\n",
    "\n",
    "    * Weights: [5, 3, 8, 1]\n",
    "    * Intercept: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(x):\n",
    "    \"\"\"\n",
    "    Given an input vector x, returns the following value:\n",
    "    5*x[0] + 3*x[1] + 8*x[2] + x[3] + 20 + random()\n",
    "    \n",
    "    :input x: input vector\n",
    "    :return: computated value\n",
    "    \"\"\"\n",
    "    \n",
    "    return 5*x[0] + 3*x[1] + 8*x[2] + x[3] + 20 + random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_features = [[randint(0,20) for _ in range(4)] for _ in range(100)]\n",
    "reg_features_rdd = sc.parallelize(reg_features)\n",
    "scaler = StandardScaler()\n",
    "reg_features_scale = scaler.fit(reg_features_rdd).transform(reg_features_rdd)\n",
    "reg_data = reg_features_scale.map(lambda x: LabeledPoint(linear_reg(x), Vectors.dense(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(31.44320327503508, [0.325623877528,1.31040867515,0.518832866202,1.03965430882]),\n",
       " LabeledPoint(60.720532751402224, [3.25623877528,1.96561301272,2.07533146481,1.21293002696])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been created, we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegressionWithSGD.train(data = reg_data, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the value of the original and computated weights and intercpet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed --> Weights: [5.89847605314,3.62444173624,8.39588463624,1.62514271703]; Intercept: 16.034763324762647\n",
      "Original --> Weights: [5, 3, 8, 1]; Intercept: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Computed --> Weights: {0}; Intercept: {1}\".format(lr_model.weights, lr_model.intercept))\n",
    "print(\"Original --> Weights: {0}; Intercept: {1}\".format([5, 3, 8, 1], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
