{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with MLlib\n",
    "\n",
    "In this Notebook, we will review the RDD-Based Machine Learning library MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MLlib\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "First, we have to understand the different data structures used by MLlib. In particular, they are:\n",
    "\n",
    "    * Vectors\n",
    "    * Labeled Points\n",
    "    * Rating\n",
    "    * Model Classes\n",
    "\n",
    "We will se `Vectors` and `Labeled Points` in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vector()` --> thold the features values. It can be `dense` and `sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dense = Vectors.dense([1.0,1.0,2.0,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sparse_1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sparse_2 = Vectors.sparse(4, [0, 2], [1.0, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint()` --> hold both features values and label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_point = LabeledPoint(1, vector_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "In this section, we will review the different algorithms associated with Machine Learning problems. Among other, we could highlight the following families of algorithms:\n",
    "\n",
    "    * Feature Extraction\n",
    "    * Statistics\n",
    "    * Classification and Regression\n",
    "    * Collaborative Filtering and Recommendation\n",
    "    * Dimensionality Reduction\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "ML algorithms only accept numerical values as inputs. Here, we discuss some algorithm that help us to translate some inputs (like text, non-scaled numerical vectors, etc) to numerical values that ML algorithms can understand. In particular, we will discuss the following algorithms:\n",
    "\n",
    "    * TD-IDF\n",
    "    * Scaling\n",
    "    * Normalization\n",
    "    * Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### td-idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`td-idf()` --> Term Frecuency - Inverse Document Frequency, useful to convert text input to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sc.parallelize([\"hello\", \"hello how are you\", \"good bye\", \"bye\"])\n",
    "words = sentences.map(lambda word: word.split(\" \"))\n",
    "tf = HashingTF(100)\n",
    "tf_vectors = tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {45: 1.0}),\n",
       " SparseVector(100, {1: 1.0, 21: 1.0, 24: 1.0, 45: 1.0}),\n",
       " SparseVector(100, {64: 1.0, 88: 1.0}),\n",
       " SparseVector(100, {88: 1.0})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF()\n",
    "idf_model = idf.fit(tf_vectors)\n",
    "tf_idf_vectors = idf_model.transform(tf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {45: 0.5108}),\n",
       " SparseVector(100, {1: 0.9163, 21: 0.9163, 24: 0.9163, 45: 0.5108}),\n",
       " SparseVector(100, {64: 0.9163, 88: 0.5108}),\n",
       " SparseVector(100, {88: 0.5108})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect\n",
    "\n",
    "`Word2Vec` --> also useful to tranform text into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec().setMinCount(0)\n",
    "word2vec_model = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = word2vec_model.transform(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0041, -0.0042, 0.0003, -0.0047, -0.0049, -0.0031, 0.0016, -0.0043, 0.0004, 0.0001, 0.001, 0.0001, 0.0044, -0.0018, -0.0035, -0.0047, 0.0037, 0.0006, 0.0029, -0.0016, 0.0003, -0.0047, 0.0039, 0.0041, 0.0025, -0.0047, -0.0018, 0.0021, -0.0003, -0.0013, 0.0025, -0.0012, -0.0009, 0.0006, 0.0034, -0.004, 0.0018, -0.0032, 0.0034, -0.0001, -0.0031, -0.0005, 0.0025, 0.0022, 0.0029, -0.0013, 0.0004, -0.0038, 0.0005, -0.0012, -0.0008, 0.0035, -0.0029, -0.0005, 0.0013, -0.0045, 0.003, 0.0015, -0.0047, -0.0023, -0.0031, -0.0036, 0.0048, -0.0038, -0.0002, 0.0024, -0.0026, 0.005, -0.0019, 0.001, -0.004, -0.0021, 0.0025, -0.0015, -0.0026, 0.0046, -0.0029, 0.0026, -0.0004, -0.0025, 0.0008, -0.0031, 0.0041, 0.0039, -0.0019, -0.0028, -0.0044, -0.004, 0.0034, -0.0014, 0.0048, 0.0044, -0.0022, 0.0049, -0.0015, 0.0021, 0.0046, -0.0019, -0.0036, -0.0034])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "While our input data could be already numeric, it is useful sometimes for the ML algorithms to scale that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler()` --> to scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [Vectors.dense([-2.0, 5.0, 1.0, 4.0]),\n",
    "           Vectors.dense([2.0, 0.0, 1.0, 7.2]),\n",
    "           Vectors.dense([4.0, 2.0, 0.5, 0.8])]\n",
    "\n",
    "vectors_rdd = sc.parallelize(vectors)\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "model = scaler.fit(vectors_rdd)\n",
    "scaled_data = model.transform(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-1.0911, 1.0596, 0.5774, 0.0]),\n",
       " DenseVector([0.2182, -0.9272, 0.5774, 1.0]),\n",
       " DenseVector([0.8729, -0.1325, -1.1547, -1.0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "As with scaling, sometimes it is very usefull to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "norm_data = norm.transform(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-0.2949, 0.7372, 0.1474, 0.5898]),\n",
       " DenseVector([0.2653, 0.0, 0.1326, 0.955]),\n",
       " DenseVector([0.8752, 0.4376, 0.1094, 0.175])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "The library MLlib includes useful functionalities to calculate some main statistics over numeric RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colStats()\n",
    "\n",
    "`colStats()` --> to calculate statistics over an RDD of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_stats = Statistics.colStats(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_stats_dict = {\n",
    "    \"count\": col_stats.count(),\n",
    "    \"max\": col_stats.max(),\n",
    "    \"mean\": col_stats.mean(),\n",
    "    \"min\": col_stats.min(),\n",
    "    \"normL1\": col_stats.normL1(),\n",
    "    \"normL2\": col_stats.normL2(),\n",
    "    \"numNonzeros\": col_stats.numNonzeros(),\n",
    "    \"variance\": col_stats.variance()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 3\n",
      "max: [ 4.   5.   1.   7.2]\n",
      "mean: [ 1.33333333  2.33333333  0.83333333  4.        ]\n",
      "min: [-2.   0.   0.5  0.8]\n",
      "normL1: [  8.    7.    2.5  12. ]\n",
      "normL2: [ 4.89897949  5.38516481  1.5         8.27526435]\n",
      "numNonzeros: [ 3.  2.  3.  3.]\n",
      "variance: [  9.33333333   6.33333333   0.08333333  10.24      ]\n"
     ]
    }
   ],
   "source": [
    "for key, value in col_stats_dict.items():\n",
    "    print(\"{0}: {1}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corr()\n",
    "\n",
    "`corr()` --> to calculate the correlation matrix between the columns of one RDD or between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.73704347, -0.75592895, -0.32732684],\n",
       "       [-0.73704347,  1.        ,  0.11470787, -0.39735971],\n",
       "       [-0.75592895,  0.11470787,  1.        ,  0.8660254 ],\n",
       "       [-0.32732684, -0.39735971,  0.8660254 ,  1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "data2 = sc.parallelize([10, 19, 32, 41, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996326893005933"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chiSqTest()\n",
    "\n",
    "`chiSqTest()` --> to compute the Pearson's independence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_point_rdd = vectors_rdd.map(lambda x: LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_test = Statistics.chiSqTest(label_point_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test value: 1.0: \n",
      "Test value: 1.0: \n",
      "Test value: 1.0: \n",
      "Test value: 1.0: \n"
     ]
    }
   ],
   "source": [
    "for test in chi_sq_test:\n",
    "    print(\"Test value: {0}: \".format(test.pValue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Regression\n",
    "\n",
    "In this section, we will explore the conventional Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, random\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create training data according to a Linear Regression model with the following weights and intercept:\n",
    "\n",
    "    * Weights: [5, 3, 8, 1]\n",
    "    * Intercept: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(x):\n",
    "    \"\"\"\n",
    "    Given an input vector x, returns the following value:\n",
    "    5*x[0] + 3*x[1] + 8*x[2] + x[3] + 20 + random()\n",
    "    \n",
    "    :input x: input vector\n",
    "    :return: computated value\n",
    "    \"\"\"\n",
    "    \n",
    "    return 5*x[0] + 3*x[1] + 8*x[2] + x[3] + 20 + random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_features = [[randint(0,20) for _ in range(4)] for _ in range(100)]\n",
    "reg_features_rdd = sc.parallelize(reg_features)\n",
    "scaler = StandardScaler()\n",
    "reg_features_scale = scaler.fit(reg_features_rdd).transform(reg_features_rdd)\n",
    "reg_data = reg_features_scale.map(lambda x: LabeledPoint(linear_reg(x), Vectors.dense(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(55.09369153419037, [2.24426297982,0.0,2.61640695905,2.00223719375]),\n",
       " LabeledPoint(65.00138140972491, [2.40456747838,3.20630734343,2.4528815241,3.00335579063])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been created, we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegressionWithSGD.train(data = reg_data, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the value of the original and computated weights and intercpet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed --> Weights: [-92.7816357191,-115.27305982,-93.9699605439,-98.8850015221]; Intercept: -41.21507547619649\n",
      "Original --> Weights: [5, 3, 8, 1]; Intercept: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Computed --> Weights: {0}; Intercept: {1}\".format(lr_model.weights, lr_model.intercept))\n",
    "print(\"Original --> Weights: {0}; Intercept: {1}\".format([5, 3, 8, 1], 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Classification\n",
    "\n",
    "In this section, we will explore different classification models:\n",
    "\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines (SVMs)\n",
    "    * Naive Bayes\n",
    "    * Decision Trees\n",
    "    * Random Forests\n",
    "    \n",
    "For every case, we will try to solve the sampe problem: a model to classify messages into two groups: legitimate and Spam. For that, we will have first to preprocess some text data using come functionalities studied in previous sections of this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, SVMWithSGD, NaiveBayes\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_data = spark.read.csv(\"../data/spam.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+\n",
      "|label|                text| _c2| _c3| _c4|\n",
      "+-----+--------------------+----+----+----+\n",
      "|  ham|Go until jurong p...|null|null|null|\n",
      "|  ham|Ok lar... Joking ...|null|null|null|\n",
      "| spam|Free entry in 2 a...|null|null|null|\n",
      "|  ham|U dun say so earl...|null|null|null|\n",
      "|  ham|Nah I don't think...|null|null|null|\n",
      "| spam|FreeMsg Hey there...|null|null|null|\n",
      "|  ham|Even my brother i...|null|null|null|\n",
      "|  ham|As per your reque...|null|null|null|\n",
      "| spam|WINNER!! As a val...|null|null|null|\n",
      "| spam|Had your mobile 1...|null|null|null|\n",
      "|  ham|I'm gonna be home...|null|null|null|\n",
      "| spam|SIX chances to wi...|null|null|null|\n",
      "| spam|URGENT! You have ...|null|null|null|\n",
      "|  ham|I've been searchi...|null|null|null|\n",
      "|  ham|I HAVE A DATE ON ...|null|null|null|\n",
      "| spam|XXXMobileMovieClu...|null|null|null|\n",
      "|  ham|Oh k...i'm watchi...|null|null|null|\n",
      "|  ham|Eh u remember how...|null|null|null|\n",
      "|  ham|Fine if that��s t...|null|null|null|\n",
      "| spam|England v Macedon...|null|null|null|\n",
      "+-----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ini_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_data_rdd = ini_data.select([\"label\", \"text\"]).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='ham', text='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd.take(1)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_data_rdd_filter = ini_data_rdd.filter(lambda row: (isinstance(row.label, str) and isinstance(row.text, str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5573"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd_filter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = ini_data_rdd_filter.map(lambda row: row.text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(1000)\n",
    "tf_vectors = tf.transform(text_rdd)\n",
    "idf = IDF()\n",
    "idf_model = idf.fit(tf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_text = ini_data_rdd_filter.filter(lambda row: row.label == \"spam\").map(lambda row: row.text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Free',\n",
       "  'entry',\n",
       "  'in',\n",
       "  '2',\n",
       "  'a',\n",
       "  'wkly',\n",
       "  'comp',\n",
       "  'to',\n",
       "  'win',\n",
       "  'FA',\n",
       "  'Cup',\n",
       "  'final',\n",
       "  'tkts',\n",
       "  '21st',\n",
       "  'May',\n",
       "  '2005.',\n",
       "  'Text',\n",
       "  'FA',\n",
       "  'to',\n",
       "  '87121',\n",
       "  'to',\n",
       "  'receive',\n",
       "  'entry',\n",
       "  'question(std',\n",
       "  'txt',\n",
       "  \"rate)T&C's\",\n",
       "  'apply',\n",
       "  \"08452810075over18's\"],\n",
       " ['FreeMsg',\n",
       "  'Hey',\n",
       "  'there',\n",
       "  'darling',\n",
       "  \"it's\",\n",
       "  'been',\n",
       "  '3',\n",
       "  \"week's\",\n",
       "  'now',\n",
       "  'and',\n",
       "  'no',\n",
       "  'word',\n",
       "  'back!',\n",
       "  \"I'd\",\n",
       "  'like',\n",
       "  'some',\n",
       "  'fun',\n",
       "  'you',\n",
       "  'up',\n",
       "  'for',\n",
       "  'it',\n",
       "  'still?',\n",
       "  'Tb',\n",
       "  'ok!',\n",
       "  'XxX',\n",
       "  'std',\n",
       "  'chgs',\n",
       "  'to',\n",
       "  'send,',\n",
       "  '�1.50',\n",
       "  'to',\n",
       "  'rcv'],\n",
       " ['WINNER!!',\n",
       "  'As',\n",
       "  'a',\n",
       "  'valued',\n",
       "  'network',\n",
       "  'customer',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'selected',\n",
       "  'to',\n",
       "  'receivea',\n",
       "  '�900',\n",
       "  'prize',\n",
       "  'reward!',\n",
       "  'To',\n",
       "  'claim',\n",
       "  'call',\n",
       "  '09061701461.',\n",
       "  'Claim',\n",
       "  'code',\n",
       "  'KL341.',\n",
       "  'Valid',\n",
       "  '12',\n",
       "  'hours',\n",
       "  'only.']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_text.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = ini_data_rdd_filter.filter(lambda row: row.label != \"spam\").map(lambda row: row.text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4826"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Go',\n",
       "  'until',\n",
       "  'jurong',\n",
       "  'point,',\n",
       "  'crazy..',\n",
       "  'Available',\n",
       "  'only',\n",
       "  'in',\n",
       "  'bugis',\n",
       "  'n',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'e',\n",
       "  'buffet...',\n",
       "  'Cine',\n",
       "  'there',\n",
       "  'got',\n",
       "  'amore',\n",
       "  'wat...'],\n",
       " ['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...'],\n",
       " ['U',\n",
       "  'dun',\n",
       "  'say',\n",
       "  'so',\n",
       "  'early',\n",
       "  'hor...',\n",
       "  'U',\n",
       "  'c',\n",
       "  'already',\n",
       "  'then',\n",
       "  'say...']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_vectors = tf.transform(spam_text)\n",
    "spam_idf = idf_model.transform(spam_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1000, {4: 4.2564, 52: 1.9576, 162: 3.663, 261: 5.407, 289: 1.5941, 309: 9.9766, 359: 5.2937, 365: 3.6159, 368: 4.8647, 389: 4.2314, 408: 4.16, 505: 5.0423, 524: 9.8246, 542: 4.8882, 547: 2.8359, 569: 3.8809, 571: 5.2586, 588: 4.937, 627: 2.9288, 633: 4.467, 648: 4.4212, 655: 3.602, 665: 4.2951, 783: 2.6097})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_idf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vectors = tf.transform(gen_text)\n",
    "gen_idf = idf_model.transform(gen_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1000, {14: 4.8417, 17: 4.1041, 41: 4.8417, 52: 1.9576, 66: 4.9123, 84: 6.8547, 97: 5.0423, 125: 5.015, 501: 3.7976, 604: 2.5278, 606: 4.467, 657: 4.3218, 668: 3.3376, 683: 3.4329, 708: 5.1919, 802: 4.5828, 914: 5.0995, 932: 4.5828, 993: 4.072})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_idf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_points = spam_idf.map(lambda x: LabeledPoint(1, x))\n",
    "gen_points = gen_idf.map(lambda x: LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (1000,[4,52,162,261,289,309,359,365,368,389,408,505,524,542,547,569,571,588,627,633,648,655,665,783],[4.25642035557,1.95763995962,3.66302357778,5.40699238317,1.59412694928,9.97656409663,5.29366369787,3.61586790789,4.86466809235,4.23141905337,4.15996008939,5.04234926959,9.82459228268,4.88819858976,2.83590803714,3.88093607968,5.25857237806,4.93698875393,2.92877472154,4.46698512468,4.42117558865,3.6019876872,4.29513486776,2.60971104834]))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_points.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (1000,[14,17,41,52,66,84,97,125,501,604,606,657,668,683,708,802,914,932,993],[4.84167857412,4.10407963099,4.84167857412,1.95763995962,4.91229614134,6.85474235355,5.04234926959,5.0149502954,3.79755447074,2.52779392588,4.46698512468,4.32180311484,3.33760117735,3.43291135715,5.19188100356,4.58281694021,5.09950768343,4.58281694021,4.07199131644]))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_points.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_ini = spam_points.union(gen_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = ml_data_ini.map(lambda row: (randint(0,100), row)).sortByKey().map(lambda row: row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.map(lambda x: x.label).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_train, ml_data_test = ml_data.randomSplit(weights = [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[651] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_train.cache()\n",
    "ml_data_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4422"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1151"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(1000, {67: 4.1041, 69: 2.9288, 72: 4.4362, 80: 2.2979, 101: 2.0411, 111: 3.6914, 119: 2.7124, 175: 3.4784, 186: 3.2277, 289: 1.5941, 299: 4.1041, 300: 2.4605, 339: 5.407, 343: 3.663, 437: 3.8467, 462: 3.5507, 500: 4.1951, 575: 4.7547, 581: 2.9491, 657: 8.6436, 676: 4.5828, 818: 4.5828, 833: 5.7355, 870: 2.1097, 886: 4.7547, 925: 2.7342, 935: 3.6219})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_test.take(1)[0].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegressionWithSGD.train(data=ml_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, lr_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suport Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVMWithSGD.train(data=ml_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, svm_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = NaiveBayes.train(data=ml_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, nb_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTree.trainClassifier(data=ml_data_train, numClasses = 2, categoricalFeaturesInfo={},\n",
    "                                          maxDepth=15, maxBins=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, tree_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForest.trainClassifier(data=ml_data_train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                            maxDepth=15, maxBins=64, numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, forest_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Clustering\n",
    "\n",
    "In this section, we will explore the `K-means` algorithm, which is the main clustering algorithm included in MLlib.\n",
    "\n",
    "Here, we will study the previous spam classification problem. We will cluster our mesages into two groups, and then, we will count the number of points that fall into each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1201] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_data = ml_data.map(lambda lpoint: lpoint.features)\n",
    "cluster_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans.train(cluster_data, 2, maxIterations=1700, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cluster_data.map(lambda x: clusters.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 4596, 0: 977})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collavorative Filtering and Recommendation: Alternating Least Squares\n",
    "\n",
    "Now, we will explore the `Alternating Least Squares` algorithm, very used for collaborative filtering problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_als = sc.textFile(\"../data/als/test.data\")\n",
    "ratings = data_als.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=1, rating=5.0)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a recommendation moddel using ALS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 10\n",
    "numIterations = 10\n",
    "als_model = ALS.train(ratings, rank, numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ratings.map(lambda p: (p[0], p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 2)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_predictions = als_model.predictAll(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=1, rating=4.997434130426677),\n",
       " Rating(user=1, product=2, rating=0.9999267659226103)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_predictions.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "In this section, we will see two main functionalities included in MLlib relative to dimensionality reduction:\n",
    "\n",
    "    * Principal Component Analysis\n",
    "    * Singular Vector Decomposition\n",
    "    \n",
    "    \n",
    "We will use the data from the Clustering Section, training also a KMeans model with the \"reduced\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1000, {49: 5.6814, 63: 3.7059, 65: 4.7972, 111: 3.6914, 243: 4.6369, 320: 3.3376, 339: 5.407, 365: 2.4106, 421: 3.7817, 540: 2.9982, 564: 4.6185, 635: 3.8137, 661: 5.1919, 668: 3.3376, 686: 5.1294, 725: 5.4904, 740: 3.4668, 803: 4.0207, 813: 5.2247, 824: 3.1791, 870: 2.1097, 880: 4.2564, 948: 4.072}),\n",
       " SparseVector(1000, {51: 4.4515, 98: 3.9815, 119: 2.7124, 174: 3.4441, 278: 4.3084, 287: 3.953, 289: 1.5941, 300: 2.4605, 403: 2.6674, 477: 5.4904, 483: 3.9911, 495: 1.8168, 561: 5.5348, 581: 2.9491, 670: 4.7757, 783: 2.6097, 809: 12.3782, 853: 4.8882, 870: 2.1097, 872: 3.0846, 895: 4.8647, 976: 3.9719})]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = RowMatrix(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = mat.computePrincipalComponents(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_pca = mat.multiply(pc).rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model_pca = KMeans.train(projected_pca, 2, maxIterations=1700, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pca = projected_pca.map(lambda x: kmeans_model_pca.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {0: 5515, 1: 58})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_pca.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = mat.computeSVD(20, computeU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_svd = mat.multiply(svd.V).rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model_svd = KMeans.train(projected_svd, 2, maxIterations=1700, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svd = projected_svd.map(lambda x: kmeans_model_svd.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 4016, 0: 1557})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_svd.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "MLlib includes some functionalities to calculate automatically some metrics of trained ML models. While there are more, here we will evaluate the LR model of the spam classification section using the `BinaryClassificationMetrics` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (1000,[49,63,65,111,243,320,339,365,421,540,564,635,661,668,686,725,740,803,813,824,870,880,948],[5.68142922888,3.70588728221,4.79722681155,3.69139427491,4.63688416148,3.33760117735,5.40699238317,2.41057860526,3.78168112158,2.99824709435,4.61853502281,3.81368385267,5.19188100356,3.33760117735,5.12936064658,5.49037399211,3.46681290883,4.02069802205,5.22467082638,3.17913083638,2.109675132,4.25642035557,4.07199131644]))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_lr = ml_data_test.map(lambda lpoint: (float(lr_model.predict(lpoint.features)), lpoint.label))\n",
    "metrics_lr = BinaryClassificationMetrics(pred_label_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR model\n",
      "Area Under PR: 0.7012104519813399\n",
      "Area Under ROC: 0.8513007541963054\n"
     ]
    }
   ],
   "source": [
    "print(\"LR model\")\n",
    "print(\"Area Under PR: {0}\".format(metrics_lr.areaUnderPR))\n",
    "print(\"Area Under ROC: {0}\".format(metrics_lr.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline API\n",
    "\n",
    "ML pipelines are an interesting concept in order to organize all the tasks relative to a ML problem (data preparation + model training) into a Pipeline. In this section, we will solve the spam classification problem using ML pipelines, which are made by a series of Transformers and Estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, SQLTransformer, StringIndexer\n",
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+\n",
      "|label|                text| _c2| _c3| _c4|\n",
      "+-----+--------------------+----+----+----+\n",
      "|  ham|Go until jurong p...|null|null|null|\n",
      "|  ham|Ok lar... Joking ...|null|null|null|\n",
      "| spam|Free entry in 2 a...|null|null|null|\n",
      "|  ham|U dun say so earl...|null|null|null|\n",
      "|  ham|Nah I don't think...|null|null|null|\n",
      "| spam|FreeMsg Hey there...|null|null|null|\n",
      "|  ham|Even my brother i...|null|null|null|\n",
      "|  ham|As per your reque...|null|null|null|\n",
      "| spam|WINNER!! As a val...|null|null|null|\n",
      "| spam|Had your mobile 1...|null|null|null|\n",
      "|  ham|I'm gonna be home...|null|null|null|\n",
      "| spam|SIX chances to wi...|null|null|null|\n",
      "| spam|URGENT! You have ...|null|null|null|\n",
      "|  ham|I've been searchi...|null|null|null|\n",
      "|  ham|I HAVE A DATE ON ...|null|null|null|\n",
      "| spam|XXXMobileMovieClu...|null|null|null|\n",
      "|  ham|Oh k...i'm watchi...|null|null|null|\n",
      "|  ham|Eh u remember how...|null|null|null|\n",
      "|  ham|Fine if that��s t...|null|null|null|\n",
      "| spam|England v Macedon...|null|null|null|\n",
      "+-----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ini_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_select = SQLTransformer(statement = \"SELECT label, text FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_filter = SQLTransformer(statement = \"SELECT * from __THIS__ WHERE text is not null AND label is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"text_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(numFeatures = 1000, inputCol = \"text_token\", outputCol = \"text_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"text_tf\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline = Pipeline(stages=[sql_select, sql_filter, label_indexer, tokenizer, tf, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_model = ml_pipeline.fit(ini_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|                text|label_num|          text_token|             text_tf|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  ham|Go until jurong p...|      0.0|[go, until, juron...|(1000,[7,77,150,1...|(1000,[7,77,150,1...|[46.1925496142281...|[1.0,9.5478469531...|       0.0|\n",
      "|  ham|Ok lar... Joking ...|      0.0|[ok, lar..., joki...|(1000,[20,316,484...|(1000,[20,316,484...|[22.7239392220272...|[0.99999999999972...|       0.0|\n",
      "| spam|Free entry in 2 a...|      1.0|[free, entry, in,...|(1000,[30,35,73,1...|(1000,[30,35,73,1...|[-49.707099745267...|[5.78975520257700...|       1.0|\n",
      "|  ham|U dun say so earl...|      0.0|[u, dun, say, so,...|(1000,[57,368,372...|(1000,[57,368,372...|[51.7359225992089...|[1.0,5.3440271207...|       0.0|\n",
      "|  ham|Nah I don't think...|      0.0|[nah, i, don't, t...|(1000,[135,163,32...|(1000,[135,163,32...|[38.6006856216011...|[1.0,3.0544816603...|       0.0|\n",
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline_model.transform(ini_data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
