{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with MLlib\n",
    "\n",
    "In this Notebook, we will review the RDD-Based Machine Learning library MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MLlib\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "First, we have to understand the different data structures used by MLlib. In particular, they are:\n",
    "\n",
    "    * Vectors\n",
    "    * Labeled Points\n",
    "    * Rating\n",
    "    * Model Classes\n",
    "\n",
    "We will se `Vectors` and `Labeled Points` in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vector()` --> thold the features values. It can be `dense` and `sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dense = Vectors.dense([1.0,1.0,2.0,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sparse_1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sparse_2 = Vectors.sparse(4, [0, 2], [1.0, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint()` --> hold both features values and label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_point = LabeledPoint(1, vector_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "In this section, we will review the different algorithms associated with Machine Learning problems. Among other, we could highlight the following families of algorithms:\n",
    "\n",
    "    * Feature Extraction\n",
    "    * Statistics\n",
    "    * Classification and Regression\n",
    "    * Collaborative Filtering and Recommendation\n",
    "    * Dimensionality Reduction\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "ML algorithms only accept numerical values as inputs. Here, we discuss some algorithm that help us to translate some inputs (like text, non-scaled numerical vectors, etc) to numerical values that ML algorithms can understand. In particular, we will discuss the following algorithms:\n",
    "\n",
    "    * TD-IDF\n",
    "    * Scaling\n",
    "    * Normalization\n",
    "    * Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### td-idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`td-idf()` --> Term Frecuency - Inverse Document Frequency, useful to convert text input to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sc.parallelize([\"hello\", \"hello how are you\", \"good bye\", \"bye\"])\n",
    "words = sentences.map(lambda word: word.split(\" \"))\n",
    "tf = HashingTF(100)\n",
    "tf_vectors = tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {45: 1.0}),\n",
       " SparseVector(100, {1: 1.0, 21: 1.0, 24: 1.0, 45: 1.0}),\n",
       " SparseVector(100, {64: 1.0, 88: 1.0}),\n",
       " SparseVector(100, {88: 1.0})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF()\n",
    "idf_model = idf.fit(tf_vectors)\n",
    "tf_idf_vectors = idf_model.transform(tf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {45: 0.5108}),\n",
       " SparseVector(100, {1: 0.9163, 21: 0.9163, 24: 0.9163, 45: 0.5108}),\n",
       " SparseVector(100, {64: 0.9163, 88: 0.5108}),\n",
       " SparseVector(100, {88: 0.5108})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectors.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect\n",
    "\n",
    "`Word2Vec` --> also useful to tranform text into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec().setMinCount(0)\n",
    "word2vec_model = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectors = word2vec_model.transform(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0008, 0.0023, -0.0008, 0.0018, 0.0032, 0.0007, 0.0045, 0.004, -0.0012, -0.0022, 0.0036, -0.001, 0.0029, 0.0021, 0.0011, -0.0029, 0.0022, -0.004, 0.0014, -0.001, 0.0012, -0.0003, -0.0027, 0.004, 0.0026, 0.0023, -0.0038, -0.0011, 0.0037, -0.0025, 0.0021, -0.0009, 0.0042, -0.003, 0.0004, -0.0015, 0.0032, 0.001, 0.0023, 0.0016, 0.0032, 0.0013, -0.0022, -0.0, 0.0035, 0.0019, 0.0047, 0.0049, -0.0035, 0.0022, 0.0009, 0.001, -0.0047, 0.0008, 0.0045, -0.0005, 0.0001, -0.0013, -0.002, -0.0038, -0.004, -0.0006, 0.0039, -0.001, -0.002, -0.0035, 0.0032, 0.0016, 0.0018, 0.0024, -0.0036, 0.0031, 0.0014, -0.0033, 0.0049, 0.0037, -0.0028, -0.0024, -0.004, 0.0021, -0.0011, 0.0046, -0.0028, 0.0046, -0.0017, 0.0004, 0.0005, -0.0009, -0.0024, -0.0002, 0.0003, 0.0023, 0.0018, 0.0044, -0.0044, -0.004, 0.0043, 0.0011, 0.0016, 0.0048])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "While our input data could be already numeric, it is useful sometimes for the ML algorithms to scale that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler()` --> to scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [Vectors.dense([-2.0, 5.0, 1.0, 4.0]),\n",
    "           Vectors.dense([2.0, 0.0, 1.0, 7.2]),\n",
    "           Vectors.dense([4.0, 2.0, 0.5, 0.8])]\n",
    "\n",
    "vectors_rdd = sc.parallelize(vectors)\n",
    "scaler = StandardScaler(withMean=True, withStd=True)\n",
    "model = scaler.fit(vectors_rdd)\n",
    "scaled_data = model.transform(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-1.0911, 1.0596, 0.5774, 0.0]),\n",
       " DenseVector([0.2182, -0.9272, 0.5774, 1.0]),\n",
       " DenseVector([0.8729, -0.1325, -1.1547, -1.0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "As with scaling, sometimes it is very usefull to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "norm_data = norm.transform(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-0.2949, 0.7372, 0.1474, 0.5898]),\n",
       " DenseVector([0.2653, 0.0, 0.1326, 0.955]),\n",
       " DenseVector([0.8752, 0.4376, 0.1094, 0.175])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "The library MLlib includes useful functionalities to calculate some main statistics over numeric RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colStats()\n",
    "\n",
    "`colStats()` --> to calculate statistics over an RDD of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_stats = Statistics.colStats(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_stats_dict = {\n",
    "    \"count\": col_stats.count(),\n",
    "    \"max\": col_stats.max(),\n",
    "    \"mean\": col_stats.mean(),\n",
    "    \"min\": col_stats.min(),\n",
    "    \"normL1\": col_stats.normL1(),\n",
    "    \"normL2\": col_stats.normL2(),\n",
    "    \"numNonzeros\": col_stats.numNonzeros(),\n",
    "    \"variance\": col_stats.variance()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 3\n",
      "max: [ 4.   5.   1.   7.2]\n",
      "mean: [ 1.33333333  2.33333333  0.83333333  4.        ]\n",
      "min: [-2.   0.   0.5  0.8]\n",
      "normL1: [  8.    7.    2.5  12. ]\n",
      "normL2: [ 4.89897949  5.38516481  1.5         8.27526435]\n",
      "numNonzeros: [ 3.  2.  3.  3.]\n",
      "variance: [  9.33333333   6.33333333   0.08333333  10.24      ]\n"
     ]
    }
   ],
   "source": [
    "for key, value in col_stats_dict.items():\n",
    "    print(\"{0}: {1}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corr()\n",
    "\n",
    "`corr()` --> to calculate the correlation matrix between the columns of one RDD or between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.73704347, -0.75592895, -0.32732684],\n",
       "       [-0.73704347,  1.        ,  0.11470787, -0.39735971],\n",
       "       [-0.75592895,  0.11470787,  1.        ,  0.8660254 ],\n",
       "       [-0.32732684, -0.39735971,  0.8660254 ,  1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(vectors_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "data2 = sc.parallelize([10, 19, 32, 41, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996326893005933"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chiSqTest()\n",
    "\n",
    "`chiSqTest()` --> to compute the Pearson's independence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_point_rdd = vectors_rdd.map(lambda x: LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_test = Statistics.chiSqTest(label_point_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test value: 1.0: \n",
      "Test value: 1.0: \n",
      "Test value: 1.0: \n",
      "Test value: 1.0: \n"
     ]
    }
   ],
   "source": [
    "for test in chi_sq_test:\n",
    "    print(\"Test value: {0}: \".format(test.pValue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Regression\n",
    "\n",
    "In this section, we will explore the conventional Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, random\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create training data according to a Linear Regression model with the following weights and intercept:\n",
    "\n",
    "    * Weights: [5, 3, 8, 1]\n",
    "    * Intercept: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(x):\n",
    "    \"\"\"\n",
    "    Given an input vector x, returns the following value:\n",
    "    5*x[0] + 3*x[1] + 8*x[2] + x[3] + 20 + random()\n",
    "    \n",
    "    :input x: input vector\n",
    "    :return: computated value\n",
    "    \"\"\"\n",
    "    \n",
    "    return 5*x[0] + 3*x[1] + 8*x[2] + x[3] + 20 + random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_features = [[randint(0,20) for _ in range(4)] for _ in range(100)]\n",
    "reg_features_rdd = sc.parallelize(reg_features)\n",
    "scaler = StandardScaler()\n",
    "reg_features_scale = scaler.fit(reg_features_rdd).transform(reg_features_rdd)\n",
    "reg_data = reg_features_scale.map(lambda x: LabeledPoint(linear_reg(x), Vectors.dense(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(56.20769631921166, [1.53178195737,2.74840646972,2.41231590034,0.318865696145]),\n",
       " LabeledPoint(43.21709523468858, [1.19138596685,0.0,2.06769934315,0.159432848072])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been created, we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegressionWithSGD.train(data = reg_data, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the value of the original and computated weights and intercpet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed --> Weights: [3.50836115201,1.73364022695,6.31743781407,-0.276410685449]; Intercept: 14.306450618220648\n",
      "Original --> Weights: [5, 3, 8, 1]; Intercept: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Computed --> Weights: {0}; Intercept: {1}\".format(lr_model.weights, lr_model.intercept))\n",
    "print(\"Original --> Weights: {0}; Intercept: {1}\".format([5, 3, 8, 1], 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Classification\n",
    "\n",
    "In this section, we will explore different classification models:\n",
    "\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines (SVMs)\n",
    "    * Naive Bayes\n",
    "    * Decision Trees\n",
    "    * Random Forests\n",
    "    \n",
    "For every case, we will try to solve the sampe problem: a model to classify messages into two groups: legitimate and Spam. For that, we will have first to preprocess some text data using come functionalities studied in previous sections of this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, SVMWithSGD, NaiveBayes\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_data = spark.read.csv(\"../data/spam.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+\n",
      "|label|                text| _c2| _c3| _c4|\n",
      "+-----+--------------------+----+----+----+\n",
      "|  ham|Go until jurong p...|null|null|null|\n",
      "|  ham|Ok lar... Joking ...|null|null|null|\n",
      "| spam|Free entry in 2 a...|null|null|null|\n",
      "|  ham|U dun say so earl...|null|null|null|\n",
      "|  ham|Nah I don't think...|null|null|null|\n",
      "| spam|FreeMsg Hey there...|null|null|null|\n",
      "|  ham|Even my brother i...|null|null|null|\n",
      "|  ham|As per your reque...|null|null|null|\n",
      "| spam|WINNER!! As a val...|null|null|null|\n",
      "| spam|Had your mobile 1...|null|null|null|\n",
      "|  ham|I'm gonna be home...|null|null|null|\n",
      "| spam|SIX chances to wi...|null|null|null|\n",
      "| spam|URGENT! You have ...|null|null|null|\n",
      "|  ham|I've been searchi...|null|null|null|\n",
      "|  ham|I HAVE A DATE ON ...|null|null|null|\n",
      "| spam|XXXMobileMovieClu...|null|null|null|\n",
      "|  ham|Oh k...i'm watchi...|null|null|null|\n",
      "|  ham|Eh u remember how...|null|null|null|\n",
      "|  ham|Fine if that��s t...|null|null|null|\n",
      "| spam|England v Macedon...|null|null|null|\n",
      "+-----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ini_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_data_rdd = ini_data.select([\"label\", \"text\"]).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='ham', text='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd.take(1)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_data_rdd_filter = ini_data_rdd.filter(lambda row: (isinstance(row.label, str) and isinstance(row.text, str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5573"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_data_rdd_filter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = ini_data_rdd_filter.map(lambda row: row.text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = HashingTF(1000)\n",
    "tf_vectors = tf.transform(text_rdd)\n",
    "idf = IDF()\n",
    "idf_model = idf.fit(tf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_text = ini_data_rdd_filter.filter(lambda row: row.label == \"spam\").map(lambda row: row.text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Free',\n",
       "  'entry',\n",
       "  'in',\n",
       "  '2',\n",
       "  'a',\n",
       "  'wkly',\n",
       "  'comp',\n",
       "  'to',\n",
       "  'win',\n",
       "  'FA',\n",
       "  'Cup',\n",
       "  'final',\n",
       "  'tkts',\n",
       "  '21st',\n",
       "  'May',\n",
       "  '2005.',\n",
       "  'Text',\n",
       "  'FA',\n",
       "  'to',\n",
       "  '87121',\n",
       "  'to',\n",
       "  'receive',\n",
       "  'entry',\n",
       "  'question(std',\n",
       "  'txt',\n",
       "  \"rate)T&C's\",\n",
       "  'apply',\n",
       "  \"08452810075over18's\"],\n",
       " ['FreeMsg',\n",
       "  'Hey',\n",
       "  'there',\n",
       "  'darling',\n",
       "  \"it's\",\n",
       "  'been',\n",
       "  '3',\n",
       "  \"week's\",\n",
       "  'now',\n",
       "  'and',\n",
       "  'no',\n",
       "  'word',\n",
       "  'back!',\n",
       "  \"I'd\",\n",
       "  'like',\n",
       "  'some',\n",
       "  'fun',\n",
       "  'you',\n",
       "  'up',\n",
       "  'for',\n",
       "  'it',\n",
       "  'still?',\n",
       "  'Tb',\n",
       "  'ok!',\n",
       "  'XxX',\n",
       "  'std',\n",
       "  'chgs',\n",
       "  'to',\n",
       "  'send,',\n",
       "  '�1.50',\n",
       "  'to',\n",
       "  'rcv'],\n",
       " ['WINNER!!',\n",
       "  'As',\n",
       "  'a',\n",
       "  'valued',\n",
       "  'network',\n",
       "  'customer',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'selected',\n",
       "  'to',\n",
       "  'receivea',\n",
       "  '�900',\n",
       "  'prize',\n",
       "  'reward!',\n",
       "  'To',\n",
       "  'claim',\n",
       "  'call',\n",
       "  '09061701461.',\n",
       "  'Claim',\n",
       "  'code',\n",
       "  'KL341.',\n",
       "  'Valid',\n",
       "  '12',\n",
       "  'hours',\n",
       "  'only.']]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_text.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = ini_data_rdd_filter.filter(lambda row: row.label != \"spam\").map(lambda row: row.text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4826"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Go',\n",
       "  'until',\n",
       "  'jurong',\n",
       "  'point,',\n",
       "  'crazy..',\n",
       "  'Available',\n",
       "  'only',\n",
       "  'in',\n",
       "  'bugis',\n",
       "  'n',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'e',\n",
       "  'buffet...',\n",
       "  'Cine',\n",
       "  'there',\n",
       "  'got',\n",
       "  'amore',\n",
       "  'wat...'],\n",
       " ['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...'],\n",
       " ['U',\n",
       "  'dun',\n",
       "  'say',\n",
       "  'so',\n",
       "  'early',\n",
       "  'hor...',\n",
       "  'U',\n",
       "  'c',\n",
       "  'already',\n",
       "  'then',\n",
       "  'say...']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_vectors = tf.transform(spam_text)\n",
    "spam_idf = idf_model.transform(spam_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1000, {4: 4.2564, 52: 1.9576, 162: 3.663, 261: 5.407, 289: 1.5941, 309: 9.9766, 359: 5.2937, 365: 3.6159, 368: 4.8647, 389: 4.2314, 408: 4.16, 505: 5.0423, 524: 9.8246, 542: 4.8882, 547: 2.8359, 569: 3.8809, 571: 5.2586, 588: 4.937, 627: 2.9288, 633: 4.467, 648: 4.4212, 655: 3.602, 665: 4.2951, 783: 2.6097})]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_idf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vectors = tf.transform(gen_text)\n",
    "gen_idf = idf_model.transform(gen_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1000, {14: 4.8417, 17: 4.1041, 41: 4.8417, 52: 1.9576, 66: 4.9123, 84: 6.8547, 97: 5.0423, 125: 5.015, 501: 3.7976, 604: 2.5278, 606: 4.467, 657: 4.3218, 668: 3.3376, 683: 3.4329, 708: 5.1919, 802: 4.5828, 914: 5.0995, 932: 4.5828, 993: 4.072})]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_idf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_points = spam_idf.map(lambda x: LabeledPoint(1, x))\n",
    "gen_points = gen_idf.map(lambda x: LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (1000,[4,52,162,261,289,309,359,365,368,389,408,505,524,542,547,569,571,588,627,633,648,655,665,783],[4.25642035557,1.95763995962,3.66302357778,5.40699238317,1.59412694928,9.97656409663,5.29366369787,3.61586790789,4.86466809235,4.23141905337,4.15996008939,5.04234926959,9.82459228268,4.88819858976,2.83590803714,3.88093607968,5.25857237806,4.93698875393,2.92877472154,4.46698512468,4.42117558865,3.6019876872,4.29513486776,2.60971104834]))]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_points.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (1000,[14,17,41,52,66,84,97,125,501,604,606,657,668,683,708,802,914,932,993],[4.84167857412,4.10407963099,4.84167857412,1.95763995962,4.91229614134,6.85474235355,5.04234926959,5.0149502954,3.79755447074,2.52779392588,4.46698512468,4.32180311484,3.33760117735,3.43291135715,5.19188100356,4.58281694021,5.09950768343,4.58281694021,4.07199131644]))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_points.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_ini = spam_points.union(gen_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = ml_data_ini.map(lambda row: (randint(0,100), row)).sortByKey().map(lambda row: row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data.map(lambda x: x.label).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_train, ml_data_test = ml_data.randomSplit(weights = [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1481] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_train.cache()\n",
    "ml_data_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4451"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(1000, {60: 4.2821, 70: 4.3218, 80: 2.2979, 146: 4.8192, 186: 3.2277, 222: 2.4774, 234: 3.7817, 242: 3.1246, 299: 4.1041, 300: 2.4605, 303: 4.6005, 312: 4.8192, 317: 1.7434, 321: 1.4815, 349: 4.4827, 365: 1.2053, 467: 3.7976, 689: 4.6746, 791: 4.5828, 837: 4.9123, 868: 4.6185, 870: 2.1097, 887: 4.1951, 903: 2.6546, 912: 4.4987})"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_data_test.take(1)[0].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegressionWithSGD.train(data=ml_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1\n",
      "Actual label: 1.0; Prediction: 1\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, lr_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suport Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVMWithSGD.train(data=ml_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1\n",
      "Actual label: 1.0; Prediction: 1\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n",
      "Actual label: 0.0; Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, svm_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = NaiveBayes.train(data=ml_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, nb_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTree.trainClassifier(data=ml_data_train, numClasses = 2, categoricalFeaturesInfo={},\n",
    "                                          maxDepth=15, maxBins=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, tree_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForest.trainClassifier(data=ml_data_train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                            maxDepth=15, maxBins=64, numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for data in ml_data_test.take(10):\n",
    "    print(\"Actual label: {0}; Prediction: {1}\".format(data.label, forest_model.predict(data.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
