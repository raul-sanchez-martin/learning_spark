{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with MLlib\n",
    "\n",
    "In this Notebook, we will review the RDD-Based Machine Learning library MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "First, we have to understand the different data structures used by MLlib. In particular, they are:\n",
    "\n",
    "    * Vectors\n",
    "    * Labeled Points\n",
    "    * Rating\n",
    "    * Model Classes\n",
    "    \n",
    "We will se `Vectors` and `Labeled Points` in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vector()` --> to hold the features values. It can be `dense` and `sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorDense = [1.0,1.0,2.0,2.0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.0,1.0,2.0,2.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorDense = Vectors.dense(Array(1.0,1.0,2.0,2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorSparse = (4,[0,2],[1.0,2.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4,[0,2],[1.0,2.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorSparse = Vectors.sparse(4, Array(0, 2), Array(1.0, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint()` --> hold both features values and label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelPoint = (1.0,[1.0,1.0,2.0,2.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0,[1.0,1.0,2.0,2.0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelPoint = LabeledPoint(1, vectorDense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "In this section, we will review the different algorithms associated with Machine Learning problems. Among other, we could highlight the following families of algorithms:\n",
    "\n",
    "    * Feature Extraction\n",
    "    * Statistics\n",
    "    * Classification and Regression\n",
    "    * Collaborative Filtering and Recommendation\n",
    "    * Dimensionality Reduction\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "ML algorithms only accept numerical values as inputs. Here, we discuss some algorithm that help us to translate some inputs (like text, non-scaled numerical vectors, etc) to numerical values that ML algorithms can understand. In particular, we will discuss the following algorithms:\n",
    "\n",
    "    * TD-IDF\n",
    "    * Scaling\n",
    "    * Normalization\n",
    "    * Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### td-idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`td-idf()` --> Term Frecuency - Inverse Document Frequency, useful to convert text input to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.{HashingTF, IDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentences = ParallelCollectionRDD[0] at parallelize at <console>:30\n",
       "words = MapPartitionsRDD[1] at map at <console>:31\n",
       "tf = org.apache.spark.mllib.feature.HashingTF@2702df19\n",
       "tfVectors = MapPartitionsRDD[2] at map at HashingTF.scala:120\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at map at HashingTF.scala:120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sentences = sc.parallelize(Array(\"hello\", \"hello how are you\", \"good bye\", \"bye\"))\n",
    "val words = sentences.map(_.split(\" \").toSeq)\n",
    "val tf = new HashingTF(100)\n",
    "val tfVectors = tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100,[48],[1.0]), (100,[25,37,38,48],[1.0,1.0,1.0,1.0]), (100,[5,68],[1.0,1.0]), (100,[5],[1.0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfVectors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf = org.apache.spark.mllib.feature.IDF@2df6d0cd\n",
       "idfModel = org.apache.spark.mllib.feature.IDFModel@5b423390\n",
       "tfIdfVectors = MapPartitionsRDD[7] at mapPartitions at IDF.scala:178\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at mapPartitions at IDF.scala:178"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF()\n",
    "val idfModel = idf.fit(tfVectors)\n",
    "val tfIdfVectors = idfModel.transform(tfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100,[48],[0.5108256237659907]), (100,[25,37,38,48],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.5108256237659907]), (100,[5,68],[0.5108256237659907,0.9162907318741551]), (100,[5],[0.5108256237659907])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfVectors.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect\n",
    "\n",
    "`Word2Vec` --> also useful to tranform text into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vec = org.apache.spark.mllib.feature.Word2Vec@569b130\n",
       "word2vecModel = org.apache.spark.mllib.feature.Word2VecModel@1eb73e35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.Word2VecModel@1eb73e35"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word2vec = new Word2Vec().setMinCount(0)\n",
    "val word2vecModel = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vecVectors = [0.004266463220119476,0.003502710023894906,0.002203498501330614,0.0018562937621027231,-8.637388236820698E-4,0.002249166602268815,0.0012658028863370419,0.0013508968986570835,0.002924516098573804,0.004383440129458904,-7.926452672109008E-4,-0.002415268449112773,0.0041963099502027035,-0.002809106605127454,-2.040117105934769E-4,0.0025048169773072004,0.002964975079521537,0.004649922251701355,-0.0030415733344852924,-0.001955014420673251,0.003540246980264783,0.004680196288973093,-0.002487494144588709,0.004491603467613459,4.556482599582523E-4,-0.003490323666483164,0.0037351890932768583,0.0012482206802815199,8.974045631475747E-4,-0.00364275393076241,4.5830095768906176E-4,0.004314391873776913,6.77360367262736E-5,0.0029419169295579195,-0.00169...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.004266463220119476,0.003502710023894906,0.002203498501330614,0.0018562937621027231,-8.637388236820698E-4,0.002249166602268815,0.0012658028863370419,0.0013508968986570835,0.002924516098573804,0.004383440129458904,-7.926452672109008E-4,-0.002415268449112773,0.0041963099502027035,-0.002809106605127454,-2.040117105934769E-4,0.0025048169773072004,0.002964975079521537,0.004649922251701355,-0.0030415733344852924,-0.001955014420673251,0.003540246980264783,0.004680196288973093,-0.002487494144588709,0.004491603467613459,4.556482599582523E-4,-0.003490323666483164,0.0037351890932768583,0.0012482206802815199,8.974045631475747E-4,-0.00364275393076241,4.5830095768906176E-4,0.004314391873776913,6.77360367262736E-5,0.0029419169295579195,-0.0016973037272691727,0.00388229894451797,-0.003601585514843464,0.004590285010635853,9.515656856819987E-4,0.0027853359933942556,-0.004616547375917435,0.0022647911682724953,0.002998405136168003,0.002881892491132021,0.004812108352780342,-0.004743906203657389,0.0038659917190670967,4.819836249225773E-5,-0.0012907792115584016,-0.0014887454453855753,0.002538153901696205,0.0045069195330142975,-0.0027882405556738377,-0.003931839484721422,0.0028343205340206623,0.0016795787960290909,-0.003448444651439786,-0.0034580964129418135,7.541959639638662E-4,-0.0015497375279664993,-0.003482647705823183,8.906067232601345E-4,0.003972508478909731,-0.00484028784558177,-5.286823725327849E-4,0.003955578897148371,-0.0014348187251016498,0.0031838894356042147,-0.0013733237283304334,0.003021826734766364,0.0017759875627234578,-0.002125183353200555,0.002626419300213456,0.0025008919183164835,-0.0028808568604290485,-1.7421803931938484E-5,1.1071997141698375E-4,0.0018664533272385597,4.2021507397294044E-4,0.004271072801202536,0.0022985078394412994,-0.004487383645027876,0.004626242443919182,0.0043344260193407536,-0.0029898788779973984,-0.0046254112385213375,0.002213350497186184,-8.893145713955164E-4,0.004865651950240135,2.760587085504085E-4,4.6606757678091526E-4,0.004206806421279907,0.00104624277446419,-0.0024425634182989597,0.0019666594453155994,2.6795914163812995E-4,-0.0023727440275251865,-7.219725521281362E-4,0.0015413833316415548,0.003884983016178012]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word2vecVectors = word2vecModel.transform(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "While our input data could be already numeric, it is useful sometimes for the ML algorithms to scale that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler()` --> to scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectors = Array([-2.0,5.0,1.0,4.0], [2.0,0.0,1.0,7.2], [4.0,2.0,0.5,0.8])\n",
       "vectorsRdd = ParallelCollectionRDD[20] at parallelize at <console>:36\n",
       "scaler = org.apache.spark.mllib.feature.StandardScaler@371cefbc\n",
       "model = org.apache.spark.mllib.feature.StandardScalerModel@73dd2ccb\n",
       "scaledData = MapPartitionsRDD[25] at map at VectorTransformer.scala:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[25] at map at VectorTransformer.scala:52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectors = Array(Vectors.dense(Array(-2.0, 5.0, 1.0, 4.0)),\n",
    "                    Vectors.dense(Array(2.0, 0.0, 1.0, 7.2)),\n",
    "                    Vectors.dense(Array(4.0, 2.0, 0.5, 0.8)))\n",
    "\n",
    "val vectorsRdd = sc.parallelize(vectors)\n",
    "val scaler = new StandardScaler(withMean=true, withStd=true)\n",
    "val model = scaler.fit(vectorsRdd)\n",
    "val scaledData = model.transform(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.0910894511799618,1.0596258856520353,0.5773502691896257,0.0], [0.2182178902359923,-0.9271726499455306,0.5773502691896257,1.0], [0.8728715609439694,-0.13245323570650427,-1.1547005383792517,-1.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "As with scaling, sometimes it is very usefull to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm = org.apache.spark.mllib.feature.Normalizer@69e6d72b\n",
       "normData = MapPartitionsRDD[26] at map at VectorTransformer.scala:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at map at VectorTransformer.scala:52"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val norm = new Normalizer()\n",
    "val normData = norm.transform(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.29488391230979427,0.7372097807744856,0.14744195615489714,0.5897678246195885], [0.2652790545386455,0.0,0.13263952726932274,0.9550045963391238], [0.8751666735874727,0.43758333679373634,0.10939583419843409,0.17503333471749455]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "The library MLlib includes useful functionalities to calculate some main statistics over numeric RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.stat.Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colStats()\n",
    "\n",
    "`colStats()` --> to calculate statistics over an RDD of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colStats = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@46765e4b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@46765e4b"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colStats = Statistics.colStats(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colStatsMap = Map(count -> 3, variance -> [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002], mean -> [1.3333333333333335,2.333333333333333,0.8333333333333334,4.0], numNonzeros -> [3.0,2.0,3.0,3.0], min -> [-2.0,0.0,0.5,0.8], normL1 -> [8.0,7.0,2.5,12.0], normL2 -> [4.898979485566356,5.385164807134504,1.5,8.27526434623088], max -> [4.0,5.0,1.0,7.2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(count -> 3, variance -> [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002], mean -> [1.3333333333333335,2.333333333333333,0.8333333333333334,4.0], numNonzeros -> [3.0,2.0,3.0,3.0], min -> [-2.0,0.0,0.5,0.8], normL1 -> [8.0,7.0,2.5,12.0], normL2 -> [4.898979485566356,5.385164807134504,1.5,8.27526434623088], max -> [4.0,5.0,1.0,7.2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colStatsMap = Map(\"count\" -> colStats.count, \n",
    "                      \"max\" -> colStats.max,\n",
    "                      \"mean\" -> colStats.mean,\n",
    "                      \"min\" -> colStats.min,\n",
    "                      \"normL1\" -> colStats.normL1,\n",
    "                      \"normL2\" -> colStats.normL2,\n",
    "                      \"numNonzeros\" -> colStats.numNonzeros,\n",
    "                      \"variance\" -> colStats.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 3\n",
      "variance: [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002]\n",
      "mean: [1.3333333333333335,2.333333333333333,0.8333333333333334,4.0]\n",
      "numNonzeros: [3.0,2.0,3.0,3.0]\n",
      "min: [-2.0,0.0,0.5,0.8]\n",
      "normL1: [8.0,7.0,2.5,12.0]\n",
      "normL2: [4.898979485566356,5.385164807134504,1.5,8.27526434623088]\n",
      "max: [4.0,5.0,1.0,7.2]\n"
     ]
    }
   ],
   "source": [
    "colStatsMap.foreach{case(key, value) => println(key + \": \" + value)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corr()\n",
    "\n",
    "`corr()` --> to calculate the correlation matrix between the columns of one RDD or between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0                   -0.7370434740955019   -0.7559289460184548  -0.32732683535398843  \n",
       "-0.7370434740955019   1.0                   0.11470786693528112  -0.39735970711951274  \n",
       "-0.7559289460184548   0.11470786693528112   1.0                  0.8660254037844397    \n",
       "-0.32732683535398843  -0.39735970711951274  0.8660254037844397   1.0                   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data1 = ParallelCollectionRDD[39] at parallelize at <console>:35\n",
       "data2 = ParallelCollectionRDD[40] at parallelize at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[40] at parallelize at <console>:36"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1: RDD[Double] = sc.parallelize(Array(1, 2, 3, 4, 5))\n",
    "val data2: RDD[Double] = sc.parallelize(Array(10, 19, 32, 41, 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996326893005933"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chiSqTest()\n",
    "\n",
    "`chiSqTest()` --> to compute the Pearson's independence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelPointRdd = MapPartitionsRDD[51] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[51] at map at <console>:38"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelPointRdd = vectorsRdd.map(x => LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSqTest = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesi...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSqTest = Statistics.chiSqTest(labelPointRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test value: 1.0\n",
      "Test value: 1.0\n",
      "Test value: 1.0\n",
      "Test value: 1.0\n"
     ]
    }
   ],
   "source": [
    "chiSqTest.foreach(x => println(\"Test value: \" + x.pValue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Regression\n",
    "\n",
    "In this section, we will explore the conventional Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randGenerator = java.util.Random@4ade2aef\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "java.util.Random@4ade2aef"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Random\n",
    "val randGenerator = new Random()\n",
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create training data according to a Linear Regression model with the following weights:\n",
    "\n",
    "    * Weights: [2.5, 1.25, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regFeatures = Vector(Vector(17, 9, 6, 14), Vector(17, 13, 12, 9), Vector(17, 12, 10, 16), Vector(11, 13, 19, 5), Vector(17, 10, 19, 3), Vector(10, 8, 18, 2), Vector(17, 12, 17, 7), Vector(14, 18, 3, 17), Vector(11, 12, 7, 8), Vector(0, 15, 9, 18), Vector(3, 4, 15, 0), Vector(12, 18, 10, 19), Vector(8, 17, 14, 12), Vector(13, 17, 14, 15), Vector(14, 17, 6, 11), Vector(1, 14, 19, 11), Vector(7, 4, 5, 8), Vector(4, 19, 5, 0), Vector(4, 0, 4, 13), Vector(10, 7, 5, 18), Vector(7, 17, 15, 10), Vector(19, 6, 8, 18), Vector(13, 5, 14, 13), Vector(8, 12, 18, 12), Vector(18, 3, 15, 17), Vector(4, 6, 13, 5), Vector(9, 19, 4, 6), Vector(18, 19, 13, 2), Vector(15, 0, 15, 2), Vector(19, 16, 14, 19), Vector(13, 17, 18, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Vector(17, 9, 6, 14), Vector(17, 13, 12, 9), Vector(17, 12, 10, 16), Vector(11, 13, 19, 5), Vector(17, 10, 19, 3), Vector(10, 8, 18, 2), Vector(17, 12, 17, 7), Vector(14, 18, 3, 17), Vector(11, 12, 7, 8), Vector(0, 15, 9, 18), Vector(3, 4, 15, 0), Vector(12, 18, 10, 19), Vector(8, 17, 14, 12), Vector(13, 17, 14, 15), Vector(14, 17, 6, 11), Vector(1, 14, 19, 11), Vector(7, 4, 5, 8), Vector(4, 19, 5, 0), Vector(4, 0, 4, 13), Vector(10, 7, 5, 18), Vector(7, 17, 15, 10), Vector(19, 6, 8, 18), Vector(13, 5, 14, 13), Vector(8, 12, 18, 12), Vector(18, 3, 15, 17), Vector(4, 6, 13, 5), Vector(9, 19, 4, 6), Vector(18, 19, 13, 2), Vector(15, 0, 15, 2), Vector(19, 16, 14, 19), Vector(13, 17, 18, 10), Vector(2, 8, 7, 4), Vector(8, 13, 14, 17), Vector(18, 16, 7, 18), Vector(9, 10, 5, 15), Vector(4, 11, 4, 19), Vector(7, 4, 10, 7), Vector(19, 15, 6, 13), Vector(6, 7, 5, 13), Vector(1, 17, 6, 7), Vector(12, 12, 6, 12), Vector(1, 9, 13, 4), Vector(17, 16, 12, 1), Vector(0, 19, 8, 5), Vector(8, 17, 7, 2), Vector(17, 2, 7, 7), Vector(5, 9, 0, 11), Vector(19, 17, 4, 5), Vector(14, 2, 6, 8), Vector(16, 16, 18, 14), Vector(4, 13, 5, 4), Vector(5, 3, 16, 0), Vector(1, 5, 15, 10), Vector(3, 3, 17, 10), Vector(13, 14, 3, 3), Vector(9, 14, 3, 3), Vector(18, 18, 11, 6), Vector(4, 6, 6, 19), Vector(18, 5, 15, 15), Vector(5, 2, 0, 5), Vector(10, 6, 6, 16), Vector(15, 16, 8, 11), Vector(2, 19, 14, 2), Vector(19, 17, 17, 3), Vector(2, 17, 12, 8), Vector(2, 3, 13, 17), Vector(16, 13, 7, 2), Vector(7, 10, 0, 18), Vector(4, 7, 0, 19), Vector(2, 1, 1, 14), Vector(9, 10, 0, 8), Vector(5, 10, 17, 18), Vector(3, 17, 10, 2), Vector(12, 15, 16, 18), Vector(5, 15, 0, 15), Vector(6, 5, 8, 0), Vector(10, 8, 0, 8), Vector(12, 12, 14, 19), Vector(2, 2, 2, 0), Vector(3, 12, 10, 8), Vector(19, 8, 17, 19), Vector(11, 6, 2, 0), Vector(6, 10, 3, 19), Vector(0, 12, 1, 11), Vector(0, 18, 14, 15), Vector(1, 4, 17, 2), Vector(7, 11, 3, 18), Vector(1, 12, 15, 6), Vector(17, 10, 13, 8), Vector(19, 7, 2, 18), Vector(6, 15, 8, 1), Vector(15, 14, 2, 4), Vector(3, 8, 12, 7), Vector(13, 4, 1, 3), Vector(11, 10, 8, 19), Vector(7, 10, 13, 18), Vector(6, 9, 13, 16), Vector(0, 19, 2, 15), Vector(17, 16, 6, 2), Vector(12, 1, 18, 17), Vector(7, 10, 0, 12), Vector(16, 13, 2, 3), Vector(15, 2, 9, 15), Vector(17, 14, 13, 17), Vector(5, 3, 10, 4), Vector(5, 10, 9, 13), Vector(8, 0, 7, 9), Vector(19, 15, 3, 15), Vector(6, 8, 2, 11), Vector(14, 19, 8, 12), Vector(5, 3, 5, 17), Vector(6, 4, 16, 4), Vector(10, 19, 11, 11), Vector(16, 16, 11, 15), Vector(14, 19, 0, 15), Vector(12, 9, 9, 7), Vector(3, 19, 7, 12), Vector(10, 9, 3, 3), Vector(1, 9, 14, 1), Vector(0, 15, 4, 17), Vector(18, 9, 13, 6), Vector(4, 16, 1, 13), Vector(8, 10, 12, 19), Vector(15, 11, 18, 11), Vector(15, 14, 11, 18), Vector(14, 12, 11, 12), Vector(9, 9, 17, 8), Vector(5, 12, 3, 2), Vector(4, 18, 4, 4), Vector(8, 19, 1, 11), Vector(17, 0, 17, 7), Vector(0, 12, 19, 13), Vector(16, 14, 6, 10), Vector(4, 9, 13, 8), Vector(0, 18, 5, 6), Vector(2, 19, 8, 1), Vector(5, 13, 0, 6), Vector(1, 1, 5, 17), Vector(9, 9, 0, 7), Vector(15, 18, 13, 3), Vector(6, 9, 11, 11), Vector(6, 9, 0, 13), Vector(11, 0, 16, 2), Vector(0, 4, 11, 8), Vector(12, 2, 0, 16), Vector(8, 18, 16, 2), Vector(5, 1, 4, 12), Vector(3, 19, 19, 7), Vector(7, 5, 19, 2), Vector(10, 7, 16, 5), Vector(4, 4, 10, 5), Vector(5, 2, 13, 9), Vector(10, 6, 4, 17), Vector(7, 14, 10, 8), Vector(12, 13, 10, 9), Vector(14, 0, 5, 15), Vector(18, 1, 10, 2), Vector(5, 18, 9, 10), Vector(19, 17, 1, 10), Vector(10, 7, 0, 18), Vector(6, 12, 18, 17), Vector(15, 0, 11, 2), Vector(7, 4, 6, 3), Vector(10, 12, 3, 17), Vector(8, 18, 3, 12), Vector(15, 13, 11, 10), Vector(3, 7, 15, 10), Vector(19, 9, 2, 6), Vector(12, 16, 4, 13), Vector(12, 9, 3, 16), Vector(17, 2, 3, 2), Vector(17, 13, 9, 0), Vector(19, 18, 3, 11), Vector(4, 9, 10, 19), Vector(12, 14, 17, 8), Vector(3, 5, 2, 16), Vector(13, 14, 11, 17), Vector(6, 1, 7, 14), Vector(14, 16, 2, 9), Vector(9, 13, 15, 3), Vector(7, 3, 9, 8), Vector(4, 14, 19, 19), Vector(14, 11, 6, 13), Vector(0, 2, 4, 18), Vector(16, 19, 10, 4), Vector(9, 8, 9, 4), Vector(3, 8, 15, 9), Vector(7, 13, 14, 16), Vector(2, 1, 7, 2), Vector(15, 19, 5, 12), Vector(6, 13, 15, 16), Vector(1, 6, 12, 7), Vector(5, 18, 18, 7), Vector(1, 17, 11, 3), Vector(7, 1, 1, 17), Vector(15, 5, 16, 10), Vector(11, 2, 7, 14), Vector(13, 18, 11, 0), Vector(4, 12, 16, 10), Vector(1, 19, 6, 18), Vector(13, 15, 11, 9), Vector(17, 9, 0, 1), Vector(7, 0, 13, 18), Vector(17, 4, 7, 1), Vector(7, 0, 5, 11), Vector(9, 17, 11, 14), Vector(15, 9, 12, 15), Vector(10, 14, 10, 5), Vector(8, 10, 0, 14), Vector(15, 12, 8, 19), Vector(10, 15, 7, 7), Vector(1, 4, 14, 4), Vector(4, 14, 2, 10), Vector(17, 9, 9, 17), Vector(3, 2, 4, 2), Vector(18, 15, 1, 15), Vector(0, 0, 6, 12), Vector(1, 6, 11, 17), Vector(12, 13, 10, 8), Vector(19, 7, 5, 17), Vector(2, 4, 14, 15), Vector(15, 9, 15, 15), Vector(5, 18, 11, 15), Vector(14, 9, 10, 8), Vector(7, 12, 13, 0), Vector(17, 3, 14, 15), Vector(18, 18, 19, 19), Vector(7, 14, 5, 19), Vector(0, 15, 5, 13), Vector(0, 15, 10, 7), Vector(8, 2, 3, 3), Vector(3, 9, 13, 8), Vector(1, 10, 0, 1), Vector(17, 10, 15, 6), Vector(16, 10, 9, 9), Vector(8, 1, 8, 14), Vector(10, 6, 18, 4), Vector(13, 17, 19, 8), Vector(16, 0, 14, 18), Vector(3, 4, 14, 7), Vector(9, 19, 4, 16), Vector(6, 0, 12, 0), Vector(0, 10, 6, 13), Vector(16, 3, 8, 14), Vector(1, 13, 18, 5), Vector(16, 8, 17, 4), Vector(10, 11, 0, 15), Vector(11, 3, 9, 16), Vector(19, 14, 17, 16), Vector(0, 17, 0, 6), Vector(8, 1, 16, 13), Vector(17, 9, 0, 19), Vector(16, 9, 16, 8), Vector(18, 4, 16, 10), Vector(18, 11, 6, 15), Vector(15, 1, 6, 9), Vector(4, 19, 18, 15), Vector(19, 9, 11, 4), Vector(16, 16, 3, 1), Vector(13, 7, 0, 17), Vector(11, 17, 13, 8), Vector(11, 9, 12, 10), Vector(2, 0, 0, 5), Vector(6, 17, 14, 9), Vector(3, 14, 4, 9), Vector(11, 0, 13, 19), Vector(11, 13, 4, 6), Vector(18, 17, 4, 16), Vector(19, 13, 9, 18), Vector(15, 18, 17, 12), Vector(8, 11, 18, 1), Vector(14, 6, 19, 16), Vector(5, 8, 6, 15), Vector(8, 5, 6, 1), Vector(9, 14, 15, 3), Vector(13, 3, 8, 16), Vector(3, 1, 3, 9), Vector(1, 16, 16, 16), Vector(16, 19, 15, 17), Vector(14, 5, 11, 11), Vector(13, 12, 14, 13), Vector(2, 13, 19, 8), Vector(7, 18, 12, 17), Vector(14, 8, 12, 1), Vector(12, 2, 4, 10), Vector(11, 3, 19, 6), Vector(9, 4, 8, 6), Vector(9, 10, 9, 0), Vector(15, 6, 10, 6), Vector(12, 0, 7, 18), Vector(0, 2, 2, 8), Vector(1, 13, 9, 19), Vector(1, 7, 8, 4), Vector(16, 7, 16, 18), Vector(15, 7, 17, 3), Vector(17, 0, 17, 16), Vector(16, 7, 2, 14), Vector(4, 13, 13, 5), Vector(1, 9, 10, 19), Vector(6, 14, 18, 6), Vector(19, 12, 10, 4), Vector(19, 12, 6, 12), Vector(7, 19, 13, 13), Vector(4, 16, 12, 3), Vector(11, 2, 19, 6), Vector(8, 6, 7, 19), Vector(5, 19, 11, 3), Vector(17, 15, 10, 14), Vector(9, 9, 4, 9), Vector(1, 1, 1, 12), Vector(0, 8, 17, 15), Vector(7, 11, 4, 5), Vector(17, 11, 1, 6), Vector(17, 17, 8, 2), Vector(0, 4, 7, 3), Vector(6, 4, 11, 16), Vector(3, 19, 15, 8), Vector(19, 3, 1, 6), Vector(19, 11, 4, 7), Vector(6, 10, 11, 18), Vector(13, 10, 8, 9), Vector(4, 11, 13, 16), Vector(7, 13, 11, 11), Vector(16, 4, 5, 10), Vector(10, 1, 2, 16), Vector(7, 2, 14, 2), Vector(15, 14, 11, 13), Vector(5, 12, 12, 15), Vector(8, 12, 11, 3), Vector(7, 4, 12, 7), Vector(5, 12, 14, 4), Vector(1, 4, 10, 5), Vector(5, 0, 19, 10), Vector(14, 10, 17, 12), Vector(4, 15, 10, 18), Vector(4, 8, 6, 2), Vector(16, 16, 15, 18), Vector(3, 15, 3, 2), Vector(10, 16, 18, 6), Vector(3, 16, 11, 9), Vector(16, 2, 14, 10), Vector(12, 14, 8, 12), Vector(7, 2, 0, 13), Vector(3, 17, 8, 16), Vector(4, 2, 11, 15), Vector(8, 14, 12, 12), Vector(3, 6, 16, 8), Vector(15, 3, 9, 15), Vector(8, 17, 17, 7), Vector(5, 3, 13, 18), Vector(7, 1, 15, 7), Vector(2, 8, 19, 13), Vector(17, 11, 5, 17), Vector(18, 19, 17, 3), Vector(8, 0, 15, 3), Vector(2, 19, 7, 9), Vector(5, 8, 10, 17), Vector(13, 17, 11, 3), Vector(12, 14, 16, 12), Vector(17, 13, 16, 14), Vector(17, 10, 18, 1), Vector(0, 7, 9, 18), Vector(4, 5, 4, 6), Vector(10, 10, 17, 4), Vector(2, 16, 13, 17), Vector(19, 12, 6, 13), Vector(7, 0, 6, 15), Vector(15, 11, 12, 17), Vector(12, 11, 5, 18), Vector(4, 1, 17, 13), Vector(7, 9, 15, 2), Vector(8, 5, 7, 17), Vector(11, 11, 16, 3), Vector(17, 8, 7, 9), Vector(16, 16, 11, 17), Vector(6, 4, 5, 0), Vector(5, 14, 8, 2), Vector(5, 11, 7, 19), Vector(5, 1, 12, 11), Vector(4, 16, 17, 18), Vector(12, 3, 1, 14), Vector(10, 16, 16, 4), Vector(6, 8, 12, 13), Vector(4, 10, 13, 1), Vector(4, 4, 8, 5), Vector(14, 3, 13, 14), Vector(3, 8, 6, 19), Vector(5, 2, 0, 17), Vector(14, 4, 16, 7), Vector(9, 13, 11, 16), Vector(9, 6, 0, 15), Vector(14, 10, 19, 10), Vector(18, 12, 4, 8), Vector(9, 2, 5, 1), Vector(9, 10, 13, 17), Vector(9, 1, 6, 12), Vector(9, 12, 6, 17), Vector(11, 7, 18, 6), Vector(18, 2, 8, 1), Vector(1, 0, 15, 5), Vector(14, 10, 17, 6), Vector(8, 16, 9, 8), Vector(6, 15, 7, 15), Vector(15, 15, 1, 5), Vector(19, 5, 11, 13), Vector(13, 6, 19, 7), Vector(12, 6, 8, 12), Vector(3, 19, 14, 0), Vector(4, 15, 11, 8), Vector(4, 16, 0, 19), Vector(2, 12, 14, 5), Vector(2, 13, 8, 14), Vector(11, 10, 7, 2), Vector(1, 17, 5, 1), Vector(3, 16, 3, 2), Vector(13, 8, 14, 1), Vector(13, 1, 5, 2), Vector(9, 4, 14, 6), Vector(13, 12, 2, 12), Vector(4, 2, 7, 13), Vector(2, 4, 13, 1), Vector(4, 9, 1, 6), Vector(4, 11, 14, 19), Vector(3, 11, 17, 6), Vector(12, 6, 13, 15), Vector(3, 6, 19, 13), Vector(9, 11, 13, 16), Vector(19, 16, 18, 9), Vector(18, 14, 6, 1), Vector(18, 14, 13, 4), Vector(18, 4, 13, 7), Vector(8, 16, 14, 6), Vector(18, 14, 12, 7), Vector(14, 11, 16, 16), Vector(19, 10, 1, 0), Vector(17, 17, 14, 1), Vector(7, 7, 2, 15), Vector(15, 0, 11, 13), Vector(6, 13, 7, 12), Vector(14, 11, 5, 0), Vector(18, 3, 13, 15), Vector(10, 19, 4, 15), Vector(5, 3, 7, 1), Vector(4, 10, 9, 11), Vector(16, 18, 8, 14), Vector(11, 7, 5, 11), Vector(19, 0, 5, 1), Vector(0, 19, 18, 18), Vector(5, 3, 10, 11), Vector(19, 1, 18, 2), Vector(18, 5, 14, 18), Vector(5, 11, 5, 14), Vector(9, 12, 1, 15), Vector(9, 14, 17, 4), Vector(17, 9, 11, 12), Vector(1, 18, 1, 7), Vector(10, 13, 19, 14), Vector(4, 2, 0, 5), Vector(7, 5, 7, 15), Vector(0, 8, 13, 2), Vector(14, 7, 0, 13), Vector(7, 2, 6, 3), Vector(13, 10, 0, 9), Vector(15, 9, 14, 7), Vector(1, 6, 2, 17), Vector(3, 18, 17, 9), Vector(11, 9, 13, 2), Vector(13, 8, 1, 2), Vector(0, 18, 6, 17), Vector(19, 11, 4, 11), Vector(18, 5, 13, 1), Vector(4, 19, 6, 13), Vector(11, 1, 1, 3), Vector(18, 13, 14, 12), Vector(10, 12, 2, 8), Vector(16, 8, 1, 8), Vector(3, 10, 11, 3), Vector(11, 1, 8, 10), Vector(17, 9, 14, 4), Vector(17, 15, 12, 7), Vector(7, 18, 4, 5), Vector(19, 15, 17, 11), Vector(1, 18, 4, 16), Vector(16, 0, 1, 3), Vector(2, 1, 19, 2), Vector(13, 3, 8, 11), Vector(13, 2, 18, 5), Vector(3, 16, 16, 0), Vector(10, 17, 9, 18), Vector(8, 7, 8, 14), Vector(3, 7, 7, 3), Vector(19, 8, 18, 7), Vector(1, 13, 9, 6), Vector(14, 11, 18, 10), Vector(19, 5, 2, 0), Vector(4, 11, 2, 4), Vector(7, 7, 18, 19), Vector(10, 14, 17, 2), Vector(13, 4, 17, 4), Vector(3, 11, 14, 12))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val regFeatures = for(_ <- 1 to 500) yield {for (_ <- 1 to 4) yield randGenerator.nextInt(20)}\n",
    "val regFeaturesRdd = sc.parallelize(regFeatures).map(x => Vectors.dense(x.toArray.map(_.toDouble)))\n",
    "val scaler = new StandardScaler()\n",
    "val regFeaturesScale = scaler.fit(regFeaturesRdd).transform(regFeaturesRdd)\n",
    "val regData = regFeaturesScale.map(x => LabeledPoint({\n",
    "    val arrayValue = x.toArray\n",
    "    val randGenerator = new Random()\n",
    "    2.5*x(0) + 1.25*x(1) + 0.5*x(2) + x(3) + randGenerator.nextDouble\n",
    "},x))\n",
    "regData.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been created, we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numIterations = 10000\n",
       "stepSize = 0.1\n",
       "miniBatchFraction = 1.0\n",
       "lrModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numIterations = 10000\n",
    "val stepSize = 0.1\n",
    "val miniBatchFraction = 1.0\n",
    "val lrModel = LinearRegressionWithSGD.train(regData, numIterations = numIterations, \n",
    "                                            stepSize = stepSize, miniBatchFraction = miniBatchFraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the value of the original and computated weights and intercpet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed weights: [2.3334699312242164,1.3148656506876297,0.7281496791674907,1.1377969698586878]\n",
      "Original weights: [2.5, 1.25, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "println(\"Computed weights: \" + lrModel.weights)\n",
    "println(\"Original weights: [2.5, 1.25, 0.5, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Classification\n",
    "\n",
    "In this section, we will explore different classification models:\n",
    "\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines (SVMs)\n",
    "    * Naive Bayes\n",
    "    * Decision Trees\n",
    "    * Random Forests\n",
    "    \n",
    "For every case, we will try to solve the sampe problem: a model to classify messages into two groups: legitimate and Spam. For that, we will have first to preprocess some text data using come functionalities studied in previous sections of this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.classification.{LogisticRegressionWithSGD, SVMWithSGD, NaiveBayes}\n",
    "import org.apache.spark.mllib.tree.{DecisionTree, RandomForest}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniData = [label: string, text: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: string, text: string ... 3 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniData = spark.read.option(\"header\", \"true\").csv(\"../data/spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+\n",
      "|label|                text| _c2| _c3| _c4|\n",
      "+-----+--------------------+----+----+----+\n",
      "|  ham|Go until jurong p...|null|null|null|\n",
      "|  ham|Ok lar... Joking ...|null|null|null|\n",
      "| spam|Free entry in 2 a...|null|null|null|\n",
      "|  ham|U dun say so earl...|null|null|null|\n",
      "|  ham|Nah I don't think...|null|null|null|\n",
      "| spam|FreeMsg Hey there...|null|null|null|\n",
      "|  ham|Even my brother i...|null|null|null|\n",
      "|  ham|As per your reque...|null|null|null|\n",
      "| spam|WINNER!! As a val...|null|null|null|\n",
      "| spam|Had your mobile 1...|null|null|null|\n",
      "|  ham|I'm gonna be home...|null|null|null|\n",
      "| spam|SIX chances to wi...|null|null|null|\n",
      "| spam|URGENT! You have ...|null|null|null|\n",
      "|  ham|I've been searchi...|null|null|null|\n",
      "|  ham|I HAVE A DATE ON ...|null|null|null|\n",
      "| spam|XXXMobileMovieClu...|null|null|null|\n",
      "|  ham|Oh k...i'm watchi...|null|null|null|\n",
      "|  ham|Eh u remember how...|null|null|null|\n",
      "|  ham|Fine if that��s t...|null|null|null|\n",
      "| spam|England v Macedon...|null|null|null|\n",
      "+-----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iniData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniDataRdd = MapPartitionsRDD[517] at rdd at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[517] at rdd at <console>:41"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniDataRdd = iniData.select(\"label\", \"text\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>ham</td><td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----+-----------------------------------------------------------------------------------------------------------------+\n",
       "| ham | Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... |\n",
       "+-----+-----------------------------------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.take(1)(0)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iniDataRddFilter = iniDataRdd.filter(lambda row: (isinstance(row.label, str) and isinstance(row.text, str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniDataRddFilter = MapPartitionsRDD[520] at filter at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[520] at filter at <console>:43"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniDataRddFilter = iniDataRdd.filter(row => (row(0), row(1)) match {\n",
    "    case (key: String, value: String) => true\n",
    "    case _ => false\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5573"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRddFilter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textRdd = MapPartitionsRDD[540] at map at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[540] at map at <console>:45"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textRdd = iniDataRddFilter.map(row => row(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf = org.apache.spark.mllib.feature.HashingTF@449e4d4f\n",
       "tfVectors = MapPartitionsRDD[541] at map at <console>:52\n",
       "idf = org.apache.spark.mllib.feature.IDF@325a239e\n",
       "idfModel = org.apache.spark.mllib.feature.IDFModel@70a425ac\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.IDFModel@70a425ac"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = new HashingTF(1000)\n",
    "val tfVectors = textRdd.map(x => tf.transform(x.toString.split(\" \")))\n",
    "val idf = new IDF()\n",
    "val idfModel = idf.fit(tfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spamText = MapPartitionsRDD[537] at map at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[537] at map at <console>:45"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spamText = iniDataRddFilter.filter(_(0) == \"spam\").map(_(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamText.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's, FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv, WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamText.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genText = MapPartitionsRDD[539] at map at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[539] at map at <console>:45"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genText = iniDataRddFilter.filter(_(0) == \"ham\").map(_(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4825"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genText.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..., Ok lar... Joking wif u oni..., U dun say so early hor... U c already then say...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genText.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf2 = org.apache.spark.mllib.feature.HashingTF@39f15be7\n",
       "tfVectors = MapPartitionsRDD[544] at map at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[544] at map at <console>:50"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf2 = tf\n",
    "val tfVectors = textRdd.map(x => tf2.transform(x.toString.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfSpam = org.apache.spark.mllib.feature.HashingTF@449e4d4f\n",
       "idfModelSpam = org.apache.spark.mllib.feature.IDFModel@70a425ac\n",
       "spamVectors = MapPartitionsRDD[549] at map at <console>:60\n",
       "spamIdf = MapPartitionsRDD[550] at map at <console>:61\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[550] at map at <console>:61"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tfSpam = tf\n",
    "val idfModelSpam = idfModel\n",
    "val spamVectors = spamText.map(x => tfSpam.transform(x.toString.split(\" \")))\n",
    "val spamIdf = spamVectors.map(x => idfModelSpam.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000,[30,33,35,72,128,140,166,170,388,409,445,468,508,634,667,670,685,692,716,755,784,880,887,989],[5.3300313420375645,5.917818006939683,4.498733822996802,4.137231838309754,2.8732955692162605,4.713845202613747,4.406360502865787,1.4846230856914024,3.5413519540206178,4.531523645819793,2.0151721633241344,4.2951348677555625,4.600516517306744,5.191881003556747,5.160132305242167,3.3894262452119444,4.269159381352302,4.498733822996802,4.421175588650928,7.3400823008812655,5.3300313420375645,4.451480938146257,10.14104029310496,2.514400868539215])]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamIdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfGen = org.apache.spark.mllib.feature.HashingTF@449e4d4f\n",
       "idfModelGen = org.apache.spark.mllib.feature.IDFModel@70a425ac\n",
       "genVectors = MapPartitionsRDD[551] at map at <console>:56\n",
       "genIdf = MapPartitionsRDD[552] at map at <console>:57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[552] at map at <console>:57"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tfGen = tf\n",
    "val idfModelGen = idfModel\n",
    "val genVectors = genText.map(x => tfGen.transform(x.toString.split(\" \")))\n",
    "val genIdf = genVectors.map(x => idfModelGen.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000,[7,42,150,165,258,260,360,362,445,647,655,687,744,745,785,831,854,878,899,966],[3.2413731452528047,5.129360646575413,4.962306561912247,5.447814377693948,4.7972268115527985,4.020698022053802,5.099507683425732,4.531523645819793,2.0151721633241344,5.5348257546835775,3.8055866424368565,5.25857237805542,3.846744714930364,4.841678574123632,5.681429228875453,3.4001215343286924,5.917818006939683,3.698614522884689,3.4001215343286924,2.8177257180614497])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genIdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spamPoints = MapPartitionsRDD[553] at map at <console>:66\n",
       "genPoints = MapPartitionsRDD[554] at map at <console>:67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[554] at map at <console>:67"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spamPoints = spamIdf.map(x => LabeledPoint(1, x))\n",
    "val genPoints = genIdf.map(x => LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(1000,[30,33,35,72,128,140,166,170,388,409,445,468,508,634,667,670,685,692,716,755,784,880,887,989],[5.3300313420375645,5.917818006939683,4.498733822996802,4.137231838309754,2.8732955692162605,4.713845202613747,4.406360502865787,1.4846230856914024,3.5413519540206178,4.531523645819793,2.0151721633241344,4.2951348677555625,4.600516517306744,5.191881003556747,5.160132305242167,3.3894262452119444,4.269159381352302,4.498733822996802,4.421175588650928,7.3400823008812655,5.3300313420375645,4.451480938146257,10.14104029310496,2.514400868539215]))]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamPoints.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,(1000,[7,42,150,165,258,260,360,362,445,647,655,687,744,745,785,831,854,878,899,966],[3.2413731452528047,5.129360646575413,4.962306561912247,5.447814377693948,4.7972268115527985,4.020698022053802,5.099507683425732,4.531523645819793,2.0151721633241344,5.5348257546835775,3.8055866424368565,5.25857237805542,3.846744714930364,4.841678574123632,5.681429228875453,3.4001215343286924,5.917818006939683,3.698614522884689,3.4001215343286924,2.8177257180614497]))]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genPoints.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataIni = UnionRDD[555] at union at <console>:69\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[555] at union at <console>:69"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataIni = spamPoints.union(genPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randGenerator = java.util.Random@5abd44a0\n",
       "mlData = MapPartitionsRDD[569] at map at <console>:76\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[569] at map at <console>:76"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val randGenerator = new Random()\n",
    "randGenerator.nextInt(20)\n",
    "\n",
    "val mlData = mlDataIni.map(row => (randGenerator.nextInt(100), row)).sortByKey().map(_._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlData.map(_.label).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataTrainTest = Array(MapPartitionsRDD[577] at randomSplit at <console>:74, MapPartitionsRDD[578] at randomSplit at <console>:74)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[MapPartitionsRDD[577] at randomSplit at <console>:74, MapPartitionsRDD[578] at randomSplit at <console>:74]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataTrainTest = mlData.randomSplit(weights = Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataTrain = MapPartitionsRDD[577] at randomSplit at <console>:74\n",
       "mlDataTest = MapPartitionsRDD[578] at randomSplit at <console>:74\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[578] at randomSplit at <console>:74"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataTrain = mlDataTrainTest(0)\n",
    "val mlDataTest = mlDataTrainTest(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[578] at randomSplit at <console>:74"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrain.cache()\n",
    "mlDataTest.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4426"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrain.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,[48,73,91,146,166,170,187,207,216,303,334,364,388,425,431,447,497,517,529,624,652,696,706,712,818,851],[3.720593429603464,7.076543745619019,3.6842257854325893,2.6520585961726324,4.406360502865787,1.4846230856914024,2.619515048440161,4.321803114837723,4.421175588650928,4.912296141337586,3.9911392198122577,3.855183583576229,1.180450651340206,1.4997809347427686,8.783523406889268,3.6913942749112016,3.4610822341183796,4.207027600245295,4.713845202613747,3.8383764652598478,5.406992383173693,4.010747691200634,4.2820627861882095,3.3376011773473584,4.436213466015468,4.7972268115527985])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTest.take(1)(0).features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = new LogisticRegressionWithSGD().run(mlDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = lrModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suport Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svmModel = new SVMWithSGD().run(mlDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = svmModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nbModel = org.apache.spark.mllib.classification.NaiveBayesModel@18f17f5c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.NaiveBayesModel@18f17f5c"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nbModel = new NaiveBayes().run(mlDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = nbModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses = 2\n",
       "categoricalFeaturesInfo = Map()\n",
       "impurity = gini\n",
       "maxDepth = 15\n",
       "maxBins = 64\n",
       "treeModel = DecisionTreeModel classifier of depth 15 with 255 nodes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeModel classifier of depth 15 with 255 nodes"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo=Map[Int, Int]()\n",
    "val impurity=\"gini\"\n",
    "val maxDepth=15\n",
    "val maxBins=64\n",
    "\n",
    "\n",
    "val treeModel = DecisionTree.trainClassifier(input = mlDataTrain, numClasses = numClasses, \n",
    "                                             categoricalFeaturesInfo = categoricalFeaturesInfo,\n",
    "                                             impurity = impurity, maxDepth = maxDepth,\n",
    "                                             maxBins = maxBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = treeModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses = 2\n",
       "categoricalFeaturesInfo = Map()\n",
       "impurity = gini\n",
       "maxDepth = 15\n",
       "maxBins = 64\n",
       "featureSubsetStrategy = auto\n",
       "numTrees = 10\n",
       "forestModel = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TreeEnsembleModel classifier with 10 trees\n"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 15\n",
    "val maxBins = 64\n",
    "val featureSubsetStrategy = \"auto\"\n",
    "val numTrees = 10\n",
    "\n",
    "val forestModel = RandomForest.trainClassifier(input = mlDataTrain, numClasses = numClasses, \n",
    "                                             categoricalFeaturesInfo = categoricalFeaturesInfo, \n",
    "                                             impurity = impurity, maxDepth = maxDepth, \n",
    "                                             maxBins = maxBins, numTrees = numTrees,\n",
    "                                             featureSubsetStrategy = featureSubsetStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = forestModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
