{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with MLlib\n",
    "\n",
    "In this Notebook, we will review the RDD-Based Machine Learning library MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "First, we have to understand the different data structures used by MLlib. In particular, they are:\n",
    "\n",
    "    * Vectors\n",
    "    * Labeled Points\n",
    "    * Rating\n",
    "    * Model Classes\n",
    "    \n",
    "We will se `Vectors` and `Labeled Points` in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vector()` --> to hold the features values. It can be `dense` and `sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorDense = [1.0,1.0,2.0,2.0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.0,1.0,2.0,2.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorDense = Vectors.dense(Array(1.0,1.0,2.0,2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorSparse = (4,[0,2],[1.0,2.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4,[0,2],[1.0,2.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorSparse = Vectors.sparse(4, Array(0, 2), Array(1.0, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint()` --> hold both features values and label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelPoint = (1.0,[1.0,1.0,2.0,2.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0,[1.0,1.0,2.0,2.0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelPoint = LabeledPoint(1, vectorDense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "In this section, we will review the different algorithms associated with Machine Learning problems. Among other, we could highlight the following families of algorithms:\n",
    "\n",
    "    * Feature Extraction\n",
    "    * Statistics\n",
    "    * Classification and Regression\n",
    "    * Collaborative Filtering and Recommendation\n",
    "    * Dimensionality Reduction\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "ML algorithms only accept numerical values as inputs. Here, we discuss some algorithm that help us to translate some inputs (like text, non-scaled numerical vectors, etc) to numerical values that ML algorithms can understand. In particular, we will discuss the following algorithms:\n",
    "\n",
    "    * TD-IDF\n",
    "    * Scaling\n",
    "    * Normalization\n",
    "    * Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### td-idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`td-idf()` --> Term Frecuency - Inverse Document Frequency, useful to convert text input to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.{HashingTF, IDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentences = ParallelCollectionRDD[0] at parallelize at <console>:30\n",
       "words = MapPartitionsRDD[1] at map at <console>:31\n",
       "tf = org.apache.spark.mllib.feature.HashingTF@26f75ac0\n",
       "tfVectors = MapPartitionsRDD[2] at map at HashingTF.scala:120\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at map at HashingTF.scala:120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sentences = sc.parallelize(Array(\"hello\", \"hello how are you\", \"good bye\", \"bye\"))\n",
    "val words = sentences.map(_.split(\" \").toSeq)\n",
    "val tf = new HashingTF(100)\n",
    "val tfVectors = tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100,[48],[1.0]), (100,[25,37,38,48],[1.0,1.0,1.0,1.0]), (100,[5,68],[1.0,1.0]), (100,[5],[1.0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfVectors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf = org.apache.spark.mllib.feature.IDF@79a788d1\n",
       "idfModel = org.apache.spark.mllib.feature.IDFModel@40615a99\n",
       "tfIdfVectors = MapPartitionsRDD[7] at mapPartitions at IDF.scala:178\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at mapPartitions at IDF.scala:178"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF()\n",
    "val idfModel = idf.fit(tfVectors)\n",
    "val tfIdfVectors = idfModel.transform(tfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100,[48],[0.5108256237659907]), (100,[25,37,38,48],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.5108256237659907]), (100,[5,68],[0.5108256237659907,0.9162907318741551]), (100,[5],[0.5108256237659907])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfVectors.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect\n",
    "\n",
    "`Word2Vec` --> also useful to tranform text into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vec = org.apache.spark.mllib.feature.Word2Vec@5ffb76c3\n",
       "word2vecModel = org.apache.spark.mllib.feature.Word2VecModel@4ae04e59\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.Word2VecModel@4ae04e59"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word2vec = new Word2Vec().setMinCount(0)\n",
    "val word2vecModel = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vecVectors = [-0.002391571644693613,-0.004730306100100279,0.004567709285765886,-0.0021375345531851053,0.003772377735003829,-0.004304440226405859,-0.0035075515042990446,-0.002512869192287326,0.0043669031001627445,-0.002442498691380024,-0.002165005775168538,-0.0010312151862308383,0.0036732545122504234,-0.001366215292364359,-0.0011274006683379412,0.0032704095356166363,-4.419920442160219E-4,0.004512975923717022,0.003956434316933155,-0.0010905592935159802,-0.0027423431165516376,0.001025308622047305,0.002350220223888755,-0.003991275560110807,6.259826477617025E-4,0.0032516797073185444,0.003080913331359625,0.0022275270894169807,0.0045756129547953606,-0.0024304573889821768,-1.6684022557456046E-4,0.0036196813452988863,0.0018787361914291978,0.004775937646...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-0.002391571644693613,-0.004730306100100279,0.004567709285765886,-0.0021375345531851053,0.003772377735003829,-0.004304440226405859,-0.0035075515042990446,-0.002512869192287326,0.0043669031001627445,-0.002442498691380024,-0.002165005775168538,-0.0010312151862308383,0.0036732545122504234,-0.001366215292364359,-0.0011274006683379412,0.0032704095356166363,-4.419920442160219E-4,0.004512975923717022,0.003956434316933155,-0.0010905592935159802,-0.0027423431165516376,0.001025308622047305,0.002350220223888755,-0.003991275560110807,6.259826477617025E-4,0.0032516797073185444,0.003080913331359625,0.0022275270894169807,0.0045756129547953606,-0.0024304573889821768,-1.6684022557456046E-4,0.0036196813452988863,0.0018787361914291978,0.004775937646627426,-0.00196994561702013,-0.0027159007731825113,0.0031720369588583708,-0.0021813814528286457,0.002189431106671691,2.624143671710044E-4,-0.004954970441758633,0.002538732485845685,0.0014121009735390544,1.1002375686075538E-4,-9.805667214095592E-4,0.004107102286070585,0.004556580446660519,-0.004467046819627285,0.004756748676300049,0.00451301597058773,0.0019458166789263487,-0.003740809392184019,0.002878280356526375,-0.0039485180750489235,-0.003817942226305604,0.004060946870595217,-0.0037012826651334763,5.832381430082023E-4,8.365289540961385E-4,0.0031905367504805326,0.0013797599822282791,-8.252866682596505E-4,0.003072091145440936,-1.8741383973974735E-4,0.0025043359491974115,0.003954887390136719,0.0016772146336734295,-0.003352581523358822,-0.0031396099366247654,0.0018370399484410882,0.0022732268553227186,0.004137713927775621,0.0030971395317465067,-0.0017618973506614566,8.953867363743484E-4,-0.001114314654842019,0.0019417429575696588,0.004564931616187096,0.00448218546807766,0.003902476280927658,-0.00484638474881649,0.0033688757102936506,0.002259455854073167,-0.0029800431802868843,-0.0027051661163568497,0.002050971146672964,0.001757301390171051,0.0024099953006953,0.003315121168270707,-0.0022442464251071215,-0.00285640312358737,-0.0019096869509667158,-5.441391258500516E-4,-0.004410340916365385,0.004393909592181444,0.00308600882999599,-1.6665829753037542E-4,0.0037998545449227095,0.00203644554130733,-0.0034904840867966413]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word2vecVectors = word2vecModel.transform(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "While our input data could be already numeric, it is useful sometimes for the ML algorithms to scale that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler()` --> to scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectors = Array([-2.0,5.0,1.0,4.0], [2.0,0.0,1.0,7.2], [4.0,2.0,0.5,0.8])\n",
       "vectorsRdd = ParallelCollectionRDD[20] at parallelize at <console>:36\n",
       "scaler = org.apache.spark.mllib.feature.StandardScaler@7fd0b1ab\n",
       "model = org.apache.spark.mllib.feature.StandardScalerModel@16f42c8c\n",
       "scaledData = MapPartitionsRDD[25] at map at VectorTransformer.scala:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[25] at map at VectorTransformer.scala:52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectors = Array(Vectors.dense(Array(-2.0, 5.0, 1.0, 4.0)),\n",
    "                    Vectors.dense(Array(2.0, 0.0, 1.0, 7.2)),\n",
    "                    Vectors.dense(Array(4.0, 2.0, 0.5, 0.8)))\n",
    "\n",
    "val vectorsRdd = sc.parallelize(vectors)\n",
    "val scaler = new StandardScaler(withMean=true, withStd=true)\n",
    "val model = scaler.fit(vectorsRdd)\n",
    "val scaledData = model.transform(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.0910894511799618,1.0596258856520353,0.5773502691896257,0.0], [0.2182178902359923,-0.9271726499455306,0.5773502691896257,1.0], [0.8728715609439694,-0.13245323570650427,-1.1547005383792517,-1.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "As with scaling, sometimes it is very usefull to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm = org.apache.spark.mllib.feature.Normalizer@1ed117d5\n",
       "normData = MapPartitionsRDD[26] at map at VectorTransformer.scala:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at map at VectorTransformer.scala:52"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val norm = new Normalizer()\n",
    "val normData = norm.transform(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.29488391230979427,0.7372097807744856,0.14744195615489714,0.5897678246195885], [0.2652790545386455,0.0,0.13263952726932274,0.9550045963391238], [0.8751666735874727,0.43758333679373634,0.10939583419843409,0.17503333471749455]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "The library MLlib includes useful functionalities to calculate some main statistics over numeric RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.stat.Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colStats()\n",
    "\n",
    "`colStats()` --> to calculate statistics over an RDD of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colStats = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@69eed7f4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@69eed7f4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colStats = Statistics.colStats(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colStatsMap = Map(count -> 3, variance -> [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002], mean -> [1.3333333333333335,2.333333333333333,0.8333333333333334,4.0], numNonzeros -> [3.0,2.0,3.0,3.0], min -> [-2.0,0.0,0.5,0.8], normL1 -> [8.0,7.0,2.5,12.0], normL2 -> [4.898979485566356,5.385164807134504,1.5,8.27526434623088], max -> [4.0,5.0,1.0,7.2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(count -> 3, variance -> [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002], mean -> [1.3333333333333335,2.333333333333333,0.8333333333333334,4.0], numNonzeros -> [3.0,2.0,3.0,3.0], min -> [-2.0,0.0,0.5,0.8], normL1 -> [8.0,7.0,2.5,12.0], normL2 -> [4.898979485566356,5.385164807134504,1.5,8.27526434623088], max -> [4.0,5.0,1.0,7.2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colStatsMap = Map(\"count\" -> colStats.count, \n",
    "                      \"max\" -> colStats.max,\n",
    "                      \"mean\" -> colStats.mean,\n",
    "                      \"min\" -> colStats.min,\n",
    "                      \"normL1\" -> colStats.normL1,\n",
    "                      \"normL2\" -> colStats.normL2,\n",
    "                      \"numNonzeros\" -> colStats.numNonzeros,\n",
    "                      \"variance\" -> colStats.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 3\n",
      "variance: [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002]\n",
      "mean: [1.3333333333333335,2.333333333333333,0.8333333333333334,4.0]\n",
      "numNonzeros: [3.0,2.0,3.0,3.0]\n",
      "min: [-2.0,0.0,0.5,0.8]\n",
      "normL1: [8.0,7.0,2.5,12.0]\n",
      "normL2: [4.898979485566356,5.385164807134504,1.5,8.27526434623088]\n",
      "max: [4.0,5.0,1.0,7.2]\n"
     ]
    }
   ],
   "source": [
    "colStatsMap.foreach{case(key, value) => println(key + \": \" + value)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corr()\n",
    "\n",
    "`corr()` --> to calculate the correlation matrix between the columns of one RDD or between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0                  -0.7370434740955019   -0.755928946018455   -0.3273268353539885   \n",
       "-0.7370434740955019  1.0                   0.11470786693528112  -0.39735970711951274  \n",
       "-0.755928946018455   0.11470786693528112   1.0                  0.8660254037844397    \n",
       "-0.3273268353539885  -0.39735970711951274  0.8660254037844397   1.0                   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data1 = ParallelCollectionRDD[39] at parallelize at <console>:35\n",
       "data2 = ParallelCollectionRDD[40] at parallelize at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[40] at parallelize at <console>:36"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1: RDD[Double] = sc.parallelize(Array(1, 2, 3, 4, 5))\n",
    "val data2: RDD[Double] = sc.parallelize(Array(10, 19, 32, 41, 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996326893005933"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chiSqTest()\n",
    "\n",
    "`chiSqTest()` --> to compute the Pearson's independence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelPointRdd = MapPartitionsRDD[51] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[51] at map at <console>:38"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelPointRdd = vectorsRdd.map(x => LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSqTest = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesi...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSqTest = Statistics.chiSqTest(labelPointRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test value: 1.0\n",
      "Test value: 1.0\n",
      "Test value: 1.0\n",
      "Test value: 1.0\n"
     ]
    }
   ],
   "source": [
    "chiSqTest.foreach(x => println(\"Test value: \" + x.pValue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Regression\n",
    "\n",
    "In this section, we will explore the conventional Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randGenerator = java.util.Random@5e249b2d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "java.util.Random@5e249b2d"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Random\n",
    "val randGenerator = new Random()\n",
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create training data according to a Linear Regression model with the following weights:\n",
    "\n",
    "    * Weights: [2.5, 1.25, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regFeatures = Vector(Vector(16, 11, 8, 16), Vector(3, 19, 3, 8), Vector(2, 1, 17, 15), Vector(5, 4, 15, 0), Vector(17, 6, 8, 11), Vector(6, 15, 10, 8), Vector(3, 15, 13, 9), Vector(8, 15, 2, 5), Vector(17, 12, 18, 17), Vector(10, 10, 17, 5), Vector(7, 0, 14, 8), Vector(15, 10, 17, 6), Vector(10, 9, 0, 18), Vector(8, 18, 3, 4), Vector(16, 5, 14, 13), Vector(18, 9, 1, 6), Vector(6, 5, 17, 0), Vector(3, 7, 18, 13), Vector(7, 5, 5, 0), Vector(7, 10, 15, 11), Vector(15, 10, 3, 10), Vector(1, 2, 1, 12), Vector(4, 3, 12, 6), Vector(13, 4, 19, 4), Vector(17, 4, 17, 1), Vector(1, 6, 11, 3), Vector(3, 5, 6, 7), Vector(2, 4, 10, 19), Vector(6, 18, 17, 1), Vector(17, 0, 1, 2), Vector(18, 8, 6, 12), Vector(5, 14, 12, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Vector(16, 11, 8, 16), Vector(3, 19, 3, 8), Vector(2, 1, 17, 15), Vector(5, 4, 15, 0), Vector(17, 6, 8, 11), Vector(6, 15, 10, 8), Vector(3, 15, 13, 9), Vector(8, 15, 2, 5), Vector(17, 12, 18, 17), Vector(10, 10, 17, 5), Vector(7, 0, 14, 8), Vector(15, 10, 17, 6), Vector(10, 9, 0, 18), Vector(8, 18, 3, 4), Vector(16, 5, 14, 13), Vector(18, 9, 1, 6), Vector(6, 5, 17, 0), Vector(3, 7, 18, 13), Vector(7, 5, 5, 0), Vector(7, 10, 15, 11), Vector(15, 10, 3, 10), Vector(1, 2, 1, 12), Vector(4, 3, 12, 6), Vector(13, 4, 19, 4), Vector(17, 4, 17, 1), Vector(1, 6, 11, 3), Vector(3, 5, 6, 7), Vector(2, 4, 10, 19), Vector(6, 18, 17, 1), Vector(17, 0, 1, 2), Vector(18, 8, 6, 12), Vector(5, 14, 12, 4), Vector(15, 11, 6, 4), Vector(16, 9, 7, 0), Vector(7, 10, 10, 13), Vector(19, 12, 13, 8), Vector(9, 13, 2, 1), Vector(17, 1, 19, 19), Vector(14, 1, 16, 7), Vector(4, 1, 15, 18), Vector(5, 7, 4, 1), Vector(1, 10, 7, 16), Vector(15, 4, 0, 19), Vector(11, 3, 9, 3), Vector(17, 11, 7, 12), Vector(8, 2, 7, 5), Vector(11, 14, 15, 0), Vector(0, 9, 14, 1), Vector(17, 3, 16, 16), Vector(0, 16, 19, 5), Vector(11, 5, 8, 7), Vector(0, 4, 10, 14), Vector(14, 16, 14, 16), Vector(11, 7, 4, 8), Vector(10, 7, 9, 9), Vector(13, 8, 14, 13), Vector(0, 13, 0, 9), Vector(0, 11, 3, 10), Vector(19, 19, 6, 18), Vector(13, 11, 8, 5), Vector(8, 7, 9, 0), Vector(9, 18, 6, 11), Vector(8, 4, 8, 11), Vector(10, 8, 10, 3), Vector(2, 19, 2, 0), Vector(9, 4, 14, 5), Vector(3, 19, 14, 1), Vector(15, 17, 10, 3), Vector(15, 9, 15, 7), Vector(10, 5, 19, 7), Vector(3, 16, 7, 19), Vector(15, 8, 12, 19), Vector(2, 18, 17, 10), Vector(19, 10, 17, 6), Vector(3, 5, 8, 6), Vector(4, 8, 13, 13), Vector(11, 10, 8, 3), Vector(7, 7, 13, 11), Vector(16, 17, 14, 7), Vector(13, 9, 0, 12), Vector(10, 2, 1, 5), Vector(11, 17, 11, 10), Vector(14, 8, 1, 2), Vector(8, 6, 0, 10), Vector(7, 14, 0, 19), Vector(19, 4, 1, 4), Vector(7, 9, 13, 8), Vector(11, 10, 3, 14), Vector(5, 14, 19, 2), Vector(9, 5, 1, 17), Vector(14, 6, 4, 2), Vector(10, 11, 8, 15), Vector(9, 4, 12, 19), Vector(11, 13, 6, 6), Vector(12, 16, 8, 3), Vector(14, 11, 13, 2), Vector(2, 0, 13, 6), Vector(0, 7, 11, 10), Vector(17, 9, 9, 0), Vector(14, 17, 5, 8), Vector(11, 13, 2, 11), Vector(2, 4, 9, 19), Vector(19, 4, 4, 18), Vector(17, 8, 10, 0), Vector(3, 18, 5, 11), Vector(16, 10, 8, 14), Vector(0, 14, 8, 18), Vector(17, 2, 11, 16), Vector(11, 13, 8, 7), Vector(4, 8, 2, 6), Vector(16, 15, 15, 11), Vector(16, 2, 9, 3), Vector(18, 9, 17, 18), Vector(1, 14, 1, 12), Vector(15, 9, 1, 2), Vector(10, 18, 5, 15), Vector(3, 0, 9, 18), Vector(2, 18, 12, 5), Vector(6, 14, 11, 0), Vector(8, 7, 3, 16), Vector(12, 16, 11, 16), Vector(0, 4, 2, 17), Vector(19, 13, 4, 10), Vector(17, 19, 19, 10), Vector(12, 2, 5, 14), Vector(11, 12, 4, 16), Vector(15, 17, 3, 19), Vector(4, 10, 12, 18), Vector(6, 17, 10, 15), Vector(13, 15, 14, 4), Vector(6, 11, 6, 8), Vector(3, 7, 7, 1), Vector(10, 18, 0, 10), Vector(8, 13, 6, 4), Vector(17, 12, 7, 19), Vector(13, 16, 7, 6), Vector(18, 14, 3, 1), Vector(3, 3, 14, 6), Vector(1, 11, 1, 6), Vector(16, 5, 3, 15), Vector(1, 1, 2, 18), Vector(8, 9, 13, 5), Vector(6, 14, 19, 19), Vector(2, 3, 17, 15), Vector(13, 4, 8, 8), Vector(14, 0, 2, 18), Vector(11, 5, 19, 11), Vector(10, 13, 12, 16), Vector(13, 15, 11, 10), Vector(3, 13, 10, 18), Vector(4, 1, 19, 9), Vector(15, 6, 1, 10), Vector(2, 11, 7, 10), Vector(15, 15, 9, 9), Vector(18, 5, 15, 6), Vector(15, 4, 13, 13), Vector(10, 16, 0, 7), Vector(6, 15, 9, 16), Vector(8, 11, 16, 4), Vector(13, 1, 5, 6), Vector(15, 14, 10, 1), Vector(5, 15, 12, 18), Vector(17, 2, 13, 9), Vector(3, 13, 7, 14), Vector(0, 11, 8, 7), Vector(16, 13, 13, 6), Vector(14, 8, 5, 7), Vector(7, 16, 3, 0), Vector(15, 15, 11, 11), Vector(0, 4, 11, 18), Vector(19, 10, 1, 7), Vector(17, 12, 14, 2), Vector(7, 6, 3, 11), Vector(6, 15, 3, 12), Vector(18, 14, 9, 12), Vector(10, 8, 15, 0), Vector(6, 12, 5, 19), Vector(13, 7, 17, 9), Vector(14, 7, 12, 16), Vector(5, 14, 17, 10), Vector(18, 3, 17, 3), Vector(3, 16, 19, 0), Vector(0, 11, 6, 10), Vector(5, 4, 6, 5), Vector(7, 18, 0, 8), Vector(19, 6, 19, 2), Vector(0, 9, 0, 3), Vector(0, 5, 1, 17), Vector(19, 2, 14, 3), Vector(12, 7, 12, 11), Vector(4, 13, 16, 9), Vector(8, 9, 8, 3), Vector(3, 9, 17, 16), Vector(15, 3, 17, 13), Vector(10, 10, 11, 14), Vector(5, 3, 18, 8), Vector(8, 17, 14, 11), Vector(7, 7, 15, 3), Vector(6, 13, 1, 5), Vector(8, 14, 5, 7), Vector(7, 4, 14, 18), Vector(4, 14, 6, 11), Vector(13, 0, 0, 12), Vector(13, 0, 7, 10), Vector(14, 18, 2, 8), Vector(19, 17, 4, 0), Vector(5, 10, 6, 9), Vector(13, 2, 12, 17), Vector(17, 12, 13, 9), Vector(6, 5, 15, 0), Vector(11, 0, 7, 13), Vector(7, 2, 13, 16), Vector(5, 11, 1, 4), Vector(9, 19, 18, 6), Vector(7, 11, 16, 15), Vector(5, 12, 5, 8), Vector(2, 17, 9, 7), Vector(14, 10, 8, 19), Vector(17, 10, 12, 9), Vector(19, 7, 4, 1), Vector(10, 10, 19, 12), Vector(4, 12, 2, 11), Vector(14, 15, 14, 10), Vector(14, 9, 6, 4), Vector(2, 3, 1, 3), Vector(4, 1, 16, 12), Vector(18, 14, 16, 0), Vector(2, 14, 3, 10), Vector(11, 7, 0, 9), Vector(1, 12, 0, 18), Vector(8, 19, 8, 14), Vector(12, 7, 17, 6), Vector(5, 14, 16, 17), Vector(10, 12, 0, 0), Vector(12, 15, 17, 6), Vector(19, 16, 19, 6), Vector(0, 4, 8, 13), Vector(18, 12, 11, 17), Vector(15, 15, 14, 3), Vector(14, 17, 11, 18), Vector(7, 1, 3, 1), Vector(14, 1, 7, 3), Vector(14, 2, 1, 15), Vector(15, 0, 19, 2), Vector(11, 7, 7, 5), Vector(6, 7, 4, 12), Vector(8, 18, 8, 9), Vector(8, 12, 15, 19), Vector(16, 14, 15, 3), Vector(12, 7, 7, 8), Vector(0, 19, 11, 19), Vector(1, 10, 13, 6), Vector(7, 1, 18, 2), Vector(10, 1, 18, 0), Vector(7, 6, 14, 0), Vector(11, 18, 18, 14), Vector(11, 11, 5, 19), Vector(11, 10, 13, 8), Vector(9, 6, 10, 10), Vector(17, 10, 3, 9), Vector(0, 10, 3, 14), Vector(1, 3, 15, 13), Vector(12, 13, 13, 13), Vector(18, 13, 0, 14), Vector(14, 16, 12, 13), Vector(3, 19, 16, 14), Vector(10, 11, 13, 4), Vector(0, 12, 17, 15), Vector(1, 2, 0, 15), Vector(0, 0, 16, 2), Vector(19, 0, 0, 6), Vector(9, 0, 0, 12), Vector(15, 3, 3, 17), Vector(5, 9, 2, 3), Vector(12, 17, 9, 5), Vector(9, 1, 4, 17), Vector(3, 8, 0, 11), Vector(14, 0, 12, 3), Vector(8, 3, 11, 8), Vector(9, 8, 16, 8), Vector(4, 2, 16, 7), Vector(0, 8, 8, 3), Vector(13, 13, 19, 10), Vector(17, 16, 8, 3), Vector(11, 1, 15, 4), Vector(3, 3, 5, 9), Vector(6, 17, 6, 15), Vector(2, 0, 13, 0), Vector(7, 18, 11, 11), Vector(17, 13, 10, 8), Vector(18, 11, 8, 17), Vector(19, 6, 13, 12), Vector(11, 17, 17, 0), Vector(8, 17, 12, 12), Vector(3, 4, 5, 15), Vector(18, 4, 10, 11), Vector(13, 19, 10, 17), Vector(15, 10, 11, 13), Vector(15, 15, 19, 7), Vector(15, 18, 14, 1), Vector(16, 0, 15, 17), Vector(0, 11, 16, 7), Vector(5, 10, 4, 13), Vector(19, 16, 13, 7), Vector(10, 5, 15, 1), Vector(2, 5, 8, 16), Vector(8, 8, 12, 0), Vector(10, 7, 8, 17), Vector(2, 18, 15, 6), Vector(16, 5, 3, 7), Vector(6, 19, 0, 3), Vector(5, 13, 14, 8), Vector(18, 11, 12, 19), Vector(3, 14, 4, 13), Vector(17, 8, 11, 13), Vector(15, 8, 18, 17), Vector(17, 10, 11, 10), Vector(18, 5, 18, 7), Vector(16, 9, 19, 6), Vector(3, 5, 3, 11), Vector(5, 14, 15, 8), Vector(13, 3, 4, 17), Vector(14, 9, 0, 16), Vector(7, 14, 14, 4), Vector(17, 17, 6, 7), Vector(9, 17, 0, 12), Vector(16, 3, 3, 15), Vector(13, 19, 14, 16), Vector(11, 10, 19, 6), Vector(1, 2, 16, 4), Vector(14, 11, 11, 6), Vector(1, 19, 13, 15), Vector(14, 3, 4, 16), Vector(19, 16, 7, 5), Vector(11, 0, 15, 12), Vector(8, 12, 7, 14), Vector(4, 19, 12, 2), Vector(14, 2, 7, 1), Vector(19, 16, 1, 4), Vector(1, 10, 2, 9), Vector(8, 12, 1, 5), Vector(8, 2, 0, 7), Vector(11, 17, 15, 14), Vector(19, 10, 12, 9), Vector(11, 5, 18, 4), Vector(0, 12, 10, 12), Vector(8, 14, 13, 8), Vector(16, 4, 15, 7), Vector(18, 15, 14, 14), Vector(0, 8, 14, 13), Vector(12, 2, 4, 6), Vector(3, 8, 7, 2), Vector(6, 19, 18, 1), Vector(4, 6, 9, 11), Vector(12, 3, 6, 12), Vector(2, 11, 13, 12), Vector(6, 1, 9, 14), Vector(16, 16, 2, 0), Vector(16, 10, 8, 10), Vector(11, 10, 16, 1), Vector(15, 10, 14, 7), Vector(13, 14, 4, 11), Vector(4, 1, 11, 18), Vector(1, 17, 11, 1), Vector(14, 6, 12, 19), Vector(16, 17, 4, 16), Vector(6, 2, 5, 4), Vector(19, 5, 8, 5), Vector(8, 10, 16, 11), Vector(0, 8, 6, 18), Vector(8, 0, 13, 0), Vector(7, 13, 14, 14), Vector(7, 18, 14, 0), Vector(19, 0, 3, 8), Vector(18, 11, 9, 6), Vector(4, 16, 3, 7), Vector(11, 7, 19, 6), Vector(8, 6, 3, 0), Vector(1, 6, 1, 12), Vector(0, 4, 18, 14), Vector(9, 17, 15, 5), Vector(1, 19, 16, 0), Vector(3, 10, 10, 2), Vector(9, 19, 13, 6), Vector(9, 16, 3, 10), Vector(1, 2, 13, 16), Vector(9, 1, 19, 12), Vector(11, 5, 10, 17), Vector(5, 2, 3, 2), Vector(3, 9, 11, 17), Vector(15, 6, 15, 7), Vector(11, 14, 9, 15), Vector(6, 6, 12, 9), Vector(0, 15, 14, 11), Vector(16, 18, 12, 2), Vector(16, 12, 11, 12), Vector(5, 10, 6, 15), Vector(4, 18, 11, 7), Vector(8, 2, 5, 12), Vector(1, 6, 4, 1), Vector(10, 17, 15, 1), Vector(0, 18, 18, 4), Vector(4, 13, 11, 8), Vector(0, 17, 5, 15), Vector(2, 17, 1, 0), Vector(4, 11, 13, 17), Vector(3, 9, 18, 11), Vector(7, 10, 18, 9), Vector(13, 5, 2, 4), Vector(10, 5, 15, 16), Vector(4, 10, 12, 1), Vector(10, 5, 12, 9), Vector(7, 15, 2, 17), Vector(0, 6, 5, 19), Vector(13, 8, 15, 15), Vector(6, 18, 2, 2), Vector(7, 6, 11, 4), Vector(13, 10, 19, 11), Vector(16, 8, 15, 13), Vector(6, 7, 3, 10), Vector(2, 7, 7, 6), Vector(11, 5, 14, 18), Vector(4, 15, 5, 7), Vector(5, 11, 17, 5), Vector(12, 15, 1, 7), Vector(1, 5, 18, 15), Vector(7, 0, 8, 11), Vector(9, 3, 10, 4), Vector(6, 6, 7, 19), Vector(6, 8, 13, 18), Vector(17, 17, 19, 17), Vector(13, 16, 18, 18), Vector(7, 17, 16, 8), Vector(15, 0, 2, 3), Vector(11, 10, 12, 1), Vector(11, 17, 7, 4), Vector(14, 13, 14, 13), Vector(8, 14, 10, 10), Vector(0, 8, 1, 16), Vector(9, 13, 14, 0), Vector(18, 0, 8, 14), Vector(18, 5, 13, 1), Vector(10, 2, 5, 2), Vector(6, 10, 4, 8), Vector(0, 17, 12, 10), Vector(1, 19, 16, 3), Vector(4, 2, 19, 4), Vector(18, 14, 13, 11), Vector(7, 0, 9, 11), Vector(7, 19, 19, 5), Vector(13, 15, 5, 14), Vector(13, 17, 1, 12), Vector(10, 13, 14, 3), Vector(14, 7, 0, 4), Vector(3, 16, 13, 9), Vector(10, 12, 19, 12), Vector(11, 9, 4, 6), Vector(17, 6, 2, 0), Vector(0, 7, 15, 9), Vector(1, 6, 12, 0), Vector(9, 19, 18, 0), Vector(3, 2, 14, 6), Vector(16, 9, 12, 2), Vector(8, 14, 3, 7), Vector(3, 2, 15, 11), Vector(13, 7, 15, 10), Vector(8, 16, 11, 7), Vector(8, 10, 11, 10), Vector(18, 16, 14, 18), Vector(14, 17, 1, 16), Vector(6, 15, 18, 9), Vector(17, 16, 18, 3), Vector(1, 8, 0, 9), Vector(10, 5, 5, 6), Vector(10, 19, 1, 19), Vector(14, 16, 6, 17), Vector(7, 16, 1, 3), Vector(9, 3, 18, 19), Vector(11, 16, 13, 13), Vector(3, 19, 19, 12), Vector(14, 7, 18, 10), Vector(4, 8, 6, 13), Vector(13, 11, 9, 11), Vector(5, 16, 15, 15), Vector(5, 11, 15, 12), Vector(12, 0, 1, 19), Vector(17, 17, 8, 10), Vector(2, 17, 11, 16), Vector(18, 6, 9, 9), Vector(0, 17, 11, 6), Vector(8, 2, 8, 18), Vector(12, 11, 19, 4), Vector(16, 13, 12, 14), Vector(2, 17, 5, 15), Vector(1, 6, 19, 8), Vector(13, 16, 2, 12), Vector(5, 19, 18, 6), Vector(9, 14, 4, 9), Vector(15, 2, 17, 0), Vector(0, 3, 5, 9))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val regFeatures = for(_ <- 1 to 500) yield {for (_ <- 1 to 4) yield randGenerator.nextInt(20)}\n",
    "val regFeaturesRdd = sc.parallelize(regFeatures).map(x => Vectors.dense(x.toArray.map(_.toDouble)))\n",
    "val scaler = new StandardScaler()\n",
    "val regFeaturesScale = scaler.fit(regFeaturesRdd).transform(regFeaturesRdd)\n",
    "val regData = regFeaturesScale.map(x => LabeledPoint({\n",
    "    val arrayValue = x.toArray\n",
    "    val randGenerator = new Random()\n",
    "    2.5*x(0) + 1.25*x(1) + 0.5*x(2) + x(3) + randGenerator.nextDouble\n",
    "},x))\n",
    "regData.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been created, we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numIterations = 10000\n",
       "stepSize = 0.1\n",
       "miniBatchFraction = 1.0\n",
       "lrModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 0.0, numFeatures = 4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numIterations = 10000\n",
    "val stepSize = 0.1\n",
    "val miniBatchFraction = 1.0\n",
    "val lrModel = LinearRegressionWithSGD.train(regData, numIterations = numIterations, \n",
    "                                            stepSize = stepSize, miniBatchFraction = miniBatchFraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the value of the original and computated weights and intercpet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed weights: [2.3416762582321367,1.3269221781302136,0.7296018935053467,1.1208114768699862]\n",
      "Original weights: [2.5, 1.25, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "println(\"Computed weights: \" + lrModel.weights)\n",
    "println(\"Original weights: [2.5, 1.25, 0.5, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Classification\n",
    "\n",
    "In this section, we will explore different classification models:\n",
    "\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines (SVMs)\n",
    "    * Naive Bayes\n",
    "    * Decision Trees\n",
    "    * Random Forests\n",
    "    \n",
    "For every case, we will try to solve the sampe problem: a model to classify messages into two groups: legitimate and Spam. For that, we will have first to preprocess some text data using come functionalities studied in previous sections of this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.classification.{LogisticRegressionWithSGD, SVMWithSGD, NaiveBayes}\n",
    "import org.apache.spark.mllib.tree.{DecisionTree, RandomForest}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniData = [label: string, text: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: string, text: string ... 3 more fields]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniData = spark.read.option(\"header\", \"true\").csv(\"../data/spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+\n",
      "|label|                text| _c2| _c3| _c4|\n",
      "+-----+--------------------+----+----+----+\n",
      "|  ham|Go until jurong p...|null|null|null|\n",
      "|  ham|Ok lar... Joking ...|null|null|null|\n",
      "| spam|Free entry in 2 a...|null|null|null|\n",
      "|  ham|U dun say so earl...|null|null|null|\n",
      "|  ham|Nah I don't think...|null|null|null|\n",
      "| spam|FreeMsg Hey there...|null|null|null|\n",
      "|  ham|Even my brother i...|null|null|null|\n",
      "|  ham|As per your reque...|null|null|null|\n",
      "| spam|WINNER!! As a val...|null|null|null|\n",
      "| spam|Had your mobile 1...|null|null|null|\n",
      "|  ham|I'm gonna be home...|null|null|null|\n",
      "| spam|SIX chances to wi...|null|null|null|\n",
      "| spam|URGENT! You have ...|null|null|null|\n",
      "|  ham|I've been searchi...|null|null|null|\n",
      "|  ham|I HAVE A DATE ON ...|null|null|null|\n",
      "| spam|XXXMobileMovieClu...|null|null|null|\n",
      "|  ham|Oh k...i'm watchi...|null|null|null|\n",
      "|  ham|Eh u remember how...|null|null|null|\n",
      "|  ham|Fine if that��s t...|null|null|null|\n",
      "| spam|England v Macedon...|null|null|null|\n",
      "+-----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iniData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniDataRdd = MapPartitionsRDD[497] at rdd at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[497] at rdd at <console>:41"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniDataRdd = iniData.select(\"label\", \"text\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>ham</td><td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----+-----------------------------------------------------------------------------------------------------------------+\n",
       "| ham | Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... |\n",
       "+-----+-----------------------------------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.take(1)(0)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:1: error: ')' expected but '(' found.\n",
       "iniDataRddFilter = iniDataRdd.filter(lambda row: (isinstance(row.label, str) and isinstance(row.text, str)))\n",
       "                                                            ^\n",
       "<console>:1: error: ')' expected but '(' found.\n",
       "iniDataRddFilter = iniDataRdd.filter(lambda row: (isinstance(row.label, str) and isinstance(row.text, str)))\n",
       "                                                                                           ^\n",
       "<console>:1: error: ';' expected but ')' found.\n",
       "iniDataRddFilter = iniDataRdd.filter(lambda row: (isinstance(row.label, str) and isinstance(row.text, str)))\n",
       "                                                                                                          ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRddFilter = iniDataRdd.filter(lambda row: (isinstance(row.label, str) and isinstance(row.text, str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniDataRddFilter = MapPartitionsRDD[498] at filter at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[498] at filter at <console>:43"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniDataRddFilter = iniDataRdd.filter(row => (row(0), row(1)) match {\n",
    "    case (key: String, value: String) => true\n",
    "    case _ => false\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5573"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iniDataRddFilter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textRdd = MapPartitionsRDD[499] at map at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[499] at map at <console>:45"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textRdd = iniDataRddFilter.map(row => row(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf = org.apache.spark.mllib.feature.HashingTF@67f1b487\n",
       "tfVectors = MapPartitionsRDD[500] at map at <console>:57\n",
       "idf = org.apache.spark.mllib.feature.IDF@5ae73b77\n",
       "idfModel = org.apache.spark.mllib.feature.IDFModel@3e967f1b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.IDFModel@3e967f1b"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = new HashingTF(1000)\n",
    "val tfVectors = textRdd.map(x => tf.transform(x.toString.split(\" \")))\n",
    "val idf = new IDF()\n",
    "val idfModel = idf.fit(tfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spamText = MapPartitionsRDD[503] at map at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[503] at map at <console>:45"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spamText = iniDataRddFilter.filter(_(0) == \"spam\").map(_(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamText.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's, FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv, WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamText.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genText = MapPartitionsRDD[505] at map at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[505] at map at <console>:45"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genText = iniDataRddFilter.filter(_(0) == \"ham\").map(_(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4825"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genText.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..., Ok lar... Joking wif u oni..., U dun say so early hor... U c already then say...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genText.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf2 = org.apache.spark.mllib.feature.HashingTF@67f1b487\n",
       "tfVectors = MapPartitionsRDD[506] at map at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[506] at map at <console>:50"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf2 = tf\n",
    "val tfVectors = textRdd.map(x => tf2.transform(x.toString.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfSpam = org.apache.spark.mllib.feature.HashingTF@67f1b487\n",
       "idfModelSpam = org.apache.spark.mllib.feature.IDFModel@3e967f1b\n",
       "spamVectors = MapPartitionsRDD[507] at map at <console>:56\n",
       "spamIdf = MapPartitionsRDD[508] at map at <console>:57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[508] at map at <console>:57"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tfSpam = tf\n",
    "val idfModelSpam = idfModel\n",
    "val spamVectors = spamText.map(x => tfSpam.transform(x.toString.split(\" \")))\n",
    "val spamIdf = spamVectors.map(x => idfModelSpam.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000,[30,33,35,72,128,140,166,170,388,409,445,468,508,634,667,670,685,692,716,755,784,880,887,989],[5.3300313420375645,5.917818006939683,4.498733822996802,4.137231838309754,2.8732955692162605,4.713845202613747,4.406360502865787,1.4846230856914024,3.5413519540206178,4.531523645819793,2.0151721633241344,4.2951348677555625,4.600516517306744,5.191881003556747,5.160132305242167,3.3894262452119444,4.269159381352302,4.498733822996802,4.421175588650928,7.3400823008812655,5.3300313420375645,4.451480938146257,10.14104029310496,2.514400868539215])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamIdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfGen = org.apache.spark.mllib.feature.HashingTF@67f1b487\n",
       "idfModelGen = org.apache.spark.mllib.feature.IDFModel@3e967f1b\n",
       "genVectors = MapPartitionsRDD[509] at map at <console>:56\n",
       "genIdf = MapPartitionsRDD[510] at map at <console>:57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[510] at map at <console>:57"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tfGen = tf\n",
    "val idfModelGen = idfModel\n",
    "val genVectors = genText.map(x => tfGen.transform(x.toString.split(\" \")))\n",
    "val genIdf = genVectors.map(x => idfModelGen.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000,[7,42,150,165,258,260,360,362,445,647,655,687,744,745,785,831,854,878,899,966],[3.2413731452528047,5.129360646575413,4.962306561912247,5.447814377693948,4.7972268115527985,4.020698022053802,5.099507683425732,4.531523645819793,2.0151721633241344,5.5348257546835775,3.8055866424368565,5.25857237805542,3.846744714930364,4.841678574123632,5.681429228875453,3.4001215343286924,5.917818006939683,3.698614522884689,3.4001215343286924,2.8177257180614497])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genIdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spamPoints = MapPartitionsRDD[511] at map at <console>:66\n",
       "genPoints = MapPartitionsRDD[512] at map at <console>:67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[512] at map at <console>:67"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spamPoints = spamIdf.map(x => LabeledPoint(1, x))\n",
    "val genPoints = genIdf.map(x => LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(1000,[30,33,35,72,128,140,166,170,388,409,445,468,508,634,667,670,685,692,716,755,784,880,887,989],[5.3300313420375645,5.917818006939683,4.498733822996802,4.137231838309754,2.8732955692162605,4.713845202613747,4.406360502865787,1.4846230856914024,3.5413519540206178,4.531523645819793,2.0151721633241344,4.2951348677555625,4.600516517306744,5.191881003556747,5.160132305242167,3.3894262452119444,4.269159381352302,4.498733822996802,4.421175588650928,7.3400823008812655,5.3300313420375645,4.451480938146257,10.14104029310496,2.514400868539215]))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamPoints.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,(1000,[7,42,150,165,258,260,360,362,445,647,655,687,744,745,785,831,854,878,899,966],[3.2413731452528047,5.129360646575413,4.962306561912247,5.447814377693948,4.7972268115527985,4.020698022053802,5.099507683425732,4.531523645819793,2.0151721633241344,5.5348257546835775,3.8055866424368565,5.25857237805542,3.846744714930364,4.841678574123632,5.681429228875453,3.4001215343286924,5.917818006939683,3.698614522884689,3.4001215343286924,2.8177257180614497]))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genPoints.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataIni = UnionRDD[513] at union at <console>:69\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UnionRDD[513] at union at <console>:69"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataIni = spamPoints.union(genPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randGenerator = java.util.Random@52f20a41\n",
       "mlData = MapPartitionsRDD[518] at map at <console>:76\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[518] at map at <console>:76"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val randGenerator = new Random()\n",
    "randGenerator.nextInt(20)\n",
    "\n",
    "val mlData = mlDataIni.map(row => (randGenerator.nextInt(100), row)).sortByKey().map(_._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlData.map(_.label).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataTrainTest = Array(MapPartitionsRDD[520] at randomSplit at <console>:74, MapPartitionsRDD[521] at randomSplit at <console>:74)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[MapPartitionsRDD[520] at randomSplit at <console>:74, MapPartitionsRDD[521] at randomSplit at <console>:74]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataTrainTest = mlData.randomSplit(weights = Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlDataTrain = MapPartitionsRDD[520] at randomSplit at <console>:74\n",
       "mlDataTest = MapPartitionsRDD[521] at randomSplit at <console>:74\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[521] at randomSplit at <console>:74"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlDataTrain = mlDataTrainTest(0)\n",
    "val mlDataTest = mlDataTrainTest(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[521] at randomSplit at <console>:74"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrain.cache()\n",
    "mlDataTest.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4469"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrain.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,[12,184,410,437],[5.3300313420375645,4.498733822996802,4.962306561912247,3.9163380067295592])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTest.take(1)(0).features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = new LogisticRegressionWithSGD().run(mlDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 1.0; Prediction: 1.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = lrModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suport Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svmModel = new SVMWithSGD().run(mlDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 1.0; Prediction: 1.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = svmModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nbModel = org.apache.spark.mllib.classification.NaiveBayesModel@5750a5c8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.classification.NaiveBayesModel@5750a5c8"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nbModel = new NaiveBayes().run(mlDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 1.0; Prediction: 1.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = nbModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses = 2\n",
       "categoricalFeaturesInfo = Map()\n",
       "impurity = gini\n",
       "maxDepth = 15\n",
       "maxBins = 64\n",
       "treeModel = DecisionTreeModel classifier of depth 15 with 285 nodes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeModel classifier of depth 15 with 285 nodes"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo=Map[Int, Int]()\n",
    "val impurity=\"gini\"\n",
    "val maxDepth=15\n",
    "val maxBins=64\n",
    "\n",
    "\n",
    "val treeModel = DecisionTree.trainClassifier(input = mlDataTrain, numClasses = numClasses, \n",
    "                                             categoricalFeaturesInfo = categoricalFeaturesInfo,\n",
    "                                             impurity = impurity, maxDepth = maxDepth,\n",
    "                                             maxBins = maxBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 1.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 1.0; Prediction: 1.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = treeModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses = 2\n",
       "categoricalFeaturesInfo = Map()\n",
       "impurity = gini\n",
       "maxDepth = 15\n",
       "maxBins = 64\n",
       "featureSubsetStrategy = auto\n",
       "numTrees = 10\n",
       "forestModel = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TreeEnsembleModel classifier with 10 trees\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 15\n",
    "val maxBins = 64\n",
    "val featureSubsetStrategy = \"auto\"\n",
    "val numTrees = 10\n",
    "\n",
    "val forestModel = RandomForest.trainClassifier(input = mlDataTrain, numClasses = numClasses, \n",
    "                                             categoricalFeaturesInfo = categoricalFeaturesInfo, \n",
    "                                             impurity = impurity, maxDepth = maxDepth, \n",
    "                                             maxBins = maxBins, numTrees = numTrees,\n",
    "                                             featureSubsetStrategy = featureSubsetStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 0.0; Prediction: 0.0\n",
      "Actual label: 1.0; Prediction: 1.0\n",
      "Actual label: 1.0; Prediction: 1.0\n"
     ]
    }
   ],
   "source": [
    "for(data <- mlDataTest.take(10)){\n",
    "    val pred = forestModel.predict(data.features)\n",
    "    println(\"Actual label: \" + data.label + \"; Prediction: \" + pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Clustering\n",
    "\n",
    "In this section, we will explore the `K-means` algorithm, which is the main clustering algorithm included in MLlib.\n",
    "\n",
    "Here, we will study the previous spam classification problem. We will cluster our mesages into two groups, and then, we will count the number of points that fall into each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusterData = MapPartitionsRDD[1051] at map at <console>:75\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1051] at map at <console>:75"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clusterData = mlData.map(_.features)\n",
    "clusterData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusters = org.apache.spark.mllib.clustering.KMeansModel@214ab2b9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.clustering.KMeansModel@214ab2b9"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clusters = KMeans.train(clusterData, 2, maxIterations=1700, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clustersModel = org.apache.spark.mllib.clustering.KMeansModel@214ab2b9\n",
       "predictions = MapPartitionsRDD[1062] at map at <console>:80\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1062] at map at <console>:80"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clustersModel = clusters\n",
    "val predictions = clusterData.map(x => clustersModel.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(0 -> 5571, 1 -> 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collavorative Filtering and Recommendation: Alternating Least Squares\n",
    "\n",
    "Now, we will explore the `Alternating Least Squares` algorithm, very used for collaborative filtering problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.recommendation.{ALS, Rating}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataAls = ../data/als/test.data MapPartitionsRDD[1067] at textFile at <console>:41\n",
       "ratings = MapPartitionsRDD[1069] at map at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1069] at map at <console>:42"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataAls = sc.textFile(\"../data/als/test.data\")\n",
    "val ratings = dataAls.map(_.split(',')).map(l => Rating(l(0).toInt, l(1).toInt, l(2).toFloat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(1,1,5.0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a recommendation moddel using ALS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank = 10\n",
       "numIterations = 10\n",
       "alsModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@e5f8e32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.recommendation.MatrixFactorizationModel@e5f8e32"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rank = 10\n",
    "val numIterations = 10\n",
    "val alsModel = ALS.train(ratings, rank, numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testData = MapPartitionsRDD[1277] at map at <console>:44\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1277] at map at <console>:44"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testData = ratings.map(p => (p.user, p.product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,1), (1,2)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alsPredictions = MapPartitionsRDD[1286] at map at MatrixFactorizationModel.scala:140\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1286] at map at MatrixFactorizationModel.scala:140"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val alsPredictions = alsModel.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(1,1,4.995327538757314), Rating(1,2,1.0020486223725058)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alsPredictions.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "In this section, we will see two main functionalities included in MLlib relative to dimensionality reduction:\n",
    "\n",
    "    * Principal Component Analysis\n",
    "    * Singular Vector Decomposition\n",
    "    \n",
    "    \n",
    "We will use the data from the Clustering Section, training also a KMeans model with the \"reduced\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000,[36,146,184,387,450,508,511,534,581,620,661,691,743,769,813,852,948,999],[2.238988888679249,2.6520585961726324,4.498733822996802,4.482733481650361,3.4610822341183796,4.600516517306744,3.855183583576229,2.7481324262622544,5.191881003556747,4.988282048315508,5.25857237805542,4.2820627861882095,5.447814377693948,5.224670826379739,4.406360502865787,3.7895863010904156,5.25857237805542,5.406992383173693]), (1000,[36,73,78,146,167,170,231,263,343,388,425,431,447,517,525,526,596,660,704,803,831,903,951],[2.238988888679249,7.076543745619019,4.2438415733680115,5.304117192345265,4.207027600245295,1.4846230856914024,4.2951348677555625,2.220639750011052,2.3232492322969884,2.360901302680412,2.9995618694855373,4.391761703444634,3.6913942749112016,4.207027600245295,3.5632731750149267,2.117099071070212,3.6286559342777784,4.391761703444634,3.6422615863335572,4.051157229538511,3.4001215343286924,3.3376011773473584,5.099507683425732])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.Matrix\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mat = org.apache.spark.mllib.linalg.distributed.RowMatrix@4a9ea0b8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.linalg.distributed.RowMatrix@4a9ea0b8"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mat = new RowMatrix(clusterData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pc = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.004459599011320296   0.0041608...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.006639428117164253   0.0332054310436396      \n",
       "-0.0013645425248541843  0.0035456557549498058   \n",
       "-2.5710328882322386E-4  0.007015531669825936    \n",
       "-0.004463687500827013   0.0013951563429472902   \n",
       "-0.0023527218330895924  0.024390934315625543    \n",
       "-0.001222962075320043   0.007885626529188996    \n",
       "-0.004632324946425941   0.007985071258782759    \n",
       "-0.006097919717778844   0.013917509155318248    \n",
       "-0.0015643671662767245  0.009363235968379456    \n",
       "-4.8057066882834816E-4  0.004764736689663678    \n",
       "-6.90804677118604E-4    0.003604015306000942    \n",
       "-0.012675350915813977   0.012652788299459542    \n",
       "5.762153759525019E-4    6.107019012387356E-4    \n",
       "-0.004924684573814147   0.02397821779717647     \n",
       "8.751817252586666E-4    -0.0030496075057231545  \n",
       "-0.03903173161860575    0.011655881469136273    \n",
       "-0.004459599011320296   0.004160893626195047    \n",
       "-0.0018887112732935733  0.0062206603715747      \n",
       "-0.02907981016955361    0.05474416629343554     \n",
       "-0.016776749654633785   0.0185579592253671      \n",
       "4.008956560157287E-4    0.0014146806412460313   \n",
       "-0.010251955990435834   0.011885401525931753    \n",
       "-1.670719626628827E-4   0.0052119660201827836   \n",
       "-0.0027897608587335314  0.016226347768423133    \n",
       "-0.0042047544906971     0.013263148712955237    \n",
       "-0.01369761941036368    0.015299906823756659    \n",
       "-0.00111361912460075    0.013794160398500302    \n",
       "-5.921382805593711E-4   9.730242246137975E-6    \n",
       "-7.049323724853823E-5   0.006769022154040384    \n",
       "-0.026602838654195236   0.014650622966803558    \n",
       "-0.0011157157243399266  0.0043565862321070115   \n",
       "-0.003004743700941959   0.00999654833601042     \n",
       "-0.0027076076798990814  0.007810859912883037    \n",
       "0.0010115953869496068   -5.502051176186914E-4   \n",
       "-0.002353084377704704   0.00493800886177193     \n",
       "-0.003423313198368358   0.012925695994322249    \n",
       "-0.028748183206593735   0.12531020176734264     \n",
       "-0.02791428566381671    0.021118481892883748    \n",
       "-0.005895696980524209   0.04223506739690337     \n",
       "-0.003222591825640942   0.005363027941513235    \n",
       "-0.02410340423070485    -0.01195032229524329    \n",
       "-0.0015887153322339913  0.0024553548049629937   \n",
       "-0.005076704265274564   0.004175371754574359    \n",
       "-0.002008802616016632   0.016682927569982492    \n",
       "-0.006338674760813181   0.03243916060009197     \n",
       "-0.0026827821821818324  0.00382296947619232     \n",
       "8.708818770266344E-4    4.574045609779351E-5    \n",
       "-0.004215746768655755   0.030645514839796464    \n",
       "-0.01053936208719224    0.04016312415504607     \n",
       "0.0010435030144056517   -1.9361120548239415E-4  \n",
       "-0.003135992703127948   0.005067244658630036    \n",
       "-0.0026628767212125543  0.015164265860873792    \n",
       "6.620785520550107E-4    0.00672609611500315     \n",
       "-0.0015422440636146363  0.011889799555267715    \n",
       "-3.1131126481366675E-4  0.004817531899312607    \n",
       "-4.476139065122228E-4   0.0019271862507185794   \n",
       "-0.7788940327616126     -0.2575823401973378     \n",
       "9.676540417931728E-4    0.007492984801826262    \n",
       "-1.624463637118068E-4   0.001240938642045053    \n",
       "-2.881522921831694E-4   0.0049111208809251685   \n",
       "-0.0014342158735950975  0.0029367984573207325   \n",
       "-0.0028333138942470693  0.004235546121969715    \n",
       "-0.010012999162026729   0.028195290328740553    \n",
       "-7.173955580903398E-4   0.006384259458895884    \n",
       "-8.201690713582828E-4   0.0036261234195852474   \n",
       "-0.0010084156960661437  0.00617559127998539     \n",
       "3.0959095952497E-4      0.006491228969738951    \n",
       "-0.008178193925649925   0.004286560270672041    \n",
       "-0.0035739605379597763  0.0015620756172681413   \n",
       "-0.01360720167766196    0.010663167575712021    \n",
       "-0.00381107789734402    0.01045499108916459     \n",
       "-0.00408658610194822    0.037650031287287375    \n",
       "-0.0023950282900627178  0.009157108849105138    \n",
       "-0.009557796337778979   0.020816905481006464    \n",
       "-0.004062490222756785   0.00827542953205337     \n",
       "-0.003033819338702126   0.01590828557777977     \n",
       "-0.02413824497928258    0.11170161849425332     \n",
       "-0.010352066507557349   0.06172817976285037     \n",
       "-0.004253147487272339   0.014556650791096977    \n",
       "... (1000 total)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pc = mat.computePrincipalComponents(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projectedPca = MapPartitionsRDD[1289] at mapPartitions at RowMatrix.scala:443\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1289] at mapPartitions at RowMatrix.scala:443"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val projectedPca = mat.multiply(pc).rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeansModelPca = org.apache.spark.mllib.clustering.KMeansModel@54e75e68\n",
       "predictionsPca = MapPartitionsRDD[1333] at map at <console>:87\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1333] at map at <console>:87"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kmeansModelPca = KMeans.train(projectedPca, 2, maxIterations=1700, initializationMode=\"random\")\n",
    "val predictionsPca = projectedPca.map(x => kmeansModelPca.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(0 -> 4555, 1 -> 1017)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsPca.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svd = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.03175426599745287    0.01404...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SingularValueDecomposition(null,[230.92399413088708,168.19026124879895,109.84815231736111,108.07534735546739,99.39483397804844,96.92313709383784,92.3007084190019,89.53922163782167,87.39542396024576,84.8202049439851,83.08234143420717,82.7496467971585,81.64185008669489,80.12470389477421,78.32716692786273,76.77514947395973,75.993850263491,74.77200312260355,74.00221055778967,73.58996929973304],-0.02667562165609747    0.010690451459367756    ... (20 total)\n",
       "-0.009125903178416477   0.002387006735496973    ...\n",
       "-0.010300577087381604   0.004861417423187044    ...\n",
       "-0.010356430131513642   -8.15577265844504E-4    ...\n",
       "-0.03175426599745287    0.014044243389408843    ...\n",
       "-0.0077633835913056715  0.0033761615810848442   ...\n",
       "-0.011680558903263657   9.362731433118169E-4    ...\n",
       "-0.0403065638389083     0.010128847514276111    ...\n",
       "-0.011525735778459164   0.0047542141156844756   ...\n",
       "-0.009823169977147276   0.004221567982109323    ...\n",
       "-0.00731780883433451    0.002576731880106117    ...\n",
       "-0.015213531341521818   -0.00428772701133867    ...\n",
       "-0.006784292283089561   0.0027365232038601096   ...\n",
       "-0.02532702659044964    0.009570853378570637    ...\n",
       "-0.0073876450837178755  0.002046639168114631    ...\n",
       "-0.03383170563283324    -0.025263976412850435   ...\n",
       "-0.013060813401858329   9.483799754427605E-4    ...\n",
       "-0.012769090830585203   0.004055432355192129    ...\n",
       "-0.073671136019662      0.009174412641439561    ...\n",
       "-0.059617056875632055   0.0081023117016044      ...\n",
       "-0.011128927057620345   0.00429423980651499     ...\n",
       "-0.016734298078172186   -0.0016680208592454934  ...\n",
       "-0.009951212218886394   0.004723218560089424    ...\n",
       "-0.01318385437482685    0.006025806388921511    ...\n",
       "-0.010082053481566421   0.0026261568265669635   ...\n",
       "-0.03651712197521068    0.0019266922961523394   ...\n",
       "-0.026575257833873073   0.011506793369942037    ...\n",
       "-0.0059894776155235375  0.0012119925740404552   ...\n",
       "-0.0056019571661711535  0.0034862832728567786   ...\n",
       "-0.021955479606718155   -0.015656915140680384   ...\n",
       "-0.008243014224263559   0.0027208252798706367   ...\n",
       "-0.01401768996874986    0.004265000284191684    ...\n",
       "-0.01168873971089062    0.00308450039948395     ...\n",
       "-0.003461472139857513   0.0019297495574462867   ...\n",
       "-0.02161091613199341    0.0056877261562522895   ...\n",
       "-0.018829035842599734   0.00634253611432058     ...\n",
       "-0.11548759717844094    0.04290165572651777     ...\n",
       "-0.0275634022902319     -0.01366218992461522    ...\n",
       "-0.028641215728573203   0.01413179900141432     ...\n",
       "-0.008320471565270424   8.410849550841287E-4    ...\n",
       "-0.017217610342247638   -0.02275377509021663    ...\n",
       "-0.01336609706536058    0.0030462289711657414   ...\n",
       "-0.010840486547996176   -4.192599694738258E-4   ...\n",
       "-0.018499766363668346   0.008820377416181326    ...\n",
       "-0.039057477998589864   0.015009015236466625    ...\n",
       "-0.014404636734265168   0.003003836148314301    ...\n",
       "-0.007813870047986898   0.003327260634496173    ...\n",
       "-0.02103389895886651    0.0108969469143907      ...\n",
       "-0.04193467332372863    0.014593411718559213    ...\n",
       "-0.004244002534081398   0.002291710918316454    ...\n",
       "-0.007106482218159687   5.354189796915616E-4    ...\n",
       "-0.014167231844499164   0.006068550543817282    ...\n",
       "-0.014237122788608253   0.007249735412033179    ...\n",
       "-0.01798730064783681    0.007527618239492256    ...\n",
       "-0.007903916856886226   0.003646360909741169    ...\n",
       "-0.007721607982263741   0.0023559326740249364   ...\n",
       "-0.19471414550273825    -0.7988370157848499     ...\n",
       "-0.017542611166493194   0.008472347115801325    ...\n",
       "-0.006044365361804095   0.0020242282367481466   ...\n",
       "-0.007680218398021681   0.0036028366970787384   ...\n",
       "-0.009357085165516619   0.0021741159507774723   ...\n",
       "-0.01296267814304837    0.0022812038986226267   ...\n",
       "-0.023720877348332087   0.005514382179189085    ...\n",
       "-0.008271420149289013   0.0037634140105978087   ...\n",
       "-0.008621279883623082   0.0027052934510013536   ...\n",
       "-0.011407319482393326   0.004346510409802263    ...\n",
       "-0.008768554720868531   0.004838082932689043    ...\n",
       "-0.01006861880298096    -0.00404448936861874    ...\n",
       "-0.011301441388564745   2.5203238162739387E-4   ...\n",
       "-0.06520856492975535    0.011590574228719792    ...\n",
       "-0.010416389506750733   0.002334944497207534    ...\n",
       "-0.028189112118771245   0.01506368049039468     ...\n",
       "-0.02183583875252348    0.007288875528579441    ...\n",
       "-0.04007027190083238    0.00974681969945821     ...\n",
       "-0.014124833577169529   0.003008983831247118    ...\n",
       "-0.016237835264949395   0.006414296630378865    ...\n",
       "-0.08215756315152038    0.03282074683364193     ...\n",
       "-0.06777267420944309    0.028127066875751006    ...\n",
       "-0.022790823980789066   0.007324130442292839    ...\n",
       "... (1000 total))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svd = mat.computeSVD(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "projectedSVD = MapPartitionsRDD[1432] at mapPartitions at RowMatrix.scala:443\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1432] at mapPartitions at RowMatrix.scala:443"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val projectedSVD = mat.multiply(svd.V).rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeansModelSVD = org.apache.spark.mllib.clustering.KMeansModel@504d9f9d\n",
       "predictionsSVD = MapPartitionsRDD[1470] at map at <console>:87\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1470] at map at <console>:87"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kmeansModelSVD = KMeans.train(projectedSVD, 2, maxIterations=1700, initializationMode=\"random\")\n",
    "val predictionsSVD = projectedSVD.map(x => kmeansModelSVD.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(0 -> 1577, 1 -> 3995)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsSVD.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "MLlib includes some functionalities to calculate automatically some metrics of trained ML models. While there are more, here we will evaluate the LR model of the spam classification section using the `BinaryClassificationMetrics` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,(1000,[36,146,184,387,450,508,511,534,581,620,661,691,743,769,813,852,948,999],[2.238988888679249,2.6520585961726324,4.498733822996802,4.482733481650361,3.4610822341183796,4.600516517306744,3.855183583576229,2.7481324262622544,5.191881003556747,4.988282048315508,5.25857237805542,4.2820627861882095,5.447814377693948,5.224670826379739,4.406360502865787,3.7895863010904156,5.25857237805542,5.406992383173693]))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlDataTrain.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModelEval = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 1000, numClasses = 2, threshold = 0.5\n",
       "predLabelLr = MapPartitionsRDD[1474] at map at <console>:87\n",
       "metricsLr = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4044debc\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4044debc"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModelEval = lrModel\n",
    "val predLabelLr = mlDataTest.map(lpoint => (lrModelEval.predict(lpoint.features), lpoint.label))\n",
    "val metricsLr = new BinaryClassificationMetrics(predLabelLr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR model\n",
      "Area Under PR: 0.70969305879444\n",
      "Area Under ROC: 0.8678751748889835\n"
     ]
    }
   ],
   "source": [
    "println(\"LR model\")\n",
    "println(\"Area Under PR: \" + metricsLr.areaUnderPR)\n",
    "println(\"Area Under ROC: \" + metricsLr.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline API\n",
    "\n",
    "ML pipelines are an interesting concept in order to organize all the tasks relative to a ML problem (data preparation + model training) into a Pipeline. In this section, we will solve the spam classification problem using ML pipelines, which are made by a series of Transformers and Estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature\n",
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+----+----+\n",
      "|label|                text| _c2| _c3| _c4|\n",
      "+-----+--------------------+----+----+----+\n",
      "|  ham|Go until jurong p...|null|null|null|\n",
      "|  ham|Ok lar... Joking ...|null|null|null|\n",
      "| spam|Free entry in 2 a...|null|null|null|\n",
      "|  ham|U dun say so earl...|null|null|null|\n",
      "|  ham|Nah I don't think...|null|null|null|\n",
      "| spam|FreeMsg Hey there...|null|null|null|\n",
      "|  ham|Even my brother i...|null|null|null|\n",
      "|  ham|As per your reque...|null|null|null|\n",
      "| spam|WINNER!! As a val...|null|null|null|\n",
      "| spam|Had your mobile 1...|null|null|null|\n",
      "|  ham|I'm gonna be home...|null|null|null|\n",
      "| spam|SIX chances to wi...|null|null|null|\n",
      "| spam|URGENT! You have ...|null|null|null|\n",
      "|  ham|I've been searchi...|null|null|null|\n",
      "|  ham|I HAVE A DATE ON ...|null|null|null|\n",
      "| spam|XXXMobileMovieClu...|null|null|null|\n",
      "|  ham|Oh k...i'm watchi...|null|null|null|\n",
      "|  ham|Eh u remember how...|null|null|null|\n",
      "|  ham|Fine if that��s t...|null|null|null|\n",
      "| spam|England v Macedon...|null|null|null|\n",
      "+-----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iniData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlSelect = sql_477e8cbfde55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sql_477e8cbfde55"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlSelect = new feature.SQLTransformer().setStatement(\"SELECT label, text FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlFilter = sql_7072efbdafb2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sql_7072efbdafb2"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlFilter = new feature.SQLTransformer().setStatement(\"SELECT * from __THIS__ WHERE text is not null AND label is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer = strIdx_38a8df58a677\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "strIdx_38a8df58a677"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new feature.StringIndexer().setInputCol(\"label\").setOutputCol(\"label_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer = tok_08996ed607cb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tok_08996ed607cb"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new feature.Tokenizer().setInputCol(\"text\").setOutputCol(\"text_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf = hashingTF_9956eff1dc22\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hashingTF_9956eff1dc22"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = new feature.HashingTF().setNumFeatures(1000).setInputCol(\"text_token\").setOutputCol(\"text_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf = idf_8362dd91ab12\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "idf_8362dd91ab12"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new feature.IDF().setInputCol(\"text_tf\").setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr = logreg_00f550b98a1c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "logreg_00f550b98a1c"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression().setFeaturesCol(\"features\").setLabelCol(\"label_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlPipeline = pipeline_9a454f6620a8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_9a454f6620a8"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlPipeline = new Pipeline().setStages(Array(sqlSelect, sqlFilter, labelIndexer, tokenizer, tf, idf, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlPipelineModel = pipeline_9a454f6620a8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_9a454f6620a8"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlPipelineModel = mlPipeline.fit(iniData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|                text|label_num|          text_token|             text_tf|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  ham|Go until jurong p...|      0.0|[go, until, juron...|(1000,[7,77,150,1...|(1000,[7,77,150,1...|[46.1925496142281...|[1.0,9.5478469531...|       0.0|\n",
      "|  ham|Ok lar... Joking ...|      0.0|[ok, lar..., joki...|(1000,[20,316,484...|(1000,[20,316,484...|[22.7239392220272...|[0.99999999999972...|       0.0|\n",
      "| spam|Free entry in 2 a...|      1.0|[free, entry, in,...|(1000,[30,35,73,1...|(1000,[30,35,73,1...|[-49.707099745267...|[5.78975520257700...|       1.0|\n",
      "|  ham|U dun say so earl...|      0.0|[u, dun, say, so,...|(1000,[57,368,372...|(1000,[57,368,372...|[51.7359225992089...|[1.0,5.3440271207...|       0.0|\n",
      "|  ham|Nah I don't think...|      0.0|[nah, i, don't, t...|(1000,[135,163,32...|(1000,[135,163,32...|[38.6006856216011...|[1.0,3.0544816603...|       0.0|\n",
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlPipelineModel.transform(iniData).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
