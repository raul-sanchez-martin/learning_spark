{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with MLlib\n",
    "\n",
    "In this Notebook, we will review the RDD-Based Machine Learning library MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "First, we have to understand the different data structures used by MLlib. In particular, they are:\n",
    "\n",
    "    * Vectors\n",
    "    * Labeled Points\n",
    "    * Rating\n",
    "    * Model Classes\n",
    "    \n",
    "We will se `Vectors` and `Labeled Points` in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Vector()` --> to hold the features values. It can be `dense` and `sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorDense = [1.0,1.0,2.0,2.0]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.0,1.0,2.0,2.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorDense = Vectors.dense(Array(1.0,1.0,2.0,2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorSparse = (4,[0,2],[1.0,2.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4,[0,2],[1.0,2.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorSparse = Vectors.sparse(4, Array(0, 2), Array(1.0, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint()` --> hold both features values and label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelPoint = (1.0,[1.0,1.0,2.0,2.0])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0,[1.0,1.0,2.0,2.0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelPoint = LabeledPoint(1, vectorDense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "In this section, we will review the different algorithms associated with Machine Learning problems. Among other, we could highlight the following families of algorithms:\n",
    "\n",
    "    * Feature Extraction\n",
    "    * Statistics\n",
    "    * Classification and Regression\n",
    "    * Collaborative Filtering and Recommendation\n",
    "    * Dimensionality Reduction\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "ML algorithms only accept numerical values as inputs. Here, we discuss some algorithm that help us to translate some inputs (like text, non-scaled numerical vectors, etc) to numerical values that ML algorithms can understand. In particular, we will discuss the following algorithms:\n",
    "\n",
    "    * TD-IDF\n",
    "    * Scaling\n",
    "    * Normalization\n",
    "    * Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### td-idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`td-idf()` --> Term Frecuency - Inverse Document Frequency, useful to convert text input to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.{HashingTF, IDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentences = ParallelCollectionRDD[0] at parallelize at <console>:30\n",
       "words = MapPartitionsRDD[1] at map at <console>:31\n",
       "tf = org.apache.spark.mllib.feature.HashingTF@285f22e7\n",
       "tfVectors = MapPartitionsRDD[2] at map at HashingTF.scala:120\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at map at HashingTF.scala:120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sentences = sc.parallelize(Array(\"hello\", \"hello how are you\", \"good bye\", \"bye\"))\n",
    "val words = sentences.map(_.split(\" \").toSeq)\n",
    "val tf = new HashingTF(100)\n",
    "val tfVectors = tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100,[48],[1.0]), (100,[25,37,38,48],[1.0,1.0,1.0,1.0]), (100,[5,68],[1.0,1.0]), (100,[5],[1.0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfVectors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf = org.apache.spark.mllib.feature.IDF@78575d5\n",
       "idfModel = org.apache.spark.mllib.feature.IDFModel@71b66609\n",
       "tfIdfVectors = MapPartitionsRDD[7] at mapPartitions at IDF.scala:178\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at mapPartitions at IDF.scala:178"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF()\n",
    "val idfModel = idf.fit(tfVectors)\n",
    "val tfIdfVectors = idfModel.transform(tfVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100,[48],[0.5108256237659907]), (100,[25,37,38,48],[0.9162907318741551,0.9162907318741551,0.9162907318741551,0.5108256237659907]), (100,[5,68],[0.5108256237659907,0.9162907318741551]), (100,[5],[0.5108256237659907])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfVectors.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect\n",
    "\n",
    "`Word2Vec` --> also useful to tranform text into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vec = org.apache.spark.mllib.feature.Word2Vec@3e172326\n",
       "word2vecModel = org.apache.spark.mllib.feature.Word2VecModel@4f59a871\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.Word2VecModel@4f59a871"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word2vec = new Word2Vec().setMinCount(0)\n",
    "val word2vecModel = word2vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vecVectors = [-3.0199125831131823E-5,0.0016926409443840384,-0.004056421108543873,-0.004137029871344566,0.0011226466158404946,0.00416228175163269,-0.004522450268268585,-0.0034207457210868597,6.502352771349251E-4,0.0044151670299470425,0.003331251908093691,0.004456773865967989,-3.881255106534809E-4,-0.001828477019444108,0.0032321717590093613,-9.604980587027967E-4,-0.001065896824002266,3.4843764296965674E-5,-0.0032923424150794744,1.6158685320988297E-4,-0.0041165947914123535,-0.004293373785912991,8.68087459821254E-4,-7.831022376194596E-4,0.0038289502263069153,0.004241717979311943,0.0014030217425897717,0.004157128278166056,-7.13129120413214E-4,0.004436533898115158,-0.00357951526530087,-0.0024177739396691322,0.0020359789486974478,0.0014305984368547797...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-3.0199125831131823E-5,0.0016926409443840384,-0.004056421108543873,-0.004137029871344566,0.0011226466158404946,0.00416228175163269,-0.004522450268268585,-0.0034207457210868597,6.502352771349251E-4,0.0044151670299470425,0.003331251908093691,0.004456773865967989,-3.881255106534809E-4,-0.001828477019444108,0.0032321717590093613,-9.604980587027967E-4,-0.001065896824002266,3.4843764296965674E-5,-0.0032923424150794744,1.6158685320988297E-4,-0.0041165947914123535,-0.004293373785912991,8.68087459821254E-4,-7.831022376194596E-4,0.0038289502263069153,0.004241717979311943,0.0014030217425897717,0.004157128278166056,-7.13129120413214E-4,0.004436533898115158,-0.00357951526530087,-0.0024177739396691322,0.0020359789486974478,0.0014305984368547797,0.002553508384153247,0.004991794936358929,0.0023916843347251415,-0.0027679004706442356,-0.0016963399248197675,-0.003633954096585512,0.0018068415811285377,-0.003447020659223199,0.003823435865342617,0.004363159649074078,0.0017268708907067776,-0.0014385897666215897,-0.004827828612178564,-0.0036956307012587786,0.0016200706595554948,-7.055921596474946E-4,0.0012720038648694754,0.003133774036541581,-3.101626061834395E-4,0.002839671215042472,0.001179905841127038,0.002638245467096567,-0.0015979266026988626,4.6643897076137364E-4,0.003618876449763775,0.0018244776874780655,-0.004923512693494558,0.0012629389530047774,-0.0035483078099787235,-0.0034345753956586123,-0.002955583157017827,-0.002634344855323434,-9.330855100415647E-4,0.0011139442212879658,0.0011538431281223893,0.003194105578586459,0.0015380664262920618,0.0016252425266429782,0.0016921506030485034,-0.004923908971250057,-0.0037406906485557556,5.962211871519685E-4,-3.2937878859229386E-4,-0.0014347274554893374,0.0025788084603846073,0.0043850913643836975,0.0021359033416956663,0.0014708142261952162,0.0021801344119012356,0.0021404928993433714,-0.0036634227726608515,0.004990597255527973,0.0026104766875505447,-3.707238647621125E-4,-1.5431786596309394E-4,0.002418907592073083,0.0033742855302989483,-0.004060700070112944,4.0821649599820375E-4,0.0028542818035930395,0.0022168129216879606,0.004183533601462841,-0.004134373273700476,-0.003917126916348934,0.001665477524511516,1.2884876923635602E-4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word2vecVectors = word2vecModel.transform(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "While our input data could be already numeric, it is useful sometimes for the ML algorithms to scale that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler()` --> to scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectors = Array([-2.0,5.0,1.0,4.0], [2.0,0.0,1.0,7.2], [4.0,2.0,0.5,0.8])\n",
       "vectorsRdd = ParallelCollectionRDD[20] at parallelize at <console>:36\n",
       "scaler = org.apache.spark.mllib.feature.StandardScaler@675c2d7c\n",
       "model = org.apache.spark.mllib.feature.StandardScalerModel@1e06fc10\n",
       "scaledData = MapPartitionsRDD[25] at map at VectorTransformer.scala:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[25] at map at VectorTransformer.scala:52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectors = Array(Vectors.dense(Array(-2.0, 5.0, 1.0, 4.0)),\n",
    "                    Vectors.dense(Array(2.0, 0.0, 1.0, 7.2)),\n",
    "                    Vectors.dense(Array(4.0, 2.0, 0.5, 0.8)))\n",
    "\n",
    "val vectorsRdd = sc.parallelize(vectors)\n",
    "val scaler = new StandardScaler(withMean=true, withStd=true)\n",
    "val model = scaler.fit(vectorsRdd)\n",
    "val scaledData = model.transform(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.0910894511799618,1.0596258856520353,0.5773502691896257,0.0], [0.2182178902359924,-0.9271726499455306,0.5773502691896257,1.0], [0.8728715609439696,-0.13245323570650427,-1.1547005383792517,-1.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "As with scaling, sometimes it is very usefull to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm = org.apache.spark.mllib.feature.Normalizer@7a257871\n",
       "normData = MapPartitionsRDD[26] at map at VectorTransformer.scala:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at map at VectorTransformer.scala:52"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val norm = new Normalizer()\n",
    "val normData = norm.transform(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.29488391230979427,0.7372097807744856,0.14744195615489714,0.5897678246195885], [0.2652790545386455,0.0,0.13263952726932274,0.9550045963391238], [0.8751666735874727,0.43758333679373634,0.10939583419843409,0.17503333471749455]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "The library MLlib includes useful functionalities to calculate some main statistics over numeric RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.stat.Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colStats()\n",
    "\n",
    "`colStats()` --> to calculate statistics over an RDD of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colStats = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@57c10720\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@57c10720"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colStats = Statistics.colStats(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colStatsMap = Map(count -> 3, variance -> [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002], mean -> [1.3333333333333333,2.333333333333333,0.8333333333333334,4.0], numNonzeros -> [3.0,2.0,3.0,3.0], min -> [-2.0,0.0,0.5,0.8], normL1 -> [8.0,7.0,2.5,12.0], normL2 -> [4.898979485566356,5.385164807134504,1.5,8.27526434623088], max -> [4.0,5.0,1.0,7.2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(count -> 3, variance -> [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002], mean -> [1.3333333333333333,2.333333333333333,0.8333333333333334,4.0], numNonzeros -> [3.0,2.0,3.0,3.0], min -> [-2.0,0.0,0.5,0.8], normL1 -> [8.0,7.0,2.5,12.0], normL2 -> [4.898979485566356,5.385164807134504,1.5,8.27526434623088], max -> [4.0,5.0,1.0,7.2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colStatsMap = Map(\"count\" -> colStats.count, \n",
    "                      \"max\" -> colStats.max,\n",
    "                      \"mean\" -> colStats.mean,\n",
    "                      \"min\" -> colStats.min,\n",
    "                      \"normL1\" -> colStats.normL1,\n",
    "                      \"normL2\" -> colStats.normL2,\n",
    "                      \"numNonzeros\" -> colStats.numNonzeros,\n",
    "                      \"variance\" -> colStats.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 3\n",
      "variance: [9.333333333333334,6.333333333333333,0.08333333333333333,10.240000000000002]\n",
      "mean: [1.3333333333333333,2.333333333333333,0.8333333333333334,4.0]\n",
      "numNonzeros: [3.0,2.0,3.0,3.0]\n",
      "min: [-2.0,0.0,0.5,0.8]\n",
      "normL1: [8.0,7.0,2.5,12.0]\n",
      "normL2: [4.898979485566356,5.385164807134504,1.5,8.27526434623088]\n",
      "max: [4.0,5.0,1.0,7.2]\n"
     ]
    }
   ],
   "source": [
    "colStatsMap.foreach{case(key, value) => println(key + \": \" + value)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corr()\n",
    "\n",
    "`corr()` --> to calculate the correlation matrix between the columns of one RDD or between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0                   -0.7370434740955019   -0.7559289460184548  -0.32732683535398843  \n",
       "-0.7370434740955019   1.0                   0.11470786693528112  -0.39735970711951274  \n",
       "-0.7559289460184548   0.11470786693528112   1.0                  0.8660254037844397    \n",
       "-0.32732683535398843  -0.39735970711951274  0.8660254037844397   1.0                   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(vectorsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data1 = ParallelCollectionRDD[39] at parallelize at <console>:35\n",
       "data2 = ParallelCollectionRDD[40] at parallelize at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[40] at parallelize at <console>:36"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1: RDD[Double] = sc.parallelize(Array(1, 2, 3, 4, 5))\n",
    "val data2: RDD[Double] = sc.parallelize(Array(10, 19, 32, 41, 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996326893005933"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chiSqTest()\n",
    "\n",
    "`chiSqTest()` --> to compute the Pearson's independence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelPointRdd = MapPartitionsRDD[51] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[51] at map at <console>:38"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelPointRdd = vectorsRdd.map(x => LabeledPoint(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSqTest = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0\n",
       "statistic = 0.0\n",
       "pValue = 1.0\n",
       "No presumption against null hypothesi...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.., Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.0 \n",
       "pValue = 1.0 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSqTest = Statistics.chiSqTest(labelPointRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test value: 1.0\n",
      "Test value: 1.0\n",
      "Test value: 1.0\n",
      "Test value: 1.0\n"
     ]
    }
   ],
   "source": [
    "chiSqTest.foreach(x => println(\"Test value: \" + x.pValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
