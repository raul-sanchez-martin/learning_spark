{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Spark Streaming\n",
    "\n",
    "In this Chapter, we are going to investigate the streaming capabilities of Spark. \n",
    "\n",
    "In order to perform the exercises included in this Notebook, it is neccesary to send messages to the port 9999. There is a Python script (`send_messages.py`) that performs this automatically. In order to run that script, open a terminal using the Notebook interface. Then, place the working directory in ~/work/chapter10-spark-streaming, and type the following command: `nohup python send_messages.py &`. Please, take note of the process id shown in the terminal. If you want to finish this process, type `kill -9 <process-id>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless Transformations\n",
    "\n",
    "In this section, we will see a very easy example of some stateless transformations.\n",
    "\n",
    "### Conventional Stateless Transformations\n",
    "\n",
    "The majority of the stateless transformations are almost the same the ones we can applied in conventional RDDs (`map()`, `flatMap()`, `filter()`, ...). In order how to use them, we are going to to an example where we will count the number of occurences of different types of messages (\"info\", \"notice\", \"error\" and \"unkonwn\") coming from a log streaming input. We will perform that count on batch intervals of 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark\n",
    "\n",
    "/**\n",
    "Returns the type of message of an input line from log data deppending if the lines\n",
    "contains the \"[info]\" (type \"info\"), \"[notice]\" (type \"notice\") or \"[error]\" (type \"error\") \n",
    "keyword. If none of them are found, the returned type is \"unknown\"\n",
    "    \n",
    "@input line input line\n",
    "@return message type\n",
    "\n",
    "**/\n",
    "def getMessageType(line: String): String = {\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "val spark = \n",
    "val ssc = \n",
    "val lines = \n",
    "val codes = \n",
    "val codesCount = \n",
    "codesCount\n",
    "ssc\n",
    "ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful transformations\n",
    "\n",
    "Stateful operations are those which takes into account the values of the current batch and the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `AndWindow` type\n",
    "\n",
    "The are many equivalent stateless - statuful transformations, where the last ones are characterized by the ending \"AndWindow\". We will see one of the last examples using this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark\n",
    "\n",
    "/**\n",
    "Returns the type of message of an input line from log data deppending if the lines\n",
    "contains the \"[info]\" (type \"info\"), \"[notice]\" (type \"notice\") or \"[error]\" (type \"error\") \n",
    "keyword. If none of them are found, the returned type is \"unknown\"\n",
    "    \n",
    "@input line input line\n",
    "@return message type\n",
    "\n",
    "**/\n",
    "def getMessageType(line: String): String = {\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "val spark = \n",
    "val ssc = \n",
    "ssc\n",
    "val lines = \n",
    "val codes = \n",
    "val codesCount = \n",
    "\n",
    "codesCount\n",
    "ssc\n",
    "ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `updateStateByKey` type\n",
    "\n",
    "`updateStateByKey` function allows to keep some acumulative feautores during batch processing. For example, we are going to perform the last example but mantaining the total numbers of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark\n",
    "import org.apache.spark\n",
    "\n",
    "/**\n",
    "Returns the type of message of an input line from log data deppending if the lines\n",
    "contains the \"[info]\" (type \"info\"), \"[notice]\" (type \"notice\") or \"[error]\" (type \"error\") \n",
    "keyword. If none of them are found, the returned type is \"unknown\"\n",
    "    \n",
    "@input line input line\n",
    "@return message type\n",
    "**/\n",
    "def getMessageType(line: String): String = {\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "/**\n",
    "Accumulates an iterative counter\n",
    "**/\n",
    "def updateFunction(): Option[Int] = {\n",
    "}\n",
    "\n",
    "\n",
    "val spark = \n",
    "val ssc = \n",
    "ssc\n",
    "val lines = ssc\n",
    "val codes = lines\n",
    "val codesCount = codes\n",
    "val codesCountCumu = codesCount\n",
    "codesCountCumu\n",
    "ssc\n",
    "ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
