{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Working with Key/Value Pairs (Scala)\n",
    "\n",
    "In this Notebook, we will study the operations that can be performed on Key/Value RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pair RDDs\n",
    "\n",
    "Using `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericRdd = ParallelCollectionRDD[0] at parallelize at <console>:28\n",
       "pairRdd = MapPartitionsRDD[1] at map at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1] at map at <console>:29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val numericRdd = \n",
    "// val pairRdd = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair RDD from map(): (1,1), (4,16), (2,4), (4,16), (1,1), (3,9), (3,9)\n"
     ]
    }
   ],
   "source": [
    "// println(\"Pair RDD from map(): \" + pairRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on one Pair RDDs\n",
    "\n",
    "In addition to the RDD transformation explained in Chapter 3, we can perform the following transformations specific for individual key/value RDDs:\n",
    "\n",
    "    * reduceByKey()\n",
    "    * mapValues()\n",
    "    * groupByKey()\n",
    "    * combineByKey()\n",
    "    * flatMapValues()\n",
    "    * keys()\n",
    "    * values()\n",
    "    * sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey()` --> reduce the values of an RDD per key, `mapValues()` --> map the values of a key/value RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum values using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum values using reduceByKey(): (4,32), (1,2), (2,4), (3,18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sumValues = ShuffledRDD[2] at reduceByKey at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[2] at reduceByKey at <console>:31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val sumValues = \n",
    "println(\"Sum values using reduceByKey(): \" + sumValues.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average calculated using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average by key using reduceByKey(): (4,16), (1,1), (2,4), (3,9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "avgRedByKey = MapPartitionsRDD[5] at mapValues at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at mapValues at <console>:31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val avgRedByKey = \n",
    "println(\"Average by key using reduceByKey(): \" + avgRedByKey.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcount using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count using reduceByKey(): (package,1), (this,1), (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (page](http://spark.apache.org/documentation.html).,1), (cluster.,1), (its,1), ([run,1), (general,3)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines = ../data/README.md MapPartitionsRDD[7] at textFile at <console>:28\n",
       "words = ShuffledRDD[10] at reduceByKey at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[10] at reduceByKey at <console>:29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val lines = \n",
    "// val words = \n",
    "print(\"Word count using reduceByKey(): \" + words.take(10).mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupByKey()` --> group values of an RDD grupped by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped RDD using groupByKey(): (4,CompactBuffer(16, 16)), (1,CompactBuffer(1, 1)), (2,CompactBuffer(4)), (3,CompactBuffer(9, 9))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedValues = ShuffledRDD[11] at groupByKey at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[11] at groupByKey at <console>:31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val groupedValues = \n",
    "println(\"Grouped RDD using groupByKey(): \" + groupedValues.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combineByKey()` --> combines the values of an RDD according to their key. Here we calculate the average per key of an key/value RDD using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,1), (4,16), (2,4), (4,16), (1,1), (3,9), (3,9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average by key using combineByKey(): (4,16.0), (1,1.0), (2,4.0), (3,9.0)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sumKeyValues = ShuffledRDD[12] at combineByKey at <console>:31\n",
       "avgComByKey = MapPartitionsRDD[13] at mapValues at <console>:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at mapValues at <console>:35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val sumKeyValues = \n",
    "\n",
    "// val avgComByKey = \n",
    "\n",
    "print(\"Average by key using combineByKey(): \" + avgComByKey.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMapValues()` --> flat-maps the values of a key/value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD using flatMapValues(): (1,1), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (4,7), (4,8), (4,9)\n"
     ]
    }
   ],
   "source": [
    "// println(\"RDD using flatMapValues(): \" + pairRdd.....take(10).mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keys()` --> get the keys of a key/value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get keys from key/pair RDD using keys(): 1, 4, 2, 4, 1, 3, 3\n"
     ]
    }
   ],
   "source": [
    "// println(\"Get keys from key/pair RDD using keys(): \" + pairRdd.....collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`values()` --> get the values of a key/value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get values from key/pair RDD using keys(): 1, 16, 4, 16, 1, 9, 9\n"
     ]
    }
   ],
   "source": [
    "// println(\"Get values from key/pair RDD using keys(): \" + pairRdd.....collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sortByKey()` --> sort the values of an RDD for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddSort = ParallelCollectionRDD[17] at parallelize at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[17] at parallelize at <console>:28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddSort = sc.parallelize(List((4, (8, 2)), (1, (3, 1, 9))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get RDD sorted by keys using sortByKey(): (1,(3,1,9)), (4,(8,2))\n"
     ]
    }
   ],
   "source": [
    "// println(\"Get RDD sorted by keys using sortByKey(): \" + rddSort.....collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on two Pair RDDs\n",
    "\n",
    "In this section, the different transformation that can be performed on two key/value RDDs are studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairRdd1 = ParallelCollectionRDD[21] at parallelize at <console>:28\n",
       "pairRdd2 = ParallelCollectionRDD[22] at parallelize at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[22] at parallelize at <console>:29"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairRdd1 = sc.parallelize(List((3, 'A'), (2, 'J'), (5, 'K')))\n",
    "val pairRdd2 = sc.parallelize(List((5, 'Z'), (3, 'W'), (7, 'B')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`subtractByKey()` --> subtract two RDDs by Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from subtractByKey(): (3,A), (2,J), (5,K)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subtractRdd = MapPartitionsRDD[26] at subtract at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at subtract at <console>:31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val subtractRdd = \n",
    "println(\"RDD from subtractByKey(): \" + subtractRdd.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.join()` --> inner join two RDD by Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from join(): (5,(K,Z)), (3,(A,W))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinRdd = MapPartitionsRDD[29] at join at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[29] at join at <console>:31"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val joinRdd = \n",
    "println(\"RDD from join(): \" + joinRdd.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.leftOuterJoin()` --> left outer join two RDD by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from leftOuterJoin(): (5,(K,Some(Z))), (2,(J,None)), (3,(A,Some(W)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "leftOuterJoinRdd = MapPartitionsRDD[32] at leftOuterJoin at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[32] at leftOuterJoin at <console>:31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val leftOuterJoinRdd = \n",
    "println(\"RDD from leftOuterJoin(): \" + leftOuterJoinRdd.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.rightOuterJoin()` --> right outer join two RDD by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from rightOuterJoin(): (5,(Some(K),Z)), (3,(Some(A),W)), (7,(None,B))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rightOuterJoinRdd = MapPartitionsRDD[35] at rightOuterJoin at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[35] at rightOuterJoin at <console>:31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val rightOuterJoinRdd = \n",
    "println(\"RDD from rightOuterJoin(): \" + rightOuterJoinRdd.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.cogroup()` --> cogroup two RDD on key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from cogroup(): (5,(CompactBuffer(K),CompactBuffer(Z))), (2,(CompactBuffer(J),CompactBuffer())), (3,(CompactBuffer(A),CompactBuffer(W))), (7,(CompactBuffer(),CompactBuffer(B)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cogroupedRdd = MapPartitionsRDD[37] at cogroup at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[37] at cogroup at <console>:31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val cogroupedRdd = \n",
    "println(\"RDD from cogroup(): \" + cogroupedRdd.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions Available on Pair RDDs\n",
    "\n",
    "In addition to the actions explained in Chapter 3, we can perform the following additional actions on key/value RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`countByKey()` --> count the value ocurrences in an RDD for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countByKey(): Map(4 -> 2, 1 -> 2, 2 -> 1, 3 -> 2)\n"
     ]
    }
   ],
   "source": [
    "// println(\"countByKey(): \" + pairRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collectAsMap()` --> tranforms the key/value RDD as a HashMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collectAsMap(): Map(2 -> 4, 4 -> 16, 1 -> 1, 3 -> 9)\n"
     ]
    }
   ],
   "source": [
    "// println(\"collectAsMap(): \" + pairRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lookup()` --> lookup the value corresponding to a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup(4): WrappedArray(16, 16)\n"
     ]
    }
   ],
   "source": [
    "// println(\"lookup(4): \" + pairRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "Finally, we will discussed two extra operations regarding the partitioning of key/value RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`repartition(n)` --> repartitions the RDD according in n partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition of an RDD: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(1,1), (2,4), (4,16), (3,9)], [(4,16), (1,1), (3,9)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Repartition of an RDD: \")\n",
    "// pairRdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partitionBy()` --> custom partitions the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myPartitioner = org.apache.spark.HashPartitioner@2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.HashPartitioner@2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myPartitioner = new HashPartitioner(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom partitioning using partitionBy()"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(4,16), (2,4), (4,16)], [(1,1), (1,1), (3,9), (3,9)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Custom partitioning using partitionBy()\")\n",
    "// pairRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
