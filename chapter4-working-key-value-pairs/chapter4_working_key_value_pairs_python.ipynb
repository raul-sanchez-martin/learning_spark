{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Working with Key/Value Pairs (Python)\n",
    "\n",
    "In this Notebook, we will study the operations that can be performed on Key/Value RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Key-Value-Pairs\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pair RDDs\n",
    "\n",
    "Using `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_rdd = sc.parallelize([1,4,2,4,1,3,3])\n",
    "pair_rdd = numeric_rdd.map(lambda x: (x, x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair RDD from map(): [(1, 1), (4, 16), (2, 4), (4, 16), (1, 1), (3, 9), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pair RDD from map(): {0}\".format(pair_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on one Pair RDDs\n",
    "\n",
    "In addition to the RDD transformation explained in Chapter 3, we can perform the following transformations specific for individual key/value RDDs:\n",
    "\n",
    "    * reduceByKey()\n",
    "    * mapValues()\n",
    "    * groupByKey()\n",
    "    * combineByKey()\n",
    "    * flatMapValues()\n",
    "    * keys()\n",
    "    * values()\n",
    "    * sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey()` --> reduce the values of an RDD per key, `mapValues()` --> map the values of a key/value RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum values using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum values using reduceByKey(): [(4, 32), (1, 2), (2, 4), (3, 18)]\n"
     ]
    }
   ],
   "source": [
    "sum_values = pair_rdd.reduceByKey(lambda x, y: x+y)\n",
    "print(\"Sum values using reduceByKey(): {0}\".format(sum_values.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average calculated using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average by key using reduceByKey(): [(4, 16.0), (1, 1.0), (2, 4.0), (3, 9.0)]\n"
     ]
    }
   ],
   "source": [
    "avg_red_by_key = pair_rdd.map(lambda x: (x[0], (x[1], 1)))\\\n",
    ".reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).mapValues(lambda x: x[0]/x[1]).collect()\n",
    "\n",
    "print(\"Average by key using reduceByKey(): {0}\".format(avg_red_by_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcount using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count using reduceByKey(): [('#', 1), ('Apache', 1), ('Spark', 16), ('', 71), ('is', 6), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('in', 6)]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"../data/README.md\")\n",
    "words = lines.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1))\n",
    "words_count = words.reduceByKey(lambda x, y: x + y)\n",
    "print(\"Word count using reduceByKey(): {0}\".format(words_count.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupByKey()` --> group values of an RDD grupped by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped RDD using groupByKey(): [(4, <pyspark.resultiterable.ResultIterable object at 0x7f70684405c0>), (1, <pyspark.resultiterable.ResultIterable object at 0x7f70684404a8>), (2, <pyspark.resultiterable.ResultIterable object at 0x7f7068440cc0>), (3, <pyspark.resultiterable.ResultIterable object at 0x7f70684402b0>)]\n"
     ]
    }
   ],
   "source": [
    "grouped_values = pair_rdd.groupByKey()\n",
    "print(\"Grouped RDD using groupByKey(): {0}\".format(grouped_values.groupByKey().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combineByKey()` --> combines the values of an RDD according to their key. Here we calculate the average per key of an key/value RDD using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average by key using combineByKey(): [(4, 16.0), (1, 1.0), (2, 4.0), (3, 9.0)]\n"
     ]
    }
   ],
   "source": [
    "sum_key_values = pair_rdd.combineByKey(lambda value: (value, 1), \n",
    "                                      (lambda acc, value: (acc[0] + value, acc[1] + 1)), \n",
    "                                      (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))\n",
    "\n",
    "avg_com_by_key = sum_key_values.map(lambda x: (x[0], x[1][0]/x[1][1])).collect()\n",
    "\n",
    "print(\"Average by key using combineByKey(): {0}\".format(avg_com_by_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMapValues()` --> flat-maps the values of a key/value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD using flatMapValues(): [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (2, 0), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"RDD using flatMapValues(): {0}\".format(pair_rdd.flatMapValues(lambda x: (list(range(int(x/2))))).take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keys()` --> get the keys of a key/value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get keys from key/pair RDD using keys(): [1, 4, 2, 4, 1, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Get keys from key/pair RDD using keys(): {0}\".format(pair_rdd.keys().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`values()` --> get the values of a key/value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get values from key/pair RDD using values(): [1, 16, 4, 16, 1, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Get values from key/pair RDD using values(): {0}\".format(pair_rdd.values().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sortByKey()` --> sort the values of an RDD for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_sort = sc.parallelize([(4, (8, 2)), (1, (3, 1, 9))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get RDD sorted by keys using sortByKey(): [(1, (3, 1, 9)), (4, (8, 2))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Get RDD sorted by keys using sortByKey(): {0}\".format(rdd_sort.sortByKey().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on two Pair RDDs\n",
    "\n",
    "In this section, the different transformation that can be performed on two key/value RDDs are studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_rdd_1 = sc.parallelize([(3, 'A'), (2, 'J'), (5, 'K')]) \n",
    "pair_rdd_2 = sc.parallelize([(5, 'Z'), (3, 'W'), (7, 'B')]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`subtractByKey()` --> subtract two RDDs by Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD from subtractByKey(): [(2, 'J')]\n"
     ]
    }
   ],
   "source": [
    "subtract_rdd = pair_rdd_1.subtractByKey(pair_rdd_2)\n",
    "print(\"RDD from subtractByKey(): {0}\".format(subtract_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.join()` --> inner join two RDD by Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner join: [(3, ('A', 'W')), (5, ('K', 'Z'))]\n"
     ]
    }
   ],
   "source": [
    "inner_join_rdd = pair_rdd_1.join(pair_rdd_2)\n",
    "print(\"Inner join: {0}\".format(inner_join_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.leftOuterJoin()` --> left outer join two RDD by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left outer join: [(2, ('J', None)), (3, ('A', 'W')), (5, ('K', 'Z'))]\n"
     ]
    }
   ],
   "source": [
    "left_outer_join_rdd = pair_rdd_1.leftOuterJoin(pair_rdd_2)\n",
    "print(\"Left outer join: {0}\".format(left_outer_join_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.rightOuterJoin()` --> right outer join two RDD by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right outer join: [(3, ('A', 'W')), (5, ('K', 'Z')), (7, (None, 'B'))]\n"
     ]
    }
   ],
   "source": [
    "right_outer_join_rdd = pair_rdd_1.rightOuterJoin(pair_rdd_2)\n",
    "print(\"Right outer join: {0}\".format(right_outer_join_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.cogroup()` --> cogroup two RDD on key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cogrouped RDD: [(2, (<pyspark.resultiterable.ResultIterable object at 0x7f70682cdeb8>, <pyspark.resultiterable.ResultIterable object at 0x7f70682cdda0>)), (3, (<pyspark.resultiterable.ResultIterable object at 0x7f70682cdb70>, <pyspark.resultiterable.ResultIterable object at 0x7f708c38f898>)), (5, (<pyspark.resultiterable.ResultIterable object at 0x7f70682cdc18>, <pyspark.resultiterable.ResultIterable object at 0x7f708c38fdd8>)), (7, (<pyspark.resultiterable.ResultIterable object at 0x7f70682cdba8>, <pyspark.resultiterable.ResultIterable object at 0x7f706841f588>))]\n"
     ]
    }
   ],
   "source": [
    "cogroup_rdd = pair_rdd_1.cogroup(pair_rdd_2)\n",
    "print(\"Cogrouped RDD: {0}\".format(cogroup_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions Available on Pair RDDs\n",
    "\n",
    "In addition to the actions explained in Chapter 3, we can perform the following additional actions on key/value RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`countByKey()` --> count the value ocurrences in an RDD for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countByKey(): defaultdict(<class 'int'>, {1: 2, 4: 2, 2: 1, 3: 2})\n"
     ]
    }
   ],
   "source": [
    "print(\"countByKey(): {0}\".format(pair_rdd.countByKey()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collectAsMap()` --> tranforms the key/value RDD as a Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collectAsMap(): {1: 1, 4: 16, 2: 4, 3: 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"collectAsMap(): {0}\".format(pair_rdd.collectAsMap()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lookup()` --> lookup the value corresponding to a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup(4): [16, 16]\n"
     ]
    }
   ],
   "source": [
    "print(\"lookup(4): {0}\".format(pair_rdd.lookup(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "Finally, we will discussed two extra operations regarding the partitioning of key/value RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`repartition(n)` --> repartitions the RDD according in n partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition of an RDD: [[(1, 1), (4, 16), (1, 1), (3, 9), (3, 9)], [(4, 16), (2, 4)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Repartition of an RDD: {0}\".format(pair_rdd.repartition(2).glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partitionBy()` --> custom partitions the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom partitioning using partitionBy(): [[(1, 1), (2, 4), (1, 1)], [(4, 16), (4, 16), (3, 9), (3, 9)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom partitioning using partitionBy(): {0}\".format(pair_rdd.partitionBy(2, lambda x: int(x>2)).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
