{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Working with Key/Value Pairs (Scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@767b462b\n",
       "sc = org.apache.spark.SparkContext@189c7db3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://a994cf9fba8a:4041)\" target=\"new_tab\">Spark UI: local-1533151800656</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1533151800656: Some(http://a994cf9fba8a:4041)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"Working Key/Value Pairs\").master(\"local[*]\").getOrCreate()\n",
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pair RDDs\n",
    "\n",
    "Using `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericRdd = ParallelCollectionRDD[0] at parallelize at <console>:30\n",
       "pairRdd = MapPartitionsRDD[1] at map at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1] at map at <console>:31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericRdd = sc.parallelize(List(1,4,2,4,1,3,3))\n",
    "val pairRdd = numericRdd.map(x => (x, pow(x, 2).toInt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair RDD from map(): (1,1), (4,16), (2,4), (4,16), (1,1), (3,9), (3,9)\n"
     ]
    }
   ],
   "source": [
    "println(\"Pair RDD from map(): \" + pairRdd.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on one Pair RDDs\n",
    "\n",
    "`reduceByKey()`, `mapValues()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum values using reduceByKey(): (4,32), (1,2), (2,4), (3,18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sumValues = ShuffledRDD[2] at reduceByKey at <console>:33\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[2] at reduceByKey at <console>:33"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sumValues = pairRdd.reduceByKey(_ + _)\n",
    "println(\"Sum values using reduceByKey(): \" + sumValues.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average by key using reduceByKey(): (4,16), (1,1), (2,4), (3,9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "avgRedByKey = MapPartitionsRDD[5] at mapValues at <console>:33\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at mapValues at <console>:33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val avgRedByKey = pairRdd.map(x => (x._1, (x._2, 1))).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)).mapValues(x => x._1/x._2)\n",
    "println(\"Average by key using reduceByKey(): \" + avgRedByKey.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count using reduceByKey(): (package,1), (this,1), (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (page](http://spark.apache.org/documentation.html).,1), (cluster.,1), (its,1), ([run,1), (general,3)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines = ../data/README.md MapPartitionsRDD[7] at textFile at <console>:30\n",
       "words = ShuffledRDD[10] at reduceByKey at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[10] at reduceByKey at <console>:31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"../data/README.md\")\n",
    "val words = lines.flatMap(_.split(\" \")).map(x => (x, 1)).reduceByKey(_ + _)\n",
    "print(\"Word count using reduceByKey(): \" + words.take(10).mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupByKey()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped RDD using groupByKey(): (4,CompactBuffer(16, 16)), (1,CompactBuffer(1, 1)), (2,CompactBuffer(4)), (3,CompactBuffer(9, 9))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedValues = ShuffledRDD[11] at groupByKey at <console>:33\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[11] at groupByKey at <console>:33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedValues = pairRdd.groupByKey()\n",
    "println(\"Grouped RDD using groupByKey(): \" + groupedValues.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combineByKey()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,1), (4,16), (2,4), (4,16), (1,1), (3,9), (3,9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average by key using combineByKey(): (4,16.0), (1,1.0), (2,4.0), (3,9.0)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sumKeyValues = ShuffledRDD[12] at combineByKey at <console>:33\n",
       "avgComByKey = MapPartitionsRDD[13] at mapValues at <console>:37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at mapValues at <console>:37"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sumKeyValues = pairRdd.combineByKey((value: Int) => (value, 1),\n",
    "                                        (acc:(Int, Int), value: Int) => (acc._1 + value, acc._2 + 1),\n",
    "                                        (acc1:(Int, Int), acc2:(Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2))\n",
    "\n",
    "val avgComByKey = sumKeyValues.mapValues({case(x, y) => x.toFloat/y})\n",
    "\n",
    "print(\"Average by key using combineByKey(): \" + avgComByKey.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatMapValues()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD using flatMapValues(): (1,1), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (4,7), (4,8), (4,9)\n"
     ]
    }
   ],
   "source": [
    "println(\"RDD using flatMapValues(): \" + pairRdd.flatMapValues(x => (1 to x)).take(10).mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keys()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get keys from key/pair RDD using keys(): 1, 4, 2, 4, 1, 3, 3\n"
     ]
    }
   ],
   "source": [
    "println(\"Get keys from key/pair RDD using keys(): \" + pairRdd.keys.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`values()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get values from key/pair RDD using keys(): 1, 16, 4, 16, 1, 9, 9\n"
     ]
    }
   ],
   "source": [
    "println(\"Get values from key/pair RDD using keys(): \" + pairRdd.values.collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sortByKey()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddSort = ParallelCollectionRDD[17] at parallelize at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[17] at parallelize at <console>:30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddSort = sc.parallelize(List((4, (8, 2)), (1, (3, 1, 9))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get RDD sorted by keys using sortByKey(): (1,(3,1,9)), (4,(8,2))\n"
     ]
    }
   ],
   "source": [
    "println(\"Get RDD sorted by keys using sortByKey(): \" + rddSort.sortByKey().collect().mkString(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
