{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join of Spark Dataframes: Study and Optimization\n",
    "\n",
    "In this notebook, we are going to study how to optimize one of the most common operations in Data Science: inner join of two tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@68e0a774\n",
       "sc = org.apache.spark.SparkContext@df72a2d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://744a1323357c:4040)\" target=\"new_tab\">Spark UI: app-20180723214647-0000</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark app-20180723214647-0000: Some(http://744a1323357c:4040)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"JoinExample\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.SparkSession@68e0a774\n"
     ]
    }
   ],
   "source": [
    "println(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object DataFrameGenerator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StringType, IntegerType, StructType, DoubleType}\n",
    "import org.apache.spark.sql.functions.rand\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "object DataFrameGenerator extends Serializable{\n",
    "    \n",
    "    val r = scala.util.Random\n",
    "    \n",
    "    def getRandomInt(maxValue: Int) = {\n",
    "        r.nextInt(maxValue)\n",
    "    }\n",
    "    \n",
    "    def getRandomDouble(maxValue: Float) = {\n",
    "        r.nextDouble()*maxValue\n",
    "        \n",
    "    }\n",
    "    \n",
    "    def getIdsRdd(numObs: Int) = {\n",
    "        sc.parallelize((for(id <- 1 to numObs) yield id).toSeq)\n",
    "    }\n",
    "    \n",
    "    def getPairIntDf(numObs: Int, maxValue: Int, columnName: String) = {\n",
    "        \n",
    "        val schema = new StructType(Array(StructField(\"id\", IntegerType, true),\n",
    "                                          StructField(columnName, IntegerType, true)))\n",
    "        \n",
    "        val rddRows = getIdsRdd(numObs).map(id => Row(id, getRandomInt(maxValue)))\n",
    "        \n",
    "        spark.createDataFrame(rowRDD = rddRows, schema = schema).orderBy(rand())\n",
    "        \n",
    "    }\n",
    "    \n",
    "    def getPairDoubleDf(numObs: Int, maxValue: Float, columnName: String) = {\n",
    "        \n",
    "        val schema = new StructType(Array(StructField(\"id\", IntegerType, true),\n",
    "                                          StructField(columnName, DoubleType, true)))\n",
    "        \n",
    "        val rddRows = getIdsRdd(numObs).map(id => Row(id, getRandomDouble(maxValue)))\n",
    "        \n",
    "        spark.createDataFrame(rowRDD = rddRows, schema = schema).orderBy(rand())\n",
    "        \n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  3| 20|\n",
      "| 54| 17|\n",
      "| 14| 22|\n",
      "| 35| 21|\n",
      "| 93| 31|\n",
      "| 26|  1|\n",
      "| 15| 42|\n",
      "| 51| 30|\n",
      "| 92|  2|\n",
      "| 87| 38|\n",
      "| 81|  5|\n",
      "| 13| 34|\n",
      "|  5| 36|\n",
      "| 86|  0|\n",
      "| 78| 23|\n",
      "| 32| 10|\n",
      "|  7| 65|\n",
      "| 46| 74|\n",
      "| 25| 51|\n",
      "| 22| 23|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrameGenerator.getPairIntDf(100, 75, \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|           balance|\n",
      "+---+------------------+\n",
      "| 84| 66.62744825515291|\n",
      "| 20| 83.92155102965894|\n",
      "|  3| 40.71240944885693|\n",
      "|100|119.78321397507078|\n",
      "| 69|  4.88308752545051|\n",
      "| 88| 15.48028558840793|\n",
      "| 22|15.929961755244928|\n",
      "| 40|116.11776635989675|\n",
      "| 32| 92.46336330395428|\n",
      "| 11|28.828790918351956|\n",
      "| 31|106.12498589051039|\n",
      "| 64| 66.21497372059905|\n",
      "| 41|114.94860346168521|\n",
      "| 93| 32.09598493807166|\n",
      "| 54|3.4428875504493166|\n",
      "| 30|  39.3428290650875|\n",
      "| 43| 46.13014463237125|\n",
      "| 36| 89.44160888039372|\n",
      "| 25|16.425128380903626|\n",
      "| 77|25.073698534942054|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrameGenerator.getPairDoubleDf(100, 125, \"balance\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join Mode 1: No Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numObsMode1 = 250000\n",
       "df1 = [id: int, age: int]\n",
       "df2 = [balance: double, id: int]\n",
       "iniTime = 1.532382462584E9\n",
       "dfJoined = [id: int, age: int ... 1 more field]\n",
       "computationTimeMode1 = 7.1579999923706055\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7.1579999923706055"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numObsMode1 = 250000\n",
    "val df1 = DataFrameGenerator.getPairIntDf(numObsMode1, 75, \"age\")\n",
    "val df2 = DataFrameGenerator.getPairDoubleDf(numObsMode1, 125, \"balance\").select(\"balance\", \"id\")\n",
    "val iniTime = System.currentTimeMillis()/1000.0\n",
    "val dfJoined = df1.join(df2, Seq(\"id\"), \"inner\")\n",
    "dfJoined.count()\n",
    "val computationTimeMode1 = System.currentTimeMillis()/1000.0 - iniTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total computational time (s) - Inner Join Mode 1: No Partitioning - No Cache: 7.1579999923706055\n"
     ]
    }
   ],
   "source": [
    "println(\"Total computational time (s) - Inner Join Mode 1: No Partitioning - No Cache: \" + computationTimeMode1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfJoined.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfJoined.rdd.partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join Mode 2: Tunning Number of Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getExeTimeNumPartitions: (numPartitions: Int)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getExeTimeNumPartitions(numPartitions: Int) = {\n",
    "    val numObsMode1 = 2500000\n",
    "    val df1 = DataFrameGenerator.getPairIntDf(numObsMode1, 75, \"age\").repartition(numPartitions)\n",
    "    val df2 = DataFrameGenerator.getPairDoubleDf(numObsMode1, 125, \"balance\").select(\"balance\", \"id\").repartition(numPartitions)\n",
    "    val iniTime = System.currentTimeMillis()/1000.0\n",
    "    val dfJoined = df1.join(df2, Seq(\"id\"), \"inner\")\n",
    "    dfJoined.count()\n",
    "    System.currentTimeMillis()/1000.0 - iniTime \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timeMap = Map()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val timeMap = Map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timeMap = Map(1024 -> 50.712000131607056, 1 -> 5.406999826431274, 512 -> 27.753999948501587, 256 -> 18.930999994277954, 128 -> 14.734999895095825, 2 -> 7.053999900817871, 32 -> 7.3450000286102295, 64 -> 7.319000005722046, 16 -> 7.259999990463257, 8 -> 7.228000164031982, 4 -> 6.940999984741211)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(1024 -> 50.712000131607056, 1 -> 5.406999826431274, 512 -> 27.753999948501587, 256 -> 18.930999994277954, 128 -> 14.734999895095825, 2 -> 7.053999900817871, 32 -> 7.3450000286102295, 64 -> 7.319000005722046, 16 -> 7.259999990463257, 8 -> 7.228000164031982, 4 -> 6.940999984741211)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val timeMap = (for(time <- Array(1,2,4,8,16,32,64,128,256,512,1024)) yield time -> getExeTimeNumPartitions(time)).toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(numPartitions,IntegerType,true), StructField(time,DoubleType,true))\n",
       "timeDf = [numPartitions: int, time: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[numPartitions: int, time: double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{IntegerType, DoubleType, StructType, StructField}\n",
    "\n",
    "val schema = new StructType(Array(StructField(\"numPartitions\", IntegerType, true), \n",
    "                                  StructField(\"time\", DoubleType, true)))\n",
    "\n",
    "val timeDf = spark.createDataFrame(sc.parallelize(timeMap.toSeq).map{case(key, value) => Row(key,value)}, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|numPartitions|              time|\n",
      "+-------------+------------------+\n",
      "|            1| 5.406999826431274|\n",
      "|            4| 6.940999984741211|\n",
      "|            2| 7.053999900817871|\n",
      "|            8| 7.228000164031982|\n",
      "|           16| 7.259999990463257|\n",
      "|           64| 7.319000005722046|\n",
      "|           32|7.3450000286102295|\n",
      "|          128|14.734999895095825|\n",
      "|          256|18.930999994277954|\n",
      "|          512|27.753999948501587|\n",
      "|         1024|50.712000131607056|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeDf.orderBy(\"time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numPartitions = numWorkerNodes * numCpuCoresPerWorker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join Mode 3: Pre - Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numObsMode1 = 500000\n",
    "val df1 = DataFrameGenerator.getPairIntDf(numObsMode1, 75, \"age\")\n",
    "val df2 = DataFrameGenerator.getPairDoubleDf(numObsMode1, 125, \"balance\").select(\"balance\", \"id\")\n",
    "df1.unpersist()\n",
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iniTime = System.currentTimeMillis()/1000.0\n",
    "val df1Par = df1.repartition($\"id\")\n",
    "val df2Par = df2.repartition($\"id\")\n",
    "val partitionTimeMode2 = System.currentTimeMillis()/1000.0 - iniTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iniTime = System.currentTimeMillis()/1000.0\n",
    "val numObsMode1 = 500000\n",
    "val df1 = DataFrameGenerator.getPairIntDf(numObsMode1, 75, \"age\")\n",
    "val df2 = DataFrameGenerator.getPairDoubleDf(numObsMode1, 125, \"balance\").select(\"balance\", \"id\")\n",
    "df1.unpersist()\n",
    "df2.unpersist()\n",
    "val df1Par = df1.repartition($\"id\")\n",
    "val df2Par = df2.repartition($\"id\")\n",
    "df1Par.join(df2Par, Seq(\"id\"), \"inner\").count()\n",
    "val joinTimeMode2 = System.currentTimeMillis()/1000.0 - iniTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1Par.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://hadoop:9000/data/df1Par.parquet\")\n",
    "df2Par.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://hadoop:9000/data/df2Par.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iniTime = System.currentTimeMillis()/1000.0\n",
    "val df1ParLoaded = spark.read.format(\"parquet\").load(\"hdfs://hadoop:9000/data/df1Par.parquet\")\n",
    "val df2ParLoaded = spark.read.format(\"parquet\").load(\"hdfs://hadoop:9000/data/df2Par.parquet\")\n",
    "df1ParLoaded.join(df2ParLoaded, Seq(\"id\"), \"inner\").count()\n",
    "val joinTimeMode3 = System.currentTimeMillis()/1000.0 - iniTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join Mode 4: Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numObsMode1 = 250000\n",
       "df1 = [id: int, age: int]\n",
       "df2 = [balance: double, id: int]\n",
       "iniTime = 1.532382278788E9\n",
       "dfJoined = [id: int, age: int ... 1 more field]\n",
       "computationTimeMode1 = 3.435999870300293\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.435999870300293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numObsMode1 = 250000\n",
    "val df1 = DataFrameGenerator.getPairIntDf(numObsMode1, 75, \"age\")\n",
    "val df2 = DataFrameGenerator.getPairDoubleDf(numObsMode1/10000, 125, \"balance\").select(\"balance\", \"id\")\n",
    "val iniTime = System.currentTimeMillis()/1000.0\n",
    "val dfJoined = df1.join(df2, Seq(\"id\"), \"inner\")\n",
    "dfJoined.count()\n",
    "val computationTimeMode1 = System.currentTimeMillis()/1000.0 - iniTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numObsMode1 = 250000\n",
       "df1 = [id: int, age: int]\n",
       "df2 = [balance: double, id: int]\n",
       "iniTime = 1.532382286528E9\n",
       "dfJoined = [id: int, age: int ... 1 more field]\n",
       "computationTimeMode1 = 1.8939998149871826\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.8939998149871826"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numObsMode1 = 250000\n",
    "val df1 = DataFrameGenerator.getPairIntDf(numObsMode1, 75, \"age\")\n",
    "val df2 = DataFrameGenerator.getPairDoubleDf(numObsMode1/10000, 125, \"balance\").select(\"balance\", \"id\")\n",
    "val iniTime = System.currentTimeMillis()/1000.0\n",
    "val dfJoined = df1.join(broadcast(df2), Seq(\"id\"), \"inner\")\n",
    "dfJoined.count()\n",
    "val computationTimeMode1 = System.currentTimeMillis()/1000.0 - iniTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
